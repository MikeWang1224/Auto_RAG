# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
æ›´æ–°å…§å®¹ï¼š
âœ… UTF-8 é˜²äº‚ç¢¼
âœ… å‘½ä¸­ token ä¸é‡è¤‡
âœ… èµ°å‹¢å›ºå®šç‚ºã€Œåå‘ä¸Šæ¼² / åå‘ä¸‹è·Œ / æŒå¹³ã€
âœ… ç§»é™¤æœ€çµ‚ã€Œçµæœå·²å„²å­˜ã€çš„å°å‡º
âœ… Groq è‡ªå‹•åˆ†æ‰¹åˆ†æï¼ˆé˜²æ­¢ 413ï¼‰
"""

import os, signal, regex as re, sys
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

# ---------- é˜²æ­¢äº‚ç¢¼ ----------
sys.stdout.reconfigure(encoding="utf-8")

# ---------- è¨­å®š ----------
SILENT_MODE = False
MAX_DISPLAY_NEWS = 5
BATCH_SIZE = 5  # ğŸ”¹ Groq æ¯æ‰¹æœ€å¤šåˆ†æ 5 ç¯‡æ–°è
TAIWAN_TZ = timezone(timedelta(hours=8))

# ---------- è®€ .env ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)

TOKENS_COLLECTION = os.getenv("FIREBASE_TOKENS_COLLECTION", "bull_tokens")
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"
SCORE_THRESHOLD = float(os.getenv("SCORE_THRESHOLD", "0.2"))
LOOKBACK_DAYS = int(os.getenv("LOOKBACK_DAYS", "2"))

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- è³‡æ–™çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def first_n_sentences(text: str, n: int = 3) -> str:
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    parts = [p for p in parts if p.strip()]
    joined = "".join(parts[:n])
    if not re.search(r'[ã€‚\.ï¼!\?ï¼Ÿï¼›;]$', joined):
        joined += "..."
    return joined

def parse_docid_time(doc_id: str):
    try:
        if "_" in doc_id:
            return datetime.strptime(doc_id, "%Y%m%d_%H%M%S").replace(tzinfo=TAIWAN_TZ)
        return datetime.strptime(doc_id, "%Y%m%d").replace(tzinfo=TAIWAN_TZ)
    except:
        return None

# ---------- åˆå§‹åŒ– ----------
def get_db(): 
    return firestore.Client()

client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ---------- Token è™•ç† ----------
def load_tokens(db):
    pos, neg = [], []
    for d in db.collection(TOKENS_COLLECTION).stream():
        data = d.to_dict() or {}
        t = Token(
            polarity=data.get("polarity", ""),
            ttype=data.get("type", "substr"),
            pattern=data.get("pattern", ""),
            weight=float(data.get("weight", 1.0)),
            note=data.get("note", "")
        )
        if t.polarity.lower() == "positive":
            pos.append(t)
        elif t.polarity.lower() == "negative":
            neg.append(t)
    return pos, neg

# ---------- æ‰“åˆ† ----------
def score_text(text: str, pos_tokens, neg_tokens) -> MatchResult:
    text_norm = normalize(text)
    score, hits = 0.0, []
    seen_patterns = set()
    for t in pos_tokens + neg_tokens:
        if t.pattern in seen_patterns:
            continue
        w = t.weight if t.polarity == "positive" else -abs(t.weight)
        matched = re.search(t.pattern, text_norm, re.I) if t.ttype == "regex" else t.pattern.lower() in text_norm
        if matched:
            seen_patterns.add(t.pattern)
            hits.append((t.pattern, w, t.note))
            score += w
    return MatchResult(score, hits)

# ---------- Groq åˆ†æ‰¹åˆ†æ ----------
def groq_analyze(news_list, target):
    results = []
    for i in range(0, len(news_list), BATCH_SIZE):
        batch = news_list[i:i+BATCH_SIZE]
        text_block = "\n".join([f"{j+1}. {n}" for j, n in enumerate(batch)])
        prompt = f"""ä½ æ˜¯ä¸€ä½å°è‚¡åˆ†æå¸«ã€‚æ ¹æ“šä»¥ä¸‹{target}ç›¸é—œæ–°èï¼Œè«‹åˆ¤æ–·æ˜æ—¥{target}è‚¡åƒ¹èµ°å‹¢ï¼š
è«‹ä»¥ä»¥ä¸‹ä¸‰ç¨®å…¶ä¸€å›ç­”ï¼š
ã€Œåå‘ä¸Šæ¼² ğŸ”¼ã€ã€Œåå‘ä¸‹è·Œ ğŸ”½ã€ã€ŒæŒå¹³ âš–ï¸ã€
ä¸¦ç°¡è¿°åŸå› ï¼ˆ40å­—å…§ï¼‰ã€‚

{text_block}
"""
        try:
            resp = client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­è‚¡å¸‚åˆ†æå¸«ï¼Œå›ç­”ç°¡æ½”æº–ç¢ºã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                max_tokens=120,
            )
            ans = resp.choices[0].message.content.strip()
            ans = re.sub(r"\s+", " ", ans)
            ans = re.sub(r"ä¸æ˜ç¢º.*", "æŒå¹³ âš–ï¸", ans)
            results.append(ans)
        except Exception as e:
            results.append(f"æŒå¹³ âš–ï¸ï¼ˆGroqåˆ†æå¤±æ•—ï¼š{e}ï¼‰")

    # å°‡æ‰€æœ‰æ‰¹æ¬¡çš„åˆ¤æ–·æ•´åˆç‚ºæœ€çµ‚çµæœï¼ˆä»¥å¤šæ•¸æ±ºï¼‰
    up = sum("ä¸Šæ¼²" in r for r in results)
    down = sum("ä¸‹è·Œ" in r for r in results)
    flat = sum("æŒå¹³" in r for r in results)
    if up > down and up > flat:
        final = "åå‘ä¸Šæ¼² ğŸ”¼"
    elif down > up and down > flat:
        final = "åå‘ä¸‹è·Œ ğŸ”½"
    else:
        final = "æŒå¹³ âš–ï¸"
    reason = results[-1] if results else "ç„¡åˆ†æçµæœ"
    return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š{final}\nåŸå› ï¼š{reason}"

# ---------- ä¸»åˆ†æ ----------
def analyze_target(db, collection, target, result_field):
    pos, neg = load_tokens(db)
    items = db.collection(collection).stream()
    now = datetime.now(TAIWAN_TZ)
    start = now - timedelta(days=LOOKBACK_DAYS)

    output_lines, groq_inputs, filtered = [], [], []

    for d in items:
        t = parse_docid_time(d.id)
        if not t or t < start:
            continue
        data = d.to_dict() or {}
        for k, v in data.items():
            if not isinstance(v, dict):
                continue
            title, content = v.get("title", ""), v.get("content", "")
            full = title + " " + content
            res = score_text(full, pos, neg)
            if abs(res.score) < SCORE_THRESHOLD or not res.hits:
                continue
            trend = "âœ… æ˜æ—¥å¯èƒ½å¤§æ¼²" if res.score > 0 else "âŒ æ˜æ—¥å¯èƒ½ä¸‹è·Œ"
            hit_lines = [f"  {'+' if w > 0 else '-'} {p}ï¼ˆ{n}ï¼‰" for p, w, n in res.hits]
            part = f"[{d.id}#{k}]\næ¨™é¡Œï¼š{first_n_sentences(title)}\n{trend}\nå‘½ä¸­ï¼š\n" + "\n".join(hit_lines)
            output_lines.append(part + "\n")
            groq_inputs.append(full)
            filtered.append((d.id, k, res))

    if not filtered:
        return f"{target}ï¼šæŒå¹³ âš–ï¸ï¼ˆç„¡æ˜é¡¯è®ŠåŒ–ï¼‰\n"

    groq_result = groq_analyze(groq_inputs, target)
    output = "\n".join(output_lines) + "\n" + groq_result + "\n"
    # Firestore å¯«å›
    for doc_id, key, res in filtered:
        try:
            db.collection(collection).document(doc_id).set({
                result_field: {
                    key: {
                        "summary": groq_result,
                        "trend": "ä¸Šæ¼²" if res.score > 0 else "ä¸‹è·Œ",
                        "reason": groq_result,
                        "hits": [{"pattern": p, "weight": w, "note": n} for p, w, n in res.hits],
                        "updated_at": datetime.now(TAIWAN_TZ).isoformat()
                    }
                }
            }, merge=True)
        except Exception as e:
            print(f"[warning] Firestore å¯«å›å¤±æ•— {doc_id}#{key}: {e}")
    return output

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    print("ğŸš€ é–‹å§‹åˆ†æå°è‚¡ç„¦é»è‚¡...\n")
    db = get_db()
    today = datetime.now(TAIWAN_TZ).strftime("%Y%m%d")
    os.makedirs("results", exist_ok=True)
    result_file = f"results/result_{today}.txt"

    results = []
    for i, (target, col, field) in enumerate([
        ("å°ç©é›»", NEWS_COLLECTION_TSMC, "Groq_result"),
        ("é´»æµ·", NEWS_COLLECTION_FOX, "Groq_result_Foxxcon"),
        ("è¯é›»", NEWS_COLLECTION_UMC, "Groq_result_UMC"),
    ]):
        print(f"ğŸ“ˆ åˆ†æï¼š{target}")
        res = analyze_target(db, col, target, field)
        results.append(f"{res.strip()}\n")
        if i < 2:
            print("=" * 70)

    final_output = "\n" + ("=" * 70 + "\n").join(results)
    with open(result_file, "w", encoding="utf-8") as f:
        f.write(final_output)

if __name__ == "__main__":
    main()

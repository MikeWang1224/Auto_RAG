# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
æº–ç¢ºç‡æ¥µè‡´ç‰ˆï¼ˆçŸ­æœŸé æ¸¬ç‰¹åŒ–ï¼‰ - åŠ å…¥ Context-aware èª¿æ•´ç‰ˆ + èƒŒé›¢åµæ¸¬
âœ… åš´æ ¼ä¾æ“šæƒ…ç·’åˆ†æ•¸æ±ºç­–
âœ… æ•æ„Ÿè©åŠ æ¬Šï¼ˆæ³•èªª / è²¡å ± / æ–°å“ / åœå·¥ç­‰ï¼‰
âœ… æ”¯æ´ 3 æ—¥å»¶é²æ•ˆæ‡‰
âœ… Firestore å¯«å› + æœ¬åœ° result.txt
âœ… æ–°å¢å¥å‹åˆ¤æ–·ï¼Œé¿å…ã€Œé‡ç”³ï¼é æœŸå…§ã€èª¤åˆ¤ç‚ºåˆ©å¤š
âœ… æ–°å¢è‚¡åƒ¹æ¼²è·ŒæŠ“å–ï¼Œèˆ‡æ–°èä¸€èµ·é€ Groq åˆ†æ
âœ… æ–°å¢æ–°èé¢ vs è‚¡åƒ¹èƒŒé›¢åµæ¸¬
"""

import os, signal, regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

# ---------- è¨­å®š ----------
SILENT_MODE = True
TAIWAN_TZ = timezone(timedelta(hours=8))

TOKENS_COLLECTION = "bull_tokens"
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"

SENSITIVE_WORDS = {
    "æ³•èªª": 1.5, "è²¡å ±": 1.4, "æ–°å“": 1.3, "åˆä½œ": 1.3,
    "ä½µè³¼": 1.4, "æŠ•è³‡": 1.3, "åœå·¥": 1.6, "ä¸‹ä¿®": 1.5,
    "åˆ©ç©º": 1.5, "çˆ†æ–™": 1.4, "ç‡Ÿæ”¶": 1.3, "å±•æœ›": 1.2,
}

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

if os.path.exists(".env"):
    load_dotenv(".env", override=True)
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ---------- çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
def get_db():
    return firestore.Client()

def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def first_n_sentences(text: str, n: int = 3) -> str:
    if not text:
        return ""
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    return "".join(parts[:n]) + ("..." if len(parts) > n else "")

def parse_docid_time(doc_id: str):
    m = re.match(r"^(?P<ymd>\d{8})(?:_(?P<hms>\d{6}))?$", doc_id or "")
    if not m: return None
    ymd, hms = m.group("ymd"), m.group("hms") or "000000"
    try:
        return datetime.strptime(ymd + hms, "%Y%m%d%H%M%S").replace(tzinfo=TAIWAN_TZ)
    except: return None

# ---------- Token ----------
def load_tokens(db):
    pos, neg = [], []
    for d in db.collection(TOKENS_COLLECTION).stream():
        data = d.to_dict() or {}
        pol = data.get("polarity", "").lower()
        ttype = data.get("type", "substr").lower()
        patt = data.get("pattern", "")
        note = data.get("note", "")
        w = float(data.get("weight", 1.0))
        if pol == "positive": pos.append(Token(pol, ttype, patt, w, note))
        elif pol == "negative": neg.append(Token(pol, ttype, patt, -abs(w), note))
    return pos, neg

def compile_tokens(tokens: List[Token]):
    compiled = []
    for t in tokens:
        if t.ttype == "regex":
            try: compiled.append(("regex", re.compile(t.pattern, re.I), t.weight, t.note, t.pattern))
            except: continue
        else: compiled.append(("substr", None, t.weight, t.note, t.pattern.lower()))
    return compiled

# ---------- Scoring ----------
def score_text(text: str, pos_c, neg_c, target: str = None) -> MatchResult:
    norm = normalize(text)
    score, hits, seen = 0.0, [], set()
    aliases = {"å°ç©é›»":["å°ç©é›»","tsmc","2330"], "é´»æµ·":["é´»æµ·","foxconn","2317","å¯Œå£«åº·"], "è¯é›»":["è¯é›»","umc","2303"]}
    company_pattern = "|".join(re.escape(a) for a in aliases.get(target, []))
    if not re.search(company_pattern, norm): return MatchResult(0.0, [])
    for ttype, cre, w, note, patt in pos_c + neg_c:
        key = (patt, note)
        if key in seen: continue
        matched = cre.search(norm) if ttype == "regex" else patt in norm
        if matched:
            score += w
            hits.append((patt, w, note))
            seen.add(key)
    return MatchResult(score, hits)

# ---------- Context-aware èª¿æ•´ ----------
def adjust_score_for_context(text: str, base_score: float) -> float:
    if not text or base_score == 0: return base_score
    norm = text.lower()
    neutral_phrases = ["é‡ç”³","ç¬¦åˆé æœŸ","é æœŸå…§","ä¸­æ€§çœ‹å¾…","ç„¡é‡å¤§å½±éŸ¿","æŒå¹³","æœªè®Š"]
    if any(p in norm for p in neutral_phrases): base_score *= 0.4
    positive_boost = ["å‰µæ–°é«˜","å€å¢","å¤§å¹…æˆé•·","ç²åˆ©æš´å¢","å ±å–œ"]
    negative_boost = ["æš´è·Œ","ä¸‹æ»‘","è™§æ","åœå·¥","ä¸‹ä¿®","è£å“¡","è­¦è¨Š"]
    if any(p in norm for p in positive_boost): base_score *= 1.3
    if any(p in norm for p in negative_boost): base_score *= 1.3
    return base_score

# ---------- èƒŒé›¢åµæ¸¬ ----------
def detect_divergence(avg_score: float, top_news: List[Tuple[str,str,str,float,float,str]]) -> str:
    price_moves = []
    for _, _, _, res_score, _, pc in top_news:
        m = re.search(r"([+-]?\d+\.?\d*)", str(pc))
        if m: price_moves.append(float(m.group(1)))
    if not price_moves: return "ç„¡è¶³å¤ è‚¡åƒ¹è³‡æ–™åˆ¤æ–·èƒŒé›¢ã€‚"
    avg_price_move = sum(price_moves)/len(price_moves)
    if avg_score > 0.5 and avg_price_move < 0:
        return "æ–°èåå¤šä½†è‚¡åƒ¹ä¸‹è·Œï¼ŒçŸ­ç·šå¯èƒ½åå½ˆï¼ˆæ­£å‘èƒŒé›¢ï¼‰ã€‚"
    elif avg_score < -0.5 and avg_price_move > 0:
        return "æ–°èåç©ºä½†è‚¡åƒ¹ä¸Šæ¼²ï¼ŒçŸ­ç·šå¯èƒ½å›æª”ï¼ˆè² å‘èƒŒé›¢ï¼‰ã€‚"
    else:
        return "è‚¡åƒ¹èµ°å‹¢èˆ‡æ–°èæƒ…ç·’ä¸€è‡´ï¼Œç„¡æ˜é¡¯èƒŒé›¢ã€‚"

# ---------- Groq ----------
def groq_analyze(news_list, target, avg_score, divergence_note=None):
    if not news_list:
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš–ï¸\nåŸå› ï¼šè¿‘ä¸‰æ—¥ç„¡ç›¸é—œæ–°è\næƒ…ç·’åˆ†æ•¸ï¼š0"
    combined = "\n".join(f"{i+1}. ({s:+.2f}) {t}" for i, (t,s) in enumerate(news_list))
    divergence_text = f"\næ­¤å¤–ï¼ŒèƒŒé›¢åˆ¤æ–·ï¼š{divergence_note}" if divergence_note else ""
    prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å°è‚¡é‡‘èåˆ†æå¸«ï¼Œè«‹æ ¹æ“šä»¥ä¸‹ã€Œ{target}ã€è¿‘ä¸‰æ—¥æ–°èæ‘˜è¦ï¼Œ
ä¾æƒ…ç·’åˆ†æ•¸èˆ‡å…§å®¹è¶¨å‹¢ï¼Œ**åš´æ ¼æ¨è«–æ˜æ—¥è‚¡åƒ¹æ–¹å‘**ã€‚
ç„¡è«–çµæœç‚ºä½•ï¼Œéƒ½å¿…é ˆæ˜ç¢ºèªªæ˜ã€ŒåŸå› ã€ã€‚

åˆ†æè¦å‰‡å¦‚ä¸‹ï¼š
1ï¸âƒ£ æƒ…ç·’åˆ†æ•¸ç‚ºæ¯å‰‡æ–°èçš„åˆ©å¤š / åˆ©ç©ºåŠ æ¬Šå€¼ï¼ˆæ‹¬è™Ÿä¸­ï¼‰ã€‚
2ï¸âƒ£ å¹³å‡å¾Œå¾—æ•´é«”æƒ…ç·’åˆ†æ•¸ï¼ˆç¯„åœ -10 ~ +10ï¼‰ã€‚
3ï¸âƒ£ è«‹æ ¹æ“šä»¥ä¸‹é‚è¼¯åˆ¤å®šæ–¹å‘ï¼š
   åˆ†æ•¸ â‰¥ +2 â†’ ä¸Šæ¼² ğŸ”¼
   +0.5 â‰¤ åˆ†æ•¸ < +2 â†’ å¾®æ¼² â†—ï¸
   -0.5 < åˆ†æ•¸ < +0.5 â†’ ä¸æ˜ç¢º âš–ï¸
   -2 < åˆ†æ•¸ â‰¤ -0.5 â†’ å¾®è·Œ â†˜ï¸
   åˆ†æ•¸ â‰¤ -2 â†’ ä¸‹è·Œ ğŸ”½
4ï¸âƒ£ è«‹åŒæ™‚ç´å…¥ã€ŒèƒŒé›¢åˆ¤æ–·ã€å°è‚¡åƒ¹å¯èƒ½å½±éŸ¿çš„èªªæ˜{divergence_text}

è«‹ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”ï¼Œæ‰€æœ‰æ¬„ä½å¿…é ˆå‡ºç¾ï¼š
æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š{{ä¸Šæ¼²ï¼å¾®æ¼²ï¼å¾®è·Œï¼ä¸‹è·Œï¼ä¸æ˜ç¢º}}ï¼ˆé™„ç¬¦è™Ÿï¼‰
åŸå› ï¼š{{ä¸€å¥ 40 å­—å…§ï¼Œèªªæ˜ä¸»è¦æƒ…ç·’ä¾†æºèˆ‡èƒŒé›¢è¨Šè™Ÿ}}
æƒ…ç·’åˆ†æ•¸ï¼š{{æ•´æ•¸ -10~+10}}

æ•´é«”å¹³å‡æƒ…ç·’åˆ†æ•¸ï¼š{avg_score:+.2f}
ä»¥ä¸‹æ˜¯æ–°èæ‘˜è¦ï¼ˆå«åˆ†æ•¸ï¼‰ï¼š
{combined}
"""
    try:
        resp = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content":"ä½ æ˜¯å°è‚¡é‡åŒ–åˆ†æå“¡ï¼Œéœ€æ ¹æ“šæƒ…ç·’åˆ†æ•¸è¦å‰‡ç”¢ç”Ÿæ˜ç¢ºçµè«–ã€‚"},
                {"role": "user", "content":prompt},
            ],
            temperature=0.15,
            max_tokens=220,
        )
        ans = re.sub(r"\s+", " ", resp.choices[0].message.content.strip())
        m_trend = re.search(r"(ä¸Šæ¼²|å¾®æ¼²|å¾®è·Œ|ä¸‹è·Œ|ä¸æ˜ç¢º)", ans)
        trend = m_trend.group(1) if m_trend else "ä¸æ˜ç¢º"
        symbol_map = {"ä¸Šæ¼²":"ğŸ”¼","å¾®æ¼²":"â†—ï¸","å¾®è·Œ":"â†˜ï¸","ä¸‹è·Œ":"ğŸ”½","ä¸æ˜ç¢º":"âš–ï¸"}

        # âš¡ å– Groq åŸå› ï¼Œè‹¥æ‰¾ä¸åˆ°å°±ç”¨å‰ 40 å­—
        m_reason = re.search(r"(?:åŸå› |ç†ç”±)[:ï¼š]\s*(.*?)(?=\s*(æƒ…ç·’åˆ†æ•¸[:ï¼š]|æ•´é«”å¹³å‡æƒ…ç·’åˆ†æ•¸[:ï¼š]|$))",ans,flags=re.DOTALL)
        reason = m_reason.group(1).strip() if m_reason else ""


        m_score = re.search(r"æƒ…ç·’åˆ†æ•¸[:ï¼š]?\s*(-?\d+)", ans)
        mood_score = int(m_score.group(1)) if m_score else max(-10,min(10,int(round(avg_score*3))))
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š{trend} {symbol_map.get(trend,'')}\nåŸå› ï¼š{reason}\næƒ…ç·’åˆ†æ•¸ï¼š{mood_score:+d}"
    except Exception as e:
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šæŒå¹³ âš–ï¸\nåŸå› ï¼šGroqåˆ†æå¤±æ•—({e})\næƒ…ç·’åˆ†æ•¸ï¼š0"

# ---------- ä¸»åˆ†æ ----------
def analyze_target(db, collection, target, result_field):
    pos, neg = load_tokens(db)
    pos_c, neg_c = compile_tokens(pos), compile_tokens(neg)
    today = datetime.now(TAIWAN_TZ).date()

    filtered = []
    for d in db.collection(collection).stream():
        dt = parse_docid_time(d.id)
        if not dt: continue
        delta_days = (today - dt.date()).days
        if delta_days>2: continue
        day_weight = 1.0 if delta_days==0 else 0.85 if delta_days==1 else 0.7
        data = d.to_dict() or {}

        for k,v in data.items():
            if not isinstance(v, dict): continue
            title, content = v.get("title",""), v.get("content","")
            price_change = v.get("price_change","")
            full = f"{title} {content} è‚¡åƒ¹è®Šå‹•ï¼š{price_change}"
            res = score_text(full,pos_c,neg_c,target)
            if not res.hits: continue
            adj_score = adjust_score_for_context(full,res.score)
            token_weight = 1.0 + min(len(res.hits)*0.05,0.3)
            impact = 1.0 + sum(w*0.05 for k_sens,w in SENSITIVE_WORDS.items() if k_sens in full)
            total_weight = day_weight*token_weight*impact
            filtered.append((d.id,k,title,res,total_weight,price_change))

    if not filtered:
        print(f"{target}ï¼šè¿‘ä¸‰æ—¥ç„¡æ–°èï¼Œäº¤ç”± Groq åˆ¤æ–·ã€‚\n")
        summary = groq_analyze([],target,0)
    else:
        filtered.sort(key=lambda x: abs(x[3].score*x[4]),reverse=True)
        top_news = filtered[:10]
        print(f"\nğŸ“° {target} è¿‘æœŸé‡é»æ–°èï¼ˆå«è¡æ“Šï¼‰ï¼š")
        for docid,key,title,res,weight,price_change in top_news:
            impact = sum(w for k_sens,w in SENSITIVE_WORDS.items() if k_sens in title)
            print(f"[{docid}#{key}] ({weight:.2f}x, åˆ†æ•¸={res.score:+.2f}, è¡æ“Š={1+impact/10:.2f}) {title} | è‚¡åƒ¹è®Šå‹•ï¼š{price_change}")
            for p,w,n in res.hits: print(f"   {'+' if w>0 else '-'} {p}ï¼ˆ{n}ï¼‰")
        news_with_scores = [(f"{t} è‚¡åƒ¹è®Šå‹•ï¼š{pc}", res.score*weight) for _,_,t,res,weight,pc in top_news]
        avg_score = sum(s for _,s in news_with_scores)/len(news_with_scores)
        divergence_note = detect_divergence(avg_score, top_news)
        summary = groq_analyze(news_with_scores,target,avg_score, divergence_note)

        fname = f"result_{today.strftime('%Y%m%d')}.txt"
        with open(fname,"a",encoding="utf-8") as f:
            f.write(f"======= {target} =======\n")
            for docid,key,title,res,weight,price_change in top_news:
                hits_text = "\n".join([f"  {'+' if w>0 else '-'} {p}ï¼ˆ{n}ï¼‰" for p,w,n in res.hits])
                f.write(f"[{docid}#{key}]ï¼ˆ{weight:.2f}xï¼‰\næ¨™é¡Œï¼š{first_n_sentences(title)}\nè‚¡åƒ¹è®Šå‹•ï¼š{price_change}\nå‘½ä¸­ï¼š\n{hits_text}\n\n")
            f.write(summary+"\n\n")
    print(summary+"\n")

    try:
        db.collection(result_field).document(today.strftime("%Y%m%d")).set({
            "timestamp": datetime.now(TAIWAN_TZ).isoformat(),
            "result": summary,
        })
    except Exception as e:
        print(f"[warning] Firestore å¯«å›å¤±æ•—ï¼š{e}")

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    if not SILENT_MODE: print("ğŸš€ é–‹å§‹åˆ†æå°è‚¡ç„¦é»è‚¡ï¼ˆæº–ç¢ºç‡æ¥µè‡´ç‰ˆï¼‰...\n")
    db = get_db()
    analyze_target(db, NEWS_COLLECTION_TSMC,"å°ç©é›»","Groq_result")
    print("="*70)
    analyze_target(db, NEWS_COLLECTION_FOX,"é´»æµ·","Groq_result_Foxxcon")
    print("="*70)
    analyze_target(db, NEWS_COLLECTION_UMC,"è¯é›»","Groq_result_UMC")

if __name__=="__main__":
    main()

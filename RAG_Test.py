# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
æº–ç¢ºç‡æå‡ç‰ˆï¼ˆæƒ…ç·’èåˆ + å¤šå±¤æ¬Šé‡ + èªæ„è£œå„Ÿï¼‰
âœ… Firestore å¯«å› + æœ¬åœ° result.txt
âœ… Groq åŒæ™‚è€ƒæ…®æ¯å‰‡æƒ…ç·’åˆ†æ•¸ + å¹³å‡åˆ†æ•¸
âœ… å‘½ä¸­å¤šå‰‡æ–°èæ™‚æå‡ç©©å®šåº¦
âœ… æ–°å¢ï¼šæ”¯æ´ 3 å¤©å…§æ–°èï¼ˆå»¶é²æ•ˆæ‡‰ï¼‰
âœ… æ–°å¢ï¼šåªæŠ“ä¸€æ¬¡ä»Šæ—¥æ¼²è·Œï¼Œä¸¦å°‡å…¶ç´å…¥ Groq åˆ†æï¼Œæœ€å¾Œå¯«å…¥ result çš„åŸå› 
"""

import os
import signal
import regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Optional
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

# æ–°å¢ï¼šyfinance ç”¨æ–¼æŠ“è‚¡åƒ¹ï¼ˆåªæŠ“ä¸€æ¬¡ï¼‰
import yfinance as yf

# ---------- è¨­å®š ----------
SILENT_MODE = True
TAIWAN_TZ = timezone(timedelta(hours=8))
SCORE_THRESHOLD = 1.5

TOKENS_COLLECTION = "bull_tokens"
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"

# ticker å°ç…§è¡¨ï¼ˆyfinance ç”¨ï¼‰
TICKER_MAP = {
    "å°ç©é›»": "2330.TW",
    "é´»æµ·": "2317.TW",
    "è¯é›»": "2303.TW"
}

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- åˆå§‹åŒ– ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ---------- çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
DOCID_RE = re.compile(r"^(?P<ymd>\d{8})(?:_(?P<hms>\d{6}))?$")

def get_db():
    return firestore.Client()

def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def first_n_sentences(text: str, n: int = 3) -> str:
    if not text:
        return ""
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    return "".join(parts[:n]) + ("..." if len(parts) > n else "")

def parse_docid_time(doc_id: str):
    m = DOCID_RE.match(doc_id or "")
    if not m:
        return None
    ymd = m.group("ymd")
    hms = m.group("hms") or "000000"
    try:
        return datetime.strptime(ymd + hms, "%Y%m%d%H%M%S").replace(tzinfo=TAIWAN_TZ)
    except:
        return None

# ---------- Token ----------
def load_tokens(db) -> Tuple[List[Token], List[Token]]:
    pos, neg = [], []
    for d in db.collection(TOKENS_COLLECTION).stream():
        data = d.to_dict() or {}
        pol = data.get("polarity", "").lower()
        ttype = data.get("type", "substr").lower()
        patt = data.get("pattern", "")
        note = data.get("note", "")
        try:
            w = float(data.get("weight", 1.0))
        except:
            w = 1.0
        if pol == "positive":
            pos.append(Token(pol, ttype, patt, w, note))
        elif pol == "negative":
            neg.append(Token(pol, ttype, patt, -abs(w), note))
    return pos, neg

def compile_tokens(tokens: List[Token]):
    compiled = []
    for t in tokens:
        if t.ttype == "regex":
            try:
                compiled.append(("regex", re.compile(t.pattern, re.I), t.weight, t.note, t.pattern))
            except:
                continue
        else:
            compiled.append(("substr", None, t.weight, t.note, t.pattern.lower()))
    return compiled

# ---------- Scoring ----------
def score_text(text: str, pos_c, neg_c, target: str = None) -> MatchResult:
    norm = normalize(text)
    score, hits, seen = 0.0, [], set()
    aliases = {
        "å°ç©é›»": ["å°ç©é›»", "tsmc", "2330"],
        "é´»æµ·": ["é´»æµ·", "foxconn", "2317", "å¯Œå£«åº·"],
        "è¯é›»": ["è¯é›»", "umc", "2303"]
    }
    company_pattern = "|".join(re.escape(a) for a in aliases.get(target, []))
    if not re.search(company_pattern, norm):
        return MatchResult(0.0, [])
    for ttype, cre, w, note, patt in pos_c + neg_c:
        key = (patt, note)
        if key in seen:
            continue
        matched = cre.search(norm) if ttype == "regex" else patt in norm
        if matched:
            score += w
            hits.append((patt, w, note))
            seen.add(key)
    return MatchResult(score, hits)

# ---------- å–å¾—ä»Šæ—¥æ¼²è·Œï¼ˆåªæŠ“ä¸€æ¬¡ï¼‰ ----------
def get_today_change(ticker: str) -> Optional[str]:
    """
    å›å‚³å­—ä¸²æ ¼å¼: '+1.77%' æˆ– '-0.45%' æˆ– Noneï¼ˆæŠ“å–å¤±æ•—ï¼‰
    åªæŠ“æœ€è¿‘å…©æ—¥æ”¶ç›¤ï¼Œè¨ˆç®—ä»Šæ—¥ç›¸å°å‰ä¸€æ—¥çš„ç™¾åˆ†æ¯”
    """
    try:
        t = yf.Ticker(ticker)
        hist = t.history(period="2d")
        if hist is None or len(hist) < 2:
            return None
        prev_close = float(hist["Close"].iloc[-2])
        today_close = float(hist["Close"].iloc[-1])
        if prev_close == 0:
            return None
        pct = (today_close - prev_close) / prev_close * 100
        sign = "+" if pct >= 0 else ""
        return f"{sign}{pct:.2f}%"
    except Exception:
        return None

# ---------- Groqï¼ˆæƒ…ç·’èåˆ + æº–ç¢ºç‡å¼·åŒ–ï¼‰ ----------
def groq_analyze(news_list: List[Tuple[str, float]], target: str, avg_score: float, today_change: Optional[str]) -> str:
    """
    news_list: List of (title_or_summary, weighted_score)
    today_change: formatted string like '+1.77%' or '-0.45%' or None
    å›å‚³å®Œæ•´ summary å­—ä¸²ï¼ˆåŒ…å«èµ°å‹¢ã€åŸå› ã€æƒ…ç·’åˆ†æ•¸ï¼‰ï¼Œä¸”åŸå› è¡ŒæœƒåŒ…å«ä»Šæ—¥æ¼²è·Œ
    """
    if not news_list:
        base = f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš–ï¸\nåŸå› ï¼šè¿‘ä¸‰æ—¥ç„¡ç›¸é—œæ–°è\næƒ…ç·’åˆ†æ•¸ï¼š0"
        if today_change:
            # æŠŠä»Šæ—¥æ¼²è·Œå¯«å…¥åŸå› 
            base = re.sub(r"(åŸå› ï¼š)(.*)", r"\1\2ï¼ˆä»Šæ—¥æ¼²è·Œï¼š" + today_change + "ï¼‰", base)
        return base

    # å°‡æ–°èå…§å®¹èˆ‡åˆ†æ•¸æ•´åˆ
    combined = "\n".join(f"{i+1}. ({s:+.2f}) {t}" for i, (t, s) in enumerate(news_list))

    # å°‡ today_change å‚³çµ¦æ¨¡å‹ï¼Œä¸¦è¦æ±‚åœ¨åŸå› ä¸­æåŠ
    tc_display = today_change if today_change else "ç„¡å¯ç”¨è³‡æ–™"

    prompt_text = f"""
ä½ æ˜¯ä¸€ä½é‡‘èæ–°èåˆ†æå“¡ã€‚
è«‹é–±è®€ä»¥ä¸‹é—œæ–¼ã€Œ{target}ã€æœ€è¿‘ä¸‰å¤©çš„æ–°èæ‘˜è¦ï¼Œ
ä»¥ã€Œæƒ…ç·’èåˆæ¨¡å¼ã€é€²è¡Œæƒ…ç·’ç¸½çµèˆ‡èµ°å‹¢é æ¸¬ï¼š

1. ç¶œåˆæ–°èä¸­çš„åˆ©å¤šèˆ‡åˆ©ç©ºæƒ…ç·’ï¼Œçµ¦å‡ºæ•´é«”æƒ…ç·’åˆ†æ•¸ï¼ˆ-10 ~ +10ï¼‰ã€‚
2. è‹¥åˆ©å¤šèˆ‡åˆ©ç©ºå‹¢å‡åŠ›æ•µï¼Œè«‹å›ç­”ã€Œä¸æ˜ç¢º âš–ï¸ã€ã€‚
3. è‹¥åˆ©å¤šæƒ…ç·’æ˜é¡¯ä½”å„ªï¼ˆ> +2ï¼‰ï¼Œè«‹å›ç­”ã€Œä¸Šæ¼² ğŸ”¼ã€ã€‚
4. è‹¥åˆ©ç©ºæƒ…ç·’æ˜é¡¯ä½”å„ªï¼ˆ< -2ï¼‰ï¼Œè«‹å›ç­”ã€Œä¸‹è·Œ ğŸ”½ã€ã€‚
5. é™„ä¸Šç°¡çŸ­åŸå› ï¼ˆ40 å­—å…§ï¼‰ï¼Œèªªæ˜ä¸»å°æƒ…ç·’çš„ä¸»è¦å› ç´ ï¼Œ**ä¸¦åœ¨ç†ç”±å…§æ˜ç¢ºæåˆ°å¸‚å ´å¯¦éš›åæ‡‰ï¼šä»Šæ—¥æ¼²è·Œ {tc_display}ã€‚**

æ•´é«”å¹³å‡æƒ…ç·’åˆ†æ•¸ç‚º {avg_score:+.2f}ã€‚

ä»¥ä¸‹æ˜¯æ–°èæ‘˜è¦ï¼ˆå«æƒ…ç·’åˆ†æ•¸ï¼‰ï¼š
{combined}

è«‹è¼¸å‡ºæ ¼å¼å¦‚ä¸‹ï¼š
æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š{{ä¸Šæ¼²ï¼ä¸‹è·Œï¼ä¸æ˜ç¢º}}ï¼ˆé™„ç¬¦è™Ÿï¼‰
åŸå› ï¼š{{ä¸€å¥ç¸½çµç†ç”±ï¼ˆè«‹åŒ…å«ã€Œä»Šæ—¥æ¼²è·Œã€ï¼‰}}
æƒ…ç·’åˆ†æ•¸ï¼š{{æ•´æ•¸ï¼ˆ-10~10ï¼‰}}
"""

    try:
        resp = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­å°è‚¡åˆ†æå¸«ï¼Œéœ€ç¶œåˆæƒ…ç·’èˆ‡å¸‚å ´åæ‡‰åšå‡ºåˆ¤æ–·ã€‚"},
                {"role": "user", "content": prompt_text},
            ],
            temperature=0.2,
            max_tokens=300,
            timeout=25,
        )
        ans = resp.choices[0].message.content.strip()
        ans = re.sub(r"\s+", " ", ans)

        # æå–èµ°å‹¢
        m_trend = re.search(r"(ä¸Šæ¼²|ä¸‹è·Œ|ä¸æ˜ç¢º|å¾®æ¼²|å¾®è·Œ)", ans)
        trend = m_trend.group(1) if m_trend else "ä¸æ˜ç¢º"
        symbol_map = {"ä¸Šæ¼²": "ğŸ”¼", "å¾®æ¼²": "â†—ï¸", "å¾®è·Œ": "â†˜ï¸", "ä¸‹è·Œ": "ğŸ”½", "ä¸æ˜ç¢º": "âš–ï¸"}

        # æå–ç†ç”±ï¼ˆç›¡é‡æŠ“ã€ŒåŸå› ï¼š...ã€ï¼‰
        m_reason = re.search(r"(?:åŸå› |ç†ç”±)[:ï¼š]?\s*(.+?)(?:æƒ…ç·’åˆ†æ•¸|æƒ…ç·’|æƒ…ç·’åˆ†|$)", ans)
        reason = m_reason.group(1).strip() if m_reason else "å¸‚å ´è§€æœ›"

        # ç¢ºä¿åŸå› å…§åŒ…å«ä»Šæ—¥æ¼²è·Œï¼ˆè‹¥æä¾›äº†ï¼‰
        if today_change and today_change not in reason:
            # åœ¨åŸå› å¾Œè£œä¸Šï¼ˆä»Šæ—¥æ¼²è·Œï¼š...ï¼‰
            if reason.endswith("ã€‚"):
                reason = reason[:-1]
            reason = f"{reason}ï¼ˆä»Šæ—¥æ¼²è·Œï¼š{today_change}ï¼‰"

        # æå–æƒ…ç·’åˆ†æ•¸
        m_score = re.search(r"æƒ…ç·’åˆ†æ•¸[:ï¼š]?\s*(-?\d+)", ans)
        mood_score = int(m_score.group(1)) if m_score else int(round(avg_score))

        # å»ºç«‹æœ€çµ‚è¼¸å‡º
        final = f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š{trend} {symbol_map.get(trend,'')}\nåŸå› ï¼š{reason}\næƒ…ç·’åˆ†æ•¸ï¼š{mood_score:+d}"
        return final

    except Exception as e:
        # è‹¥ Groq å¤±æ•—ï¼Œä»å›å‚³åŸºæœ¬æ ¼å¼ä¸¦é™„ä¸Šä»Šæ—¥æ¼²è·Œè³‡è¨Š
        reason = "Groqåˆ†æå¤±æ•—ï¼Œæ”¹ç‚ºåŸºæ–¼æƒ…ç·’åˆ†æ•¸èˆ‡å¸‚å ´åæ‡‰ç°¡æ˜“åˆ¤æ–·"
        if today_change:
            reason = f"{reason}ï¼ˆä»Šæ—¥æ¼²è·Œï¼š{today_change}ï¼‰"
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šæŒå¹³ âš–ï¸\nåŸå› ï¼š{reason}\næƒ…ç·’åˆ†æ•¸ï¼š{int(round(avg_score)):+d}"

# ---------- ä¸»åˆ†æ ----------
def analyze_target(db, collection: str, target: str, result_field: str, today_change: Optional[str]):
    pos, neg = load_tokens(db)
    pos_c, neg_c = compile_tokens(pos), compile_tokens(neg)

    today = datetime.now(TAIWAN_TZ).date()
    filtered, weighted_scores = [], []

    for d in db.collection(collection).stream():
        dt = parse_docid_time(d.id)
        if not dt:
            continue
        news_date = dt.date()
        delta_days = (today - news_date).days

        # å»¶é•·æ™‚é–“çª—ï¼ˆæ”¯æ´ 1~2 å¤©å»¶é²æ•ˆæ‡‰ï¼Œæœ€å¤šå– 3 å¤©å…§ï¼‰
        if delta_days > 2:
            continue

        # æ ¹æ“šæ™‚é–“çµ¦ä¸åŒæ¬Šé‡ï¼ˆè¶Šä¹…å½±éŸ¿è¶Šå¼±ï¼‰
        if delta_days == 0:
            day_weight = 1.0   # ä»Šæ—¥æ–°èæ¬Šé‡æœ€é«˜
        elif delta_days == 1:
            day_weight = 0.85  # æ˜¨æ—¥ç¨å¼±
        else:
            day_weight = 0.7   # å‰å¤©å†å¼±ä¸€äº›

        data = d.to_dict() or {}
        for k, v in data.items():
            if not isinstance(v, dict):
                continue
            title, content = v.get("title", ""), v.get("content", "")
            full = title + " " + content
            res = score_text(full, pos_c, neg_c, target)
            if not res.hits:
                continue

            token_weight = 1.0 + min(len(res.hits) * 0.05, 0.3)
            total_weight = day_weight * token_weight

            filtered.append((d.id, k, title, res, total_weight))
            weighted_scores.append(res.score * total_weight)

    if not filtered:
        print(f"{target}ï¼šè¿‘ä¸‰æ—¥ç„¡æ–°èï¼Œäº¤ç”± Groq åˆ¤æ–·ã€‚\n")
        summary = groq_analyze([], target, 0, today_change)
    else:
        filtered.sort(key=lambda x: abs(x[3].score * x[4]), reverse=True)
        top_news = filtered[:10]

        print(f"\nğŸ“° {target} è¿‘æœŸé‡é»æ–°èï¼š")
        for docid, key, title, res, weight in top_news:
            print(f"[{docid}#{key}] ({weight:.2f}x, åˆ†æ•¸={res.score:+.2f}) {title}")
            for p, w, n in res.hits:
                print(f"   {'+' if w>0 else '-'} {p}ï¼ˆ{n}ï¼‰")

        news_with_scores = [(t, res.score * weight) for _, _, t, res, weight in top_news]
        avg_score = sum(s for _, s in news_with_scores) / len(news_with_scores)
        summary = groq_analyze(news_with_scores, target, avg_score, today_change)

        fname = f"result_{today.strftime('%Y%m%d')}.txt"
        with open(fname, "a", encoding="utf-8") as f:
            f.write(f"======= {target} =======\n")
            for docid, key, title, res, weight in top_news:
                hits_text = "\n".join([f"  {'+' if w>0 else '-'} {p}ï¼ˆ{n}ï¼‰" for p, w, n in res.hits])
                f.write(f"[{docid}#{key}]ï¼ˆ{weight:.2f}xï¼‰\næ¨™é¡Œï¼š{first_n_sentences(title)}\nå‘½ä¸­ï¼š\n{hits_text}\n\n")
            f.write(summary + "\n\n")

    print(summary + "\n")

    # å¯«å› Firestoreï¼ˆresult æ¬„ä½å…§çš„åŸå› å·²åŒ…å«ä»Šæ—¥æ¼²è·Œï¼‰
    try:
        db.collection(result_field).document(today.strftime("%Y%m%d")).set({
            "timestamp": datetime.now(TAIWAN_TZ).isoformat(),
            "result": summary,
        })
    except Exception as e:
        print(f"[warning] Firestore å¯«å›å¤±æ•—ï¼š{e}")

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    if not SILENT_MODE:
        print("ğŸš€ é–‹å§‹åˆ†æå°è‚¡ç„¦é»è‚¡ï¼ˆæº–ç¢ºç‡æå‡ç‰ˆï¼‰...\n")

    db = get_db()

    # å…ˆä¸€æ¬¡æŠ“ä»Šæ—¥æ¼²è·Œï¼ˆæ¯å®¶å…¬å¸åªæŠ“ä¸€æ¬¡ï¼‰
    tsmc_change = get_today_change(TICKER_MAP["å°ç©é›»"])
    foxconn_change = get_today_change(TICKER_MAP["é´»æµ·"])
    umc_change = get_today_change(TICKER_MAP["è¯é›»"])

    # è‹¥æŠ“å–å¤±æ•—ï¼Œæœƒå‚³ Noneï¼Œå¾ŒçºŒ groq_analyze æœƒè™•ç†
    analyze_target(db, NEWS_COLLECTION_TSMC, "å°ç©é›»", "Groq_result", tsmc_change)
    print("=" * 70)
    analyze_target(db, NEWS_COLLECTION_FOX, "é´»æµ·", "Groq_result_Foxxcon", foxconn_change)
    print("=" * 70)
    analyze_target(db, NEWS_COLLECTION_UMC, "è¯é›»", "Groq_result_UMC", umc_change)

if __name__ == "__main__":
    main()

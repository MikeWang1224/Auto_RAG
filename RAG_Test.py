# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æï¼ˆå¤šå…¬å¸ç‰ˆï¼‰
âœ… è¼¸å‡ºé¡¯ç¤ºåœ¨çµ‚ç«¯
âœ… åŒæ­¥å¯«å› Firebase
âœ… è‡ªå‹•å„²å­˜çµæœæ–¼ results/ ä¸‹
"""

import os
import signal
import regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

# ---------- å…¨åŸŸè¨­å®š ----------
SILENT_MODE = False
MAX_DISPLAY_NEWS = 5
TAIWAN_TZ = timezone(timedelta(hours=8))
STOP = False

# ---------- è®€å–ç’°å¢ƒè®Šæ•¸ ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)

PROJECT_ID = os.getenv("FIREBASE_PROJECT")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

TOKENS_COLLECTION = "bull_tokens"
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"
SCORE_THRESHOLD = 0.5
LOOKBACK_DAYS = 2

# ---------- Ctrl+C å®‰å…¨åœæ­¢ ----------
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\nâš ï¸ åµæ¸¬åˆ° Ctrl+Cï¼Œåœæ­¢ä¸­â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- è³‡æ–™çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: list[tuple[str, float, str]]

# ---------- å·¥å…· ----------
def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def first_n_sentences(text: str, n: int = 3) -> str:
    if not text:
        return ""
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    parts = [p for p in parts if p.strip()]
    joined = "".join(parts[:n])
    if not re.search(r'[ã€‚\.ï¼!\?ï¼Ÿï¼›;]$', joined):
        joined += "ã€‚"
    return joined

# ---------- åˆå§‹åŒ– ----------
def init_firestore():
    return firestore.Client(project=PROJECT_ID)

def init_groq():
    return Groq(api_key=GROQ_API_KEY)

# ---------- è¼‰å…¥ Token ----------
def load_tokens(db, collection: str):
    tokens = []
    for d in db.collection(collection).stream():
        t = d.to_dict()
        tokens.append(Token(
            polarity=t.get("polarity", ""),
            ttype=t.get("type", ""),
            pattern=t.get("pattern", ""),
            weight=float(t.get("weight", 1.0)),
            note=t.get("note", "")
        ))
    return tokens

# ---------- è©•åˆ† ----------
def score_text(text: str, pos_tokens, neg_tokens, target: str):
    text_norm = normalize(text)
    total = 0.0
    hits = []
    for tok in pos_tokens + neg_tokens:
        found = False
        if tok.ttype == "substr" and tok.pattern in text_norm:
            found = True
        elif tok.ttype == "regex" and re.search(tok.pattern, text_norm):
            found = True
        if found:
            w = tok.weight if tok.polarity == "positive" else -tok.weight
            total += w
            hits.append((tok.pattern, w, tok.note))
    return MatchResult(score=total, hits=hits)

# ---------- Firestore å¯«å…¥ ----------
def write_result(db, collection, doc_id, data):
    ref = db.collection(collection).document(doc_id)
    ref.set(data, merge=True)

# ---------- ä¸»åˆ†æå‡½æ•¸ ----------
def analyze_target(db, news_collection, target_name, result_collection, force_dir=False):
    pos_tokens = load_tokens(db, TOKENS_COLLECTION)
    neg_tokens = [t for t in pos_tokens if t.polarity == "negative"]
    pos_tokens = [t for t in pos_tokens if t.polarity == "positive"]

    now = datetime.now(TAIWAN_TZ)
    since = now - timedelta(days=LOOKBACK_DAYS)
    news_docs = list(db.collection(news_collection).stream())

    terminal_logs = []
    for doc in news_docs:
        if STOP:
            break
        it = doc.to_dict()
        it["id"] = doc.id
        text = it.get("content") or it.get("title") or ""
        res = score_text(text, pos_tokens, neg_tokens, target_name)
        if abs(res.score) >= SCORE_THRESHOLD and res.hits:
            trend = "âœ… æ˜æ—¥å¯èƒ½å¤§æ¼²" if res.score > 0 else "âŒ æ˜æ—¥å¯èƒ½ä¸‹è·Œ"
            hits_text_lines = [
                f"  {'+' if w>0 else '-'} {patt}ï¼ˆ{note}ï¼‰" for patt, w, note in res.hits
            ]
            truncated_title = first_n_sentences(it.get("title", ""), 3)
            terminal_logs.append(
                f"""[{it['id']}]
æ¨™é¡Œï¼š{truncated_title}
{trend}
å‘½ä¸­ï¼š
""" + "\n".join(hits_text_lines) + "\n"
            )

    # è¼¸å‡ºçµæœæ–‡å­—
    if not terminal_logs:
        result_text = f"{target_name}ï¼šç„¡æ˜é¡¯è®ŠåŒ–"
    else:
        result_text = "\n".join(terminal_logs)
    print(result_text)

    # å¯«å› Firestore
    write_result(db, result_collection, now.strftime("%Y%m%d"), {
        "summary": result_text,
        "updated": now.isoformat(),
    })

    return result_text

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    os.makedirs("results", exist_ok=True)
    db = init_firestore()
    now = datetime.now(TAIWAN_TZ)

    print("ğŸš€ é–‹å§‹åˆ†æå°è‚¡ç„¦é»è‚¡...\n")

    all_results = []
    targets = [
        ("å°ç©é›»", NEWS_COLLECTION_TSMC, "Groq_result", False),
        ("é´»æµ·", NEWS_COLLECTION_FOX, "Groq_result_Foxxcon", True),
        ("è¯é›»", NEWS_COLLECTION_UMC, "Groq_result_UMC", True),
    ]

    for i, (target, col, result_col, force_dir) in enumerate(targets):
        print(f"ğŸ“ˆ åˆ†æï¼š{target}")
        result_text = analyze_target(db, col, target, result_col, force_dir)
        all_results.append(result_text)
        if i < len(targets) - 1:
            print("=" * 70)

    # å„²å­˜æ–‡å­—æª”
    file_path = f"results/result_{now.strftime('%Y%m%d')}.txt"
    with open(file_path, "w", encoding="utf-8") as f:
        f.write("\n\n".join(all_results))
    print(f"\nâœ… çµæœå·²å„²å­˜è‡³ï¼š{file_path}")

# ---------- åŸ·è¡Œ ----------
if __name__ == "__main__":
    main()

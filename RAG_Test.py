# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
æ”¹è‰¯ç‰ˆï¼šåŠ å¼·éŒ¯èª¤è™•ç†ã€ç’°å¢ƒæª¢æŸ¥ã€æ—¥èªŒèˆ‡è¼¸å‡ºç©©å®šæ€§
"""
import os
import signal
import regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Dict, Optional
from dotenv import load_dotenv

# load env once
load_dotenv()

# ---------- è¨­å®š ----------
SILENT_MODE = True            # è¨­ç‚º False å¯çœ‹åˆ°è©³ç´°æ—¥èªŒ
MAX_DISPLAY_NEWS = 5
TOKENS_COLLECTION = "bull_tokens"
NEWS_COLLECTION_TSMC = "NEWS_TSMC"
NEWS_COLLECTION_FOX = "NEWS_FOXCONN"
NEWS_COLLECTION_UMC = "NEWS_UMC"
SCORE_THRESHOLD = float(os.getenv("SCORE_THRESHOLD", "0.5"))
LOOKBACK_DAYS = int(os.getenv("LOOKBACK_DAYS", "2"))
TAIWAN_TZ = timezone(timedelta(hours=8))
RESULT_DIR = "result"

def log(msg: str):
    if not SILENT_MODE:
        print(msg)

# Ctrl+C å®‰å…¨åœæ­¢
STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
DOCID_RE = re.compile(r"^(?P<ymd>\d{8})(?:_(?P<hms>\d{6}))?$")

def normalize(text: Optional[str]) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def shorten_text(t: str, n=200):
    return t[:n] + "â€¦" if len(t) > n else t

def first_n_sentences(text: str, n: int = 3) -> str:
    if not text:
        return ""
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    parts = [p for p in parts if p.strip()]
    if not parts:
        return text.strip()
    joined = "".join(parts[:n])
    if not re.search(r'[ã€‚\.ï¼!\?ï¼Ÿï¼›;]\s*$', joined):
        joined += "..."
    return joined

def parse_docid_time(doc_id: str) -> Optional[datetime]:
    """æ”¯æ´ 20251018 æˆ– 20251018_064229 å…©ç¨®æ ¼å¼ï¼›å›å‚³å¸¶æ™‚å€çš„ datetime æˆ– None"""
    doc_id = (doc_id or "").strip()
    m = DOCID_RE.match(doc_id)
    if not m:
        return None
    ymd = m.group("ymd")
    hms = m.group("hms") or "000000"
    try:
        return datetime.strptime(ymd + hms, "%Y%m%d%H%M%S").replace(tzinfo=TAIWAN_TZ)
    except Exception:
        return None

# ---------- Firestore ç›¸é—œ ----------
def get_db():
    """å»¶å¾Œ import google.cloud.firestoreï¼Œä¸¦æ•æ‰æœªè¨­å®šæ†‘è­‰æƒ…æ³"""
    try:
        from google.cloud import firestore
    except Exception as e:
        raise RuntimeError("google-cloud-firestore æœªå®‰è£æˆ–ç„¡æ³•åŒ¯å…¥ï¼šè«‹ç¢ºèªç’°å¢ƒä¸¦å®‰è£ google-cloud-firestore") from e
    try:
        return firestore.Client()
    except Exception as e:
        raise RuntimeError("å»ºç«‹ Firestore client å¤±æ•—ï¼šè«‹ç¢ºèª GOOGLE_APPLICATION_CREDENTIALS æˆ– GCP ç’°å¢ƒè¨­å®š") from e

def load_tokens(db) -> Tuple[List[Token], List[Token]]:
    pos, neg = [], []
    try:
        for d in db.collection(TOKENS_COLLECTION).stream():
            data = d.to_dict() or {}
            pol = (data.get("type") or "").lower()
            ttype = (data.get("method") or "substr").lower()
            patt = str(data.get("pattern") or "")
            note = str(data.get("note") or "")
            try:
                w = float(data.get("weight", 1.0))
            except:
                w = 1.0
            if not patt or pol not in ("positive", "negative"):
                continue
            (pos if pol == "positive" else neg).append(Token(pol, ttype, patt, w, note))
    except Exception as e:
        log(f"[warn] è®€å– tokens å¤±æ•—ï¼š{e}")
    return pos, neg

def load_news_items(db, col_name: str, days: int) -> List[Dict]:
    """å¾ collection æ’ˆ news documentsï¼ˆæ–‡ä»¶å…§æ¯å€‹ field å¯èƒ½æ˜¯ä¸åŒä¾†æºï¼‰ï¼Œåªå–æœ€è¿‘ days å¤©"""
    items, seen = [], set()
    now = datetime.now(TAIWAN_TZ)
    start = now - timedelta(days=days)
    try:
        for d in db.collection(col_name).stream():
            dt = parse_docid_time(d.id)
            # è‹¥ doc id ç„¡æ³•è§£æï¼Œä¿ç•™ï¼ˆè¦–ç‚ºè¿‘æœŸï¼‰ï¼Œä½†é¿å…éä¹…
            if dt and dt < start:
                continue
            data = d.to_dict() or {}
            for k, v in data.items():
                if not isinstance(v, dict):
                    continue
                title, content = str(v.get("title") or ""), str(v.get("content") or "")
                if not title and not content:
                    continue
                uniq = f"{title}|{content}"
                if uniq in seen:
                    continue
                seen.add(uniq)
                items.append({"id": f"{d.id}#{k}", "title": title, "content": content, "ts": dt})
    except Exception as e:
        log(f"[warn] load_news_items è®€å– {col_name} å¤±æ•—ï¼š{e}")
    items.sort(key=lambda x: x["ts"] or datetime.min.replace(tzinfo=TAIWAN_TZ), reverse=True)
    return items

# ---------- Token ç·¨è­¯èˆ‡æ‰“åˆ† ----------
def compile_tokens(tokens: List[Token]):
    """å›å‚³ list of (kind, compiled_or_none, weight, note, raw_pattern)"""
    out = []
    for t in tokens:
        w = t.weight if t.polarity == "positive" else -abs(t.weight)
        if t.ttype == "regex":
            try:
                cre = re.compile(t.pattern, flags=re.IGNORECASE)
                out.append(("regex", cre, w, t.note, t.pattern))
            except Exception:
                log(f"[warn] ç„¡æ³•ç·¨è­¯ regex pattern: {t.pattern}")
                continue
        else:
            out.append(("substr", t.pattern.lower(), w, t.note, t.pattern))
    return out

def score_text(text: str, pos_c, neg_c, target: str = None) -> MatchResult:
    """é‡å°å–®ä¸€æ–°è text åˆ¤å®šåˆ†æ•¸ï¼ˆæœƒå…ˆæª¢æŸ¥æ˜¯å¦èˆ‡ target æœ‰é—œï¼‰"""
    norm = normalize(text)
    score, hits, seen_keys = 0.0, [], set()

    aliases = {
        "å°ç©é›»": ["å°ç©é›»", "tsmc", "2330"],
        "é´»æµ·": ["é´»æµ·", "hon hai", "2317", "foxconn", "å¯Œå£«åº·"],
        "è¯é›»": ["è¯é›»", "umc", "2303"],
    }
    # all_aliases ç‚ºæ‰å¹³å°å¯«æ¸…å–®ï¼ˆå»é‡ï¼‰
    all_aliases = list({a.lower() for arr in aliases.values() for a in arr} | {"å°ç©é›»","é´»æµ·","è¯é›»"})
    target_aliases = [a.lower() for a in (aliases.get(target, []) + [target] if target else [])]

    # è‹¥æ²’æœ‰ä»»ä½• target é—œéµå­—å‡ºç¾åœ¨å…¨æ–‡ï¼ˆå¿«é€Ÿéæ¿¾ï¼‰
    if target and not any(tk in norm for tk in target_aliases):
        return MatchResult(0.0, [])

    # ä»¥å¥å­ç‚ºå–®ä½é€²è¡Œæª¢æŸ¥
    sentences = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', norm)
    for sent in sentences:
        sent = sent.strip()
        if not sent:
            continue
        # è‹¥å¥ä¸­å®Œå…¨æ²’æœ‰ä»»ä½•å…¬å¸ aliasï¼Œè·³é
        if not any(alias in sent for alias in all_aliases):
            continue
        # å°æ®µè½ï¼ˆå¥å­ï¼‰ç›´æ¥ç”¨ token åšæ¯”å°ï¼Œé¿å… segment åˆ‡å¾—å¤ªç´°é€ æˆéºæ¼
        for ttype, patt, w, note, raw in pos_c + neg_c:
            key = (raw, note)
            if key in seen_keys:
                continue
            matched = False
            if ttype == "regex":
                try:
                    if patt.search(sent):
                        matched = True
                except Exception:
                    continue
            else:
                if patt in sent:
                    matched = True
            if matched:
                score += w
                hits.append((raw, w, note))
                seen_keys.add(key)
    return MatchResult(score, hits)

# ---------- Groq å‘¼å«ï¼ˆå¯æ³¨å…¥ clientï¼‰ ----------
def make_groq_client():
    try:
        from groq import Groq
    except Exception as e:
        raise RuntimeError("groq å¥—ä»¶ç„¡æ³•åŒ¯å…¥ï¼šè«‹å®‰è£ groq SDK") from e
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        raise RuntimeError("ç’°å¢ƒè®Šæ•¸ GROQ_API_KEY æœªè¨­å®šï¼Œç„¡æ³•å‘¼å« Groq")
    return Groq(api_key=api_key)

def prepare_news_for_llm(news_items: List[Dict]) -> str:
    parts = []
    for i, it in enumerate(news_items, 1):
        title = first_n_sentences(it.get("title",""), 2)
        content = shorten_text(it.get("content",""), 500)
        parts.append(f"æ–°è {i}ï¼š\næ¨™é¡Œï¼š{title}\nå…§å®¹ï¼š{content}\n")
    return "\n".join(parts)

def groq_analyze(client, texts: List[Dict], target: str, token_summary: str = "") -> str:
    combined = prepare_news_for_llm(texts)
    prompt = f"""ä½ æ˜¯ä¸€ä½å°ç£è‚¡å¸‚ç ”ç©¶å“¡ã€‚æ ¹æ“šä»¥ä¸‹æ–°èèˆ‡æ‰“åˆ†æ‘˜è¦ï¼Œåˆ¤æ–·ã€Œæ˜å¤©{target}è‚¡åƒ¹ã€æœ€å¯èƒ½èµ°å‹¢ã€‚
è«‹åªå›è¦†ä»¥ä¸‹å…©è¡Œæ ¼å¼ï¼ˆä¸è¦å¤šé¤˜æ–‡å­—ï¼‰ï¼š

æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š<ä¸Šæ¼² / ä¸‹è·Œ / ä¸æ˜ç¢º>
åŸå› ï¼š<40å­—ä»¥å…§ï¼Œä¸€å¥è©±ç°¡æ½”èªªæ˜ä¸»è¦ç†ç”±>

æ‰“åˆ†æ‘˜è¦ï¼ˆä¾†è‡ª Firestore bull_tokensï¼‰ï¼š
{token_summary}

æ–°èæ‘˜è¦ï¼š
{combined}
"""
    try:
        resp = client.chat.completions.create(
            model=os.getenv("GROQ_MODEL","llama-3.1-8b-instant"),
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­è‚¡å¸‚æ–°èåˆ†æå“¡ï¼Œå›ç­”ç°¡æ½”æº–ç¢ºã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.0,
            max_tokens=220,
        )
        raw = resp.choices[0].message.content.strip()
        cleaned = re.sub(r"^```(?:\w+)?|```$", "", raw).strip()
        cleaned = re.sub(r"\s+", " ", cleaned)

        m_trend = re.search(r"(ä¸Šæ¼²|ä¸‹è·Œ|ä¸æ˜ç¢º)", cleaned)
        trend = m_trend.group(1) if m_trend else "ä¸æ˜ç¢º"
        symbol_map = {"ä¸Šæ¼²": "ğŸ”¼", "ä¸‹è·Œ": "ğŸ”½", "ä¸æ˜ç¢º": "âš ï¸"}
        trend_with_symbol = f"{trend} {symbol_map.get(trend, '')}"

        m_reason = re.search(r"(?:åŸå› |ç†ç”±)[:ï¼š]?\s*(.+)", cleaned)
        reason_text = m_reason.group(1) if m_reason else cleaned
        sentences = re.split(r"[ã€‚.!ï¼ï¼›;]", reason_text)
        short_reason = "ï¼Œ".join(sentences[:2]).strip()
        short_reason = re.sub(r"\s+", " ", short_reason)[:40].strip("ï¼Œ,ã€‚")
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š{trend_with_symbol}\nåŸå› ï¼š{short_reason}"
    except Exception as e:
        return f"[error] Groq å‘¼å«å¤±æ•—ï¼š{e}"

# ---------- åˆ†æ ----------
def analyze_target(db, news_col: str, target: str, result_col: str):
    try:
        pos, neg = load_tokens(db)
        pos_c, neg_c = compile_tokens(pos), compile_tokens(neg)
        items = load_news_items(db, news_col, LOOKBACK_DAYS)
    except Exception as e:
        log(f"[error] è®€å–è³‡æ–™å¤±æ•—ï¼š{e}")
        return

    # æ’é™¤æ˜ç¢ºä¸ç›¸é—œé—œéµå­—
    exclude_keywords = ["intel", "è¼é”", "nvidia", "æ—¥æœˆå…‰"]
    def is_excluded(it):
        txt = (it.get("title","") + " " + it.get("content","")).lower()
        return any(k.lower() in txt for k in exclude_keywords)
    items = [it for it in items if not is_excluded(it)]

    if not items:
        log(f"[info] {target} åœ¨æœ€è¿‘ {LOOKBACK_DAYS} å¤©ç„¡æ–°èæˆ–çš†è¢«éæ¿¾ã€‚")
        return

    filtered, terminal_logs = [], []
    for it in items:
        if STOP:
            break
        text_for_score = (it.get("content") or it.get("title") or "")
        res = score_text(text_for_score, pos_c, neg_c, target)
        if abs(res.score) >= SCORE_THRESHOLD and res.hits:
            filtered.append((it, res))
            trend = "âœ… æ˜æ—¥å¯èƒ½å¤§æ¼²" if res.score > 0 else "âŒ æ˜æ—¥å¯èƒ½ä¸‹è·Œ"
            hits_text_lines = [f"  {'+' if w>0 else '-'} {patt}ï¼ˆ{note}ï¼‰" for patt, w, note in res.hits]
            truncated_title = first_n_sentences(it.get("title",""), 3)
            terminal_logs.append(f"[{it['id']}]\næ¨™é¡Œï¼š{truncated_title}\n{trend}\nå‘½ä¸­ï¼š\n" + "\n".join(hits_text_lines) + "\n")

    for t in terminal_logs[:MAX_DISPLAY_NEWS]:
        print(t)

    token_summary = "\n".join([
        f"æ–°èï¼š{first_n_sentences(x[0].get('title',''),1)} åˆ†æ•¸ï¼š{x[1].score:+.2f} å‘½ä¸­ï¼š{', '.join([n for _,_,n in x[1].hits])}"
        for x in filtered
    ])

    # æº–å‚™å‘¼å« groq
    try:
        client = make_groq_client()
    except Exception as e:
        log(f"[warn] ç„¡æ³•å»ºç«‹ Groq clientï¼š{e}")
        summary = f"[error] ç„¡æ³•å»ºç«‹ Groq clientï¼š{e}"
    else:
        summary = groq_analyze(client, [x[0] for x in filtered], target, token_summary)

    print(summary)

    # å„²å­˜æœ¬æ©Ÿæª”æ¡ˆ
    os.makedirs(RESULT_DIR, exist_ok=True)
    fname_safe = re.sub(r"[^\w\-]", "_", target)
    local_path = os.path.join(RESULT_DIR, f"{fname_safe}_{datetime.now(TAIWAN_TZ).strftime('%Y%m%d_%H%M%S')}.txt")
    try:
        with open(local_path, "w", encoding="utf-8") as f:
            f.write("\n".join(terminal_logs))
            f.write("\n" + "="*60 + "\n")
            f.write(summary + "\n")
        log(f"[info] çµæœå·²å¯«å…¥ {local_path}")
    except Exception as e:
        log(f"[warn] ç„¡æ³•å¯«å…¥æœ¬åœ°çµæœï¼š{e}")

    # å˜—è©¦ä¸Šå‚³åˆ° Firestoreï¼ˆä»¥æ—¥æœŸç‚º doc idï¼‰
    try:
        docid = datetime.now(TAIWAN_TZ).strftime("%Y%m%d")
        db.collection(result_col).document(docid).set({
            "timestamp": datetime.now(TAIWAN_TZ),
            "result": summary,
            "items_count": len(filtered),
        })
        log(f"[info] å·²ä¸Šå‚³çµæœè‡³ Firestore: {result_col}/{docid}")
    except Exception as e:
        log(f"[warn] ä¸Šå‚³ Firestore å¤±æ•—ï¼š{e}")

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    try:
        db = get_db()
    except Exception as e:
        print(f"[error] åˆå§‹åŒ– Firestore å¤±æ•—ï¼š{e}")
        return

    analyze_target(db, NEWS_COLLECTION_TSMC, "å°ç©é›»", "Groq_result")
    print("\n" + "="*70 + "\n")
    analyze_target(db, NEWS_COLLECTION_FOX, "é´»æµ·", "Groq_result_Foxxcon")
    print("\n" + "="*70 + "\n")
    analyze_target(db, NEWS_COLLECTION_UMC, "è¯é›»", "Groq_result_UMC")

if __name__ == "__main__":
    main()

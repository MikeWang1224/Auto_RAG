# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
æº–ç¢ºç‡æå‡ç‰ˆï¼ˆæƒ…ç·’èåˆ + å¤šå±¤æ¬Šé‡ + èªæ„è£œå„Ÿï¼‰
âœ… Firestore å¯«å› + æœ¬åœ° result.txt
âœ… Groq åŒæ™‚è€ƒæ…®æ¯å‰‡æƒ…ç·’åˆ†æ•¸ + å¹³å‡åˆ†æ•¸
âœ… å‘½ä¸­å¤šå‰‡æ–°èæ™‚æå‡ç©©å®šåº¦
âœ… æ–°å¢ï¼šæ”¯æ´ 3 å¤©å…§æ–°èï¼ˆå»¶é²æ•ˆæ‡‰ï¼‰
âœ… æ–°å¢ï¼šçµ±ä¸€ä½¿ç”¨ç•¶æ—¥è‚¡åƒ¹æ¼²è·Œçµ¦ Groq
"""

import os, signal, regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

# ---------- è¨­å®š ----------
SILENT_MODE = True
TAIWAN_TZ = timezone(timedelta(hours=8))

TOKENS_COLLECTION = "bull_tokens"
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- åˆå§‹åŒ– ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ---------- çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
DOCID_RE = re.compile(r"^(?P<ymd>\d{8})(?:_(?P<hms>\d{6}))?$")

def get_db():
    return firestore.Client()

def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def first_n_sentences(text: str, n: int = 3) -> str:
    if not text:
        return ""
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    return "".join(parts[:n]) + ("..." if len(parts) > n else "")

def parse_docid_time(doc_id: str):
    m = DOCID_RE.match(doc_id or "")
    if not m:
        return None
    ymd = m.group("ymd")
    hms = m.group("hms") or "000000"
    try:
        return datetime.strptime(ymd + hms, "%Y%m%d%H%M%S").replace(tzinfo=TAIWAN_TZ)
    except:
        return None

# ---------- Token ----------
def load_tokens(db) -> Tuple[List[Token], List[Token]]:
    pos, neg = [], []
    for d in db.collection(TOKENS_COLLECTION).stream():
        data = d.to_dict() or {}
        pol = data.get("polarity", "").lower()
        ttype = data.get("type", "substr").lower()
        patt = data.get("pattern", "")
        note = data.get("note", "")
        w = float(data.get("weight", 1.0))
        if pol == "positive":
            pos.append(Token(pol, ttype, patt, w, note))
        elif pol == "negative":
            neg.append(Token(pol, ttype, patt, -abs(w), note))
    return pos, neg

def compile_tokens(tokens: List[Token]):
    compiled = []
    for t in tokens:
        if t.ttype == "regex":
            try:
                compiled.append(("regex", re.compile(t.pattern, re.I), t.weight, t.note, t.pattern))
            except:
                continue
        else:
            compiled.append(("substr", None, t.weight, t.note, t.pattern.lower()))
    return compiled

# ---------- Scoring ----------
def score_text(text: str, pos_c, neg_c, target: str = None) -> MatchResult:
    norm = normalize(text)
    score, hits, seen = 0.0, [], set()
    aliases = {
        "å°ç©é›»": ["å°ç©é›»", "tsmc", "2330"],
        "é´»æµ·": ["é´»æµ·", "foxconn", "2317", "å¯Œå£«åº·"],
        "è¯é›»": ["è¯é›»", "umc", "2303"]
    }
    company_pattern = "|".join(re.escape(a) for a in aliases.get(target, []))
    if not re.search(company_pattern, norm):
        return MatchResult(0.0, [])
    for ttype, cre, w, note, patt in pos_c + neg_c:
        key = (patt, note)
        if key in seen:
            continue
        matched = cre.search(norm) if ttype == "regex" else patt in norm
        if matched:
            score += w
            hits.append((patt, w, note))
            seen.add(key)
    return MatchResult(score, hits)

# ---------- Groqï¼ˆæƒ…ç·’èåˆ + æº–ç¢ºç‡å¼·åŒ–ï¼‰ ----------
def groq_analyze(news_list: List[Tuple[str, float]], target: str, avg_score: float) -> str:
    if not news_list:
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš–ï¸\nåŸå› ï¼šè¿‘ä¸‰æ—¥ç„¡ç›¸é—œæ–°è"

    combined = "\n".join(f"{i+1}. ({s:+.2f}) {t}" for i, (t, s) in enumerate(news_list))

    prompt_text = f"""
ä½ æ˜¯ä¸€ä½é‡‘èæ–°èåˆ†æå“¡ã€‚
è«‹é–±è®€ä»¥ä¸‹é—œæ–¼ã€Œ{target}ã€æœ€è¿‘ä¸‰å¤©çš„æ–°èæ‘˜è¦ï¼Œ
ä»¥ã€Œæƒ…ç·’èåˆæ¨¡å¼ã€é€²è¡Œæƒ…ç·’ç¸½çµèˆ‡èµ°å‹¢é æ¸¬ï¼š

1. ç¶œåˆæ–°èä¸­çš„åˆ©å¤šèˆ‡åˆ©ç©ºæƒ…ç·’ï¼Œçµ¦å‡ºæ•´é«”æƒ…ç·’åˆ†æ•¸ï¼ˆ-10 ~ +10ï¼‰ã€‚
2. è‹¥åˆ©å¤šèˆ‡åˆ©ç©ºå‹¢å‡åŠ›æ•µï¼Œè«‹å›ç­”ã€Œä¸æ˜ç¢º âš–ï¸ã€ã€‚
3. è‹¥åˆ©å¤šæƒ…ç·’æ˜é¡¯ä½”å„ªï¼ˆ> +2ï¼‰ï¼Œè«‹å›ç­”ã€Œä¸Šæ¼² ğŸ”¼ã€ã€‚
4. è‹¥åˆ©ç©ºæƒ…ç·’æ˜é¡¯ä½”å„ªï¼ˆ< -2ï¼‰ï¼Œè«‹å›ç­”ã€Œä¸‹è·Œ ğŸ”½ã€ã€‚
5. é™„ä¸Šç°¡çŸ­åŸå› ï¼ˆ40 å­—å…§ï¼‰ï¼Œèªªæ˜ä¸»å°æƒ…ç·’çš„ä¸»è¦å› ç´ ã€‚

æ•´é«”å¹³å‡æƒ…ç·’åˆ†æ•¸ç‚º {avg_score:+.2f}ã€‚

ä»¥ä¸‹æ˜¯æ–°èæ‘˜è¦ï¼ˆå«æƒ…ç·’åˆ†æ•¸èˆ‡ç•¶æ—¥è‚¡åƒ¹æ¼²è·Œï¼‰ï¼š
{combined}

è«‹è¼¸å‡ºæ ¼å¼å¦‚ä¸‹ï¼š
æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š{{ä¸Šæ¼²ï¼ä¸‹è·Œï¼ä¸æ˜ç¢º}}ï¼ˆé™„ç¬¦è™Ÿï¼‰
åŸå› ï¼š{{ä¸€å¥ç¸½çµç†ç”±}}
æƒ…ç·’åˆ†æ•¸ï¼š{{æ•´æ•¸ï¼ˆ-10~10ï¼‰}}
"""

    try:
        resp = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­å°è‚¡åˆ†æå¸«ï¼Œéœ€ç¶œåˆæƒ…ç·’èˆ‡å¸‚å ´åæ‡‰åšå‡ºåˆ¤æ–·ã€‚"},
                {"role": "user", "content": prompt_text},
            ],
            temperature=0.2,
            max_tokens=200,
            timeout=25,
        )
        ans = resp.choices[0].message.content.strip()
        ans = re.sub(r"\s+", " ", ans)

        m_trend = re.search(r"(ä¸Šæ¼²|ä¸‹è·Œ|ä¸æ˜ç¢º|å¾®æ¼²|å¾®è·Œ)", ans)
        trend = m_trend.group(1) if m_trend else "ä¸æ˜ç¢º"
        symbol_map = {"ä¸Šæ¼²": "ğŸ”¼", "å¾®æ¼²": "â†—ï¸", "å¾®è·Œ": "â†˜ï¸", "ä¸‹è·Œ": "ğŸ”½", "ä¸æ˜ç¢º": "âš–ï¸"}

        m_reason = re.search(r"(?:åŸå› |ç†ç”±)[:ï¼š]?\s*(.+?)(?:æƒ…ç·’åˆ†æ•¸|$)", ans)
        reason = m_reason.group(1).strip() if m_reason else "å¸‚å ´è§€æœ›"

        m_score = re.search(r"æƒ…ç·’åˆ†æ•¸[:ï¼š]?\s*(-?\d+)", ans)
        mood_score = int(m_score.group(1)) if m_score else 0

        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š{trend} {symbol_map.get(trend,'')}\nåŸå› ï¼š{reason}\næƒ…ç·’åˆ†æ•¸ï¼š{mood_score:+d}"

    except Exception as e:
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šæŒå¹³ âš–ï¸\nåŸå› ï¼šGroqåˆ†æå¤±æ•—({e})\næƒ…ç·’åˆ†æ•¸ï¼š0"

# ---------- ä¸»åˆ†æ ----------
def analyze_target(db, collection: str, target: str, result_field: str):
    pos, neg = load_tokens(db)
    pos_c, neg_c = compile_tokens(pos), compile_tokens(neg)

    today = datetime.now(TAIWAN_TZ).date()
    filtered = []
    price_change_today = None

    for d in db.collection(collection).stream():
        dt = parse_docid_time(d.id)
        if not dt:
            continue
        news_date = dt.date()
        delta_days = (today - news_date).days
        if delta_days > 2:
            continue

        day_weight = {0: 1.0, 1: 0.85, 2: 0.7}.get(delta_days, 1.0)
        data = d.to_dict() or {}

        for k, v in data.items():
            if not isinstance(v, dict):
                continue
            title, content = v.get("title", ""), v.get("content", "")
            full = title + " " + content
            res = score_text(full, pos_c, neg_c, target)
            if not res.hits:
                continue

            token_weight = 1.0 + min(len(res.hits) * 0.05, 0.3)
            total_weight = day_weight * token_weight

            filtered.append((title, res, total_weight))

            if price_change_today is None:
                price_change_today = v.get("price_change", None)

    if not filtered:
        summary = groq_analyze([], target, 0)
    else:
        filtered.sort(key=lambda x: abs(x[1].score * x[2]), reverse=True)
        top_news = filtered[:10]

        print(f"\nğŸ“° {target} è¿‘æœŸé‡é»æ–°èï¼š")
        for title, res, weight in top_news:
            print(f"({weight:.2f}x, åˆ†æ•¸={res.score:+.2f}) {title}")
            for p, w, n in res.hits:
                print(f"   {'+' if w>0 else '-'} {p}ï¼ˆ{n}ï¼‰")

        news_with_scores = []
        for title, res, weight in top_news:
            t_text = f"{title}"
            if price_change_today:
                t_text += f"ï¼ˆç•¶æ—¥è‚¡åƒ¹æ¼²è·Œ: {price_change_today}ï¼‰"
            news_with_scores.append((t_text, res.score * weight))

        avg_score = sum(s for _, s in news_with_scores) / len(news_with_scores)
        summary = groq_analyze(news_with_scores, target, avg_score)

        fname = f"result_{today.strftime('%Y%m%d')}.txt"
        with open(fname, "a", encoding="utf-8") as f:
            f.write(f"======= {target} =======\n")
            for title, res, weight in top_news:
                hits_text = "\n".join([f"  {'+' if w>0 else '-'} {p}ï¼ˆ{n}ï¼‰" for p, w, n in res.hits])
                f.write(f"æ¨™é¡Œï¼š{first_n_sentences(title)}ï¼ˆ{weight:.2f}xï¼‰\nå‘½ä¸­ï¼š\n{hits_text}\n\n")
            f.write(summary + "\n\n")

    print(summary + "\n")

    # Firestore å¯«å›
    try:
        db.collection(result_field).document(today.strftime("%Y%m%d")).set({
            "timestamp": datetime.now(TAIWAN_TZ).isoformat(),
            "result": summary,
        })
    except Exception as e:
        print(f"[warning] Firestore å¯«å›å¤±æ•—ï¼š{e}")

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    if not SILENT_MODE:
        print("ğŸš€ é–‹å§‹åˆ†æå°è‚¡ç„¦é»è‚¡ï¼ˆæº–ç¢ºç‡æå‡ç‰ˆï¼‰...\n")

    db = get_db()
    analyze_target(db, NEWS_COLLECTION_TSMC, "å°ç©é›»", "Groq_result")
    print("=" * 70)
    analyze_target(db, NEWS_COLLECTION_FOX, "é´»æµ·", "Groq_result_Foxxcon")
    print("=" * 70)
    analyze_target(db, NEWS_COLLECTION_UMC, "è¯é›»", "Groq_result_UMC")

if __name__ == "__main__":
    main()

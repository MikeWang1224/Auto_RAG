# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
== ä¿ç•™åŸåˆ¤æ–·é‚è¼¯ï¼Œä¸æ”¹æº–ç¢ºç‡ ==
å”¯ä¸€è®Šæ›´ï¼šè¼¸å‡ºæ ¼å¼æ”¹æˆå–®è¡Œç²¾ç°¡å­—ä¸²
"""
import os, signal, regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple
from google.cloud import firestore
from dotenv import load_dotenv
# ç•™è‘— Groq client ä»¥é˜²æ—¥å¾Œéœ€è¦ï¼Œä½†æœ¬ç‰ˆæœ¬ä¸å‘¼å« LLM
from groq import Groq

# ---------- è¨­å®š ----------
SILENT_MODE = True
TAIWAN_TZ = timezone(timedelta(hours=8))

TOKENS_COLLECTION = "bull_tokens"
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"

# å…§éƒ¨æ•æ„Ÿè©è¡¨ï¼ˆèˆŠç‰ˆä¿ç•™ï¼Œä¸»åŠ æ¬Šè¦å‰‡å¦è¨­ï¼‰
SENSITIVE_WORDS = {
    "æ³•èªª": 1.5,
    "è²¡å ±": 1.4,
    "æ–°å“": 1.3,
    "åˆä½œ": 1.3,
    "ä½µè³¼": 1.4,
    "æŠ•è³‡": 1.3,
    "åœå·¥": 1.6,
    "ä¸‹ä¿®": 1.5,
    "åˆ©ç©º": 1.5,
    "çˆ†æ–™": 1.4,
    "ç‡Ÿæ”¶": 1.3,
    "å±•æœ›": 1.2,
}

# ç¡¬è¦å‰‡åŠ æ¬Šï¼ˆå–®æ¬¡åŠ æˆæ¸…å–®ï¼‰
HARD_WEIGHTS_POS = {
    "è²¡å ±": 1.5,
    "æ³•èªª": 1.5,
    "å±•æœ›": 1.5,
    "è³‡æœ¬æ”¯å‡º": 1.5,
    "è¨‚å–®": 1.2,
    "æ“´ç”¢": 1.2,
    "çˆ†å–®": 1.2,
    "æ¼²åƒ¹": 1.2,
}
HARD_WEIGHTS_NEG = {
    "åœå·¥": -1.5,
    "è£å“¡": -1.5,
    "è™§æ": -1.5,
    "ä¸‹ä¿®": -1.5,
}

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- åˆå§‹åŒ– ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)
# å»¶å¾Œå»ºç«‹ Groq clientï¼ˆè‹¥éœ€è¦å†å‘¼å« get_groq_clientï¼‰
def get_groq_client():
    key = os.getenv("GROQ_API_KEY")
    if not key:
        return None
    return Groq(api_key=key)

# ---------- çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
def get_db():
    return firestore.Client()

def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def first_n_sentences(text: str, n: int = 3) -> str:
    if not text:
        return ""
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    return "".join(parts[:n]) + ("..." if len(parts) > n else "")

def parse_docid_time(doc_id: str):
    m = re.match(r"^(?P<ymd>\d{8})(?:_(?P<hms>\d{6}))?$", doc_id or "")
    if not m:
        return None
    ymd, hms = m.group("ymd"), m.group("hms") or "000000"
    try:
        return datetime.strptime(ymd + hms, "%Y%m%d%H%M%S").replace(tzinfo=TAIWAN_TZ)
    except:
        return None

# ---------- è§£æ price_change ----------
def parse_price_change(raw: str) -> float:
    """
    è§£ææ ¼å¼ç¯„ä¾‹ï¼š
    "+7.50 (+3.28%)" -> 0.0328
    "-1.20 (-0.42%)" -> -0.0042
    è‹¥ç„¡æ³•è§£æå‰‡å›å‚³ 0.0
    """
    if not raw:
        return 0.0
    s = str(raw).replace(",", "").strip()
    m = re.search(r"\(([-+]?[\d\.]+)%\)", s) or re.search(r"([-+]?[\d\.]+)%", s)
    if not m:
        return 0.0
    try:
        return float(m.group(1)) / 100.0
    except:
        return 0.0

# ---------- Token ----------
def load_tokens(db):
    pos, neg = [], []
    try:
        for d in db.collection(TOKENS_COLLECTION).stream():
            data = d.to_dict() or {}
            pol = data.get("polarity", "").lower()
            ttype = data.get("type", "substr").lower()
            patt = data.get("pattern", "")
            note = data.get("note", "")
            try:
                w = float(data.get("weight", 1.0))
            except:
                w = 1.0
            if pol == "positive":
                pos.append(Token(pol, ttype, patt, w, note))
            elif pol == "negative":
                neg.append(Token(pol, ttype, patt, -abs(w), note))
    except Exception as e:
        # è‹¥ tokens collection ä¸å­˜åœ¨æˆ–è®€å–å¤±æ•—ï¼Œå›å‚³ç©ºåˆ—è¡¨ï¼ˆç¨‹å¼ä»å¯é‹è¡Œï¼‰
        print(f"[warning] load_tokens å¤±æ•—ï¼š{e}")
    return pos, neg

def compile_tokens(tokens: List[Token]):
    compiled = []
    for t in tokens:
        if t.ttype == "regex":
            try:
                compiled.append(("regex", re.compile(t.pattern, re.I), t.weight, t.note, t.pattern))
            except:
                continue
        else:
            compiled.append(("substr", None, t.weight, t.note, t.pattern.lower()))
    return compiled

# ---------- Scoring ----------
def score_text(text: str, pos_c, neg_c, target: str = None) -> MatchResult:
    norm = normalize(text)
    score, hits, seen = 0.0, [], set()
    aliases = {"å°ç©é›»": ["å°ç©é›»", "tsmc", "2330"],
               "é´»æµ·": ["é´»æµ·", "foxconn", "2317", "å¯Œå£«åº·"],
               "è¯é›»": ["è¯é›»", "umc", "2303"]}
    if target not in aliases:
        return MatchResult(0.0, [])
    company_pattern = r"\b(?:" + "|".join(re.escape(a) for a in aliases.get(target, [])) + r")\b"
    if not re.search(company_pattern, norm):
        return MatchResult(0.0, [])

    for ttype, cre, w, note, patt in pos_c + neg_c:
        key = (patt, note)
        if key in seen:
            continue
        matched = cre.search(norm) if ttype == "regex" else (patt in norm)
        if matched:
            score += w
            hits.append((patt, w, note))
            seen.add(key)
    return MatchResult(score, hits)

# ---------- Context-aware èª¿æ•´ ----------
def adjust_score_for_context(text: str, base_score: float) -> float:
    if not text or base_score == 0:
        return base_score

    norm = text.lower()
    neutral_phrases = ["é‡ç”³", "ç¬¦åˆé æœŸ", "é æœŸå…§", "ä¸­æ€§çœ‹å¾…", "ç„¡é‡å¤§å½±éŸ¿", "æŒå¹³", "æœªè®Š"]
    if any(p in norm for p in neutral_phrases):
        base_score *= 0.4

    positive_boost = ["å‰µæ–°é«˜", "å€å¢", "å¤§å¹…æˆé•·", "ç²åˆ©æš´å¢", "å ±å–œ"]
    negative_boost = ["æš´è·Œ", "ä¸‹æ»‘", "è™§æ", "åœå·¥", "ä¸‹ä¿®", "è£å“¡", "è­¦è¨Š"]
    if any(p in norm for p in positive_boost):
        base_score *= 1.3
    if any(p in norm for p in negative_boost):
        base_score *= 1.3

    return base_score

# ---------- å¸‚å ´èª¿æ•´èˆ‡èƒŒé›¢åµæ¸¬ ----------
def adjust_by_market(avg_score: float, today_change: float) -> float:
    """
    æ ¹æ“šä»Šæ—¥æ¼²è·Œå¹…èª¿æ•´å¹³å‡åˆ†æ•¸ã€‚
    ä¿å®ˆé è¨­ï¼š
      - ç•¶æ—¥å¤§æ¼² (>= 3%)ï¼š+0.5
      - ç•¶æ—¥ä¸­åº¦æ¼²è·Œ (|1%~3%|)ï¼š+/-0.2
      - ç•¶æ—¥å¤§è·Œ (<= -3%)ï¼š-0.5
    """
    if today_change >= 0.03:
        return avg_score + 0.5
    if today_change <= -0.03:
        return avg_score - 0.5
    if today_change >= 0.01:
        return avg_score + 0.2
    if today_change <= -0.01:
        return avg_score - 0.2
    return avg_score

def detect_divergence(avg_score: float, today_change: float) -> str:
    """
    èƒŒé›¢æª¢æŸ¥ï¼š
      - avg_score > 1 ä¸” today_change < -2% -> åˆ©å¤šä¸æ¼²
      - avg_score < -1 ä¸” today_change > +2% -> åˆ©ç©ºä¸è·Œ
    """
    if avg_score > 1.0 and today_change < -0.02:
        return "åˆ©å¤šä¸æ¼²ï¼ˆç–‘ä¼¼ä¸»åŠ›å‡ºè²¨ï¼‰"
    if avg_score < -1.0 and today_change > 0.02:
        return "åˆ©ç©ºä¸è·Œï¼ˆå¯èƒ½æœ‰éš±æ€§åˆ©å¤šï¼‰"
    return "ç„¡æ˜é¡¯èƒŒé›¢"

# ---------- ç¡¬è¦å‰‡æ±ºç­–å‡½å¼ï¼ˆæ›¿ä»£ LLMï¼‰ ----------
def decide_by_hard_rules(news_list: List[Tuple[str, float]], today_change: float, full_texts: List[str] = None, adjusted_avg: float = None, divergence: str = None) -> Tuple[str,int,List[str]]:
    """
    è¿”å›ï¼š
      - concise_str: å–®è¡Œè¼¸å‡ºï¼ˆä½¿ç”¨è€…è¦æ±‚æ ¼å¼ï¼‰
      - mood_score_int: æƒ…ç·’åˆ†æ•¸æ•´æ•¸ï¼ˆ-10..+10ï¼‰
      - top_phrases: ç”¨æ–¼åŸå› æ¬„çš„é—œéµçŸ­èªæ¸…å–®
    ä¿ç•™åŸå§‹æ±ºç­–é‚è¼¯ä½†æ”¹é€ è¼¸å‡ºæ ¼å¼ã€‚
    """
    n = len(news_list)
    if n == 0:
        concise = "æ˜å¤©è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš–ï¸ åŸå› ï¼šè¿‘ä¸‰æ—¥ç„¡ç›¸é—œæ–°èã€‚ æƒ…ç·’åˆ†æ•¸ï¼š0"
        return concise, 0, ["ç„¡æ–°èè³‡æ–™"]

    contributions = []
    reason_lines = []
    for idx, (title, weighted_score) in enumerate(news_list):
        base = 1.0 if weighted_score > 0 else (-1.0 if weighted_score < 0 else 0.0)
        add = 0.0
        txt = (full_texts[idx] if full_texts and idx < len(full_texts) else title).lower()

        # å–®æ¬¡æ­£æ¬Šé‡æª¢æŸ¥
        for kw, v in HARD_WEIGHTS_POS.items():
            if kw in txt:
                add += v
                reason_lines.append(f"åŒ…å«æ­£å‘é—œéµè©ã€Œ{kw}ã€")
                break
        # å–®æ¬¡è² æ¬Šé‡æª¢æŸ¥ï¼ˆå„ªå…ˆè² é¢ï¼‰
        for kw, v in HARD_WEIGHTS_NEG.items():
            if kw in txt:
                add += v
                reason_lines.append(f"åŒ…å«è² å‘é—œéµè©ã€Œ{kw}ã€")
                break

        contrib = base + add
        contributions.append(contrib)
        # ç°¡çŸ­æ‘˜è¦ä¸€å¥
        sent = first_n_sentences(title, 1)
        reason_lines.append(f"æ–°è[{idx+1}]æ‘˜è¦ï¼š{sent}")

    total_score = sum(contributions)
    standardized = total_score / (n + 1)  # èˆŠæ¨™æº–åŒ–å…¬å¼

    # è‹¥æœ‰å¸‚å ´èª¿æ•´ï¼Œæ¡ç”¨ adjusted_avgï¼ˆä¿æŒåŸç¨‹å¼è¨­è¨ˆï¼‰
    if adjusted_avg is not None:
        standardized = adjusted_avg
        reason_lines.append(f"å·²å¥—ç”¨å¸‚å ´æ¼²è·Œå¹…èª¿æ•´")

    # impact åˆ†é¡ï¼ˆæ±ºå®šæ–¹å‘ï¼‰
    if standardized >= 2.5:
        trend = "ä¸Šæ¼²"
        symbol = "ğŸ”¼"
    elif standardized >= 1.0:
        trend = "å¾®æ¼²"
        symbol = "â†—ï¸"
    elif standardized > -1.0:
        trend = "å¾®è·Œ"
        symbol = "â†˜ï¸"
    else:
        trend = "ä¸‹è·Œ"
        symbol = "ğŸ”½"

    # ä»Šæ—¥èµ°å‹¢
    pct = round(today_change * 100, 2)
    trend_today = "ä¸Šæ¼²" if today_change > 0 else "ä¸‹è·Œ" if today_change < 0 else "å¹³ç›¤"

    # market_effect åˆ¤æ–·
    dir_sign = 1 if standardized > 0 else (-1 if standardized < 0 else 0)
    today_sign = 1 if today_change > 0 else (-1 if today_change < 0 else 0)
    if dir_sign != 0 and today_sign != 0:
        if dir_sign == today_sign:
            market_effect = "ä»Šæ—¥èµ°å‹¢èˆ‡æ–°èæ–¹å‘åŒå‘ã€‚"
        else:
            market_effect = "ä»Šæ—¥èµ°å‹¢èˆ‡æ–°èæ–¹å‘ç›¸åã€‚"
    else:
        market_effect = "ä»Šæ—¥èµ°å‹¢æˆ–æ–°èæ–¹å‘ä¸­æ€§ã€‚"

    # æƒ…ç·’åˆ†æ•¸æ˜ å°„ï¼ˆ-10~+10ï¼‰
    mood_score = max(-10, min(10, int(round(standardized * 3))))

    # å½¢æˆ concise reasonï¼šå¾ reason_lines ä¸­æŒ‘æœ€é‡è¦çš„ 3 æ¢é—œéµæè¿°ï¼ˆå»é‡ï¼‰
    short_reasons = []
    seen = set()
    for line in reason_lines:
        # æå–æœ‰æ„ç¾©çŸ­èªï¼ˆå»æ‰ã€Œæ–°è[...]æ‘˜è¦ï¼šã€å­—æ¨£ï¼‰
        phrase = re.sub(r"æ–°è\[\d+\]æ‘˜è¦ï¼š", "", line).strip()
        # å–ç¬¬ä¸€å¥è©±æˆ–å‰ 60 å­—
        phrase = phrase.split("ã€‚")[0][:120]
        if phrase and phrase not in seen:
            short_reasons.append(phrase)
            seen.add(phrase)
        if len(short_reasons) >= 3:
            break

    # è‹¥ short_reasons ç©ºï¼Œæ”¾ fallback
    if not short_reasons:
        short_reasons = ["å¸‚å ´æ¶ˆæ¯ç¶œåˆå½±éŸ¿"]

    # åˆæˆå–®è¡Œè¼¸å‡ºï¼ˆç¬¦åˆä½¿ç”¨è€…æ ¼å¼ï¼‰
    reason_text = "ï¼›".join(short_reasons)
    concise_str = f"æ˜å¤©è‚¡åƒ¹èµ°å‹¢ï¼š{trend} {symbol} åŸå› ï¼š{reason_text}ã€‚ æƒ…ç·’åˆ†æ•¸ï¼š{mood_score:+d}"

    # è‹¥æœ‰ divergenceï¼Œä¹ŸæŠŠç°¡çŸ­èªªæ˜åŠ å…¥ top_phrasesï¼Œä½†ä¸è®“è¼¸å‡ºè®Šå¤ªé•·
    top_phrases = short_reasons.copy()
    if divergence and divergence != "ç„¡æ˜é¡¯èƒŒé›¢":
        top_phrases.append(divergence)

    return concise_str, mood_score, top_phrases

# ---------- Groq analyzeï¼ˆåªæ˜¯åŒ…è£ç¡¬è¦å‰‡ï¼‰ ----------
def groq_analyze(news_list, target, avg_score, today_change, adjusted_avg=None, divergence=None):
    full_texts = [t for t, _ in news_list]
    concise, mood, top_phrases = decide_by_hard_rules(news_list, today_change, full_texts, adjusted_avg=adjusted_avg, divergence=divergence)
    # åœ¨çµæœå‰åŠ ä¸Š target åç¨±
    # çµæœå·²æ˜¯å–®è¡Œï¼Œä¾‹å¦‚ "æ˜å¤©è‚¡åƒ¹èµ°å‹¢ï¼šä¸‹è·Œ ğŸ”½ åŸå› ï¼š... æƒ…ç·’åˆ†æ•¸ï¼š-3"
    return concise.replace("æ˜å¤©è‚¡åƒ¹èµ°å‹¢", f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢", 1)

# ---------- ä¸»åˆ†æï¼ˆèˆ‡åŸç¨‹å¼ä¸€è‡´ï¼‰ ----------
def analyze_target(db, collection, target, result_field):
    pos, neg = load_tokens(db)
    pos_c, neg_c = compile_tokens(pos), compile_tokens(neg)
    today = datetime.now(TAIWAN_TZ).date()

    filtered, weighted_scores = [], []
    today_price_change = 0.0

    # ---------- å…ˆæƒä¸€æ¬¡ collection å–å¾—ä»Šæ—¥ price_changeï¼ˆè‹¥æœ‰ï¼‰ ----------
    try:
        for d in db.collection(collection).stream():
            dt = parse_docid_time(d.id)
            if not dt:
                continue
            if dt.date() != today:
                continue
            data = d.to_dict() or {}
            for k, v in data.items():
                if isinstance(v, dict) and "price_change" in v:
                    today_price_change = parse_price_change(v.get("price_change"))
                    break
            if today_price_change != 0.0:
                break
    except Exception:
        today_price_change = 0.0

    # ---------- åŸæœ‰æ–°èæ‰“åˆ†æµç¨‹ï¼ˆä¿ç•™ï¼‰ ----------
    for d in db.collection(collection).stream():
        dt = parse_docid_time(d.id)
        if not dt:
            continue
        delta_days = (today - dt.date()).days
        if delta_days > 2:
            continue

        day_weight = 1.0 if delta_days == 0 else 0.85 if delta_days == 1 else 0.7
        data = d.to_dict() or {}

        for k, v in data.items():
            if not isinstance(v, dict):
                continue
            title, content = v.get("title", ""), v.get("content", "")
            full = title + " " + content
            res = score_text(full, pos_c, neg_c, target)
            if not res.hits:
                continue

            adj_score = adjust_score_for_context(full, res.score)
            token_weight = 1.0 + min(len(res.hits) * 0.05, 0.3)
            impact = 1.0 + sum(w * 0.05 for k_sens, w in SENSITIVE_WORDS.items() if k_sens in full)
            total_weight = day_weight * token_weight * impact

            filtered.append((d.id, k, title, full, res, total_weight))
            weighted_scores.append(adj_score * total_weight)

    # ---------- ç„¡æ–°è fallback ----------
    if not filtered:
        summary = groq_analyze([], target, 0, today_price_change)
        mood_score = 0
    else:
        # å»é‡æ–°è
        seen_text = set()
        top_news = []
        for docid, key, title, full, res, weight in sorted(filtered, key=lambda x: abs(x[4].score * x[5]), reverse=True):
            news_text = normalize(full)
            if news_text in seen_text:
                continue
            seen_text.add(news_text)
            top_news.append((docid, key, title, res, weight, full))
            if len(top_news) >= 10:
                break

        # è¼¸å‡ºæ–°èæ‘˜è¦ï¼ˆconsoleï¼‰
        if not SILENT_MODE:
            print(f"\nğŸ“° {target} è¿‘æœŸé‡é»æ–°èï¼ˆå«è¡æ“Šï¼‰:")
            for docid, key, title, res, weight, full in top_news:
                impact_val = sum(w for k_sens, w in SENSITIVE_WORDS.items() if k_sens in title)
                print(f"[{docid}#{key}] ({weight:.2f}x, åˆ†æ•¸={res.score:+.2f}, è¡æ“Š={1+impact_val/10:.2f}) {title}")
                for p, w, n in res.hits:
                    sign = "+" if w>0 else "-"
                    print(f"   {sign} {p}ï¼ˆ{n}ï¼‰")

        # æ§‹é€  news_with_scores ä¾›ç¡¬è¦å‰‡ä½¿ç”¨ï¼ˆä¿ç•™ title åŠåŠ æ¬Šå¾Œåˆ†æ•¸ï¼‰
        news_with_scores = []
        full_texts = []
        for _, _, title, res, weight, full in top_news:
            news_with_scores.append((title, res.score * weight))
            full_texts.append(full)

        # è¨ˆç®— avg_scoreï¼ˆæœªèª¿æ•´ï¼‰
        avg_score = sum(s for _, s in news_with_scores) / len(news_with_scores) if news_with_scores else 0.0

        # === å¸‚å ´èª¿æ•´ & èƒŒé›¢åµæ¸¬ ===
        adjusted_avg = adjust_by_market(avg_score, today_price_change)
        divergence = detect_divergence(avg_score, today_price_change)

        # ä½¿ç”¨ç¡¬è¦å‰‡æ±ºç­–
        summary = groq_analyze(news_with_scores, target, avg_score, today_price_change, adjusted_avg=adjusted_avg, divergence=divergence)

        # åŒæ­¥ mood_scoreï¼ˆå¾ decide_by_hard_rules å–å¾—æ›´æº–ç¢ºæ•¸å€¼ï¼‰
        # é‡æ–°å‘¼å«ä»¥ç²å¾— mood_score èˆ‡ top_phrases
        concise, mood_score, top_phrases = decide_by_hard_rules(news_with_scores, today_price_change, full_texts, adjusted_avg=adjusted_avg, divergence=divergence)

        # æœ¬åœ°å­˜æª”ï¼ˆä¿ç•™è¼ƒå¤šç´°ç¯€æ–¼æª”æ¡ˆï¼Œä½† Firestore åªå­˜ concise å­—ä¸²ï¼‰
        fname = f"result_{today.strftime('%Y%m%d')}.txt"
        with open(fname, "a", encoding="utf-8") as f:
            f.write(f"======= {target} =======\n")
            f.write(f"ä»Šæ—¥æ¼²è·Œ (ç¨‹å¼è®€å–)ï¼š{round(today_price_change*100,2)}%\n")
            f.write(f"avg_score (åŸå§‹)ï¼š{avg_score:+.4f}\n")
            f.write(f"avg_score (èª¿æ•´å¾Œ)ï¼š{adjusted_avg:+.4f}\n")
            f.write(f"èƒŒé›¢æª¢æ¸¬ï¼š{divergence}\n\n")
            for docid, key, title, res, weight, full in top_news:
                hits_text = "\n".join([f"  {'+' if w>0 else '-'} {p}ï¼ˆ{n}ï¼‰" for p, w, n in res.hits])
                f.write(f"[{docid}#{key}]ï¼ˆ{weight:.2f}xï¼‰\næ¨™é¡Œï¼š{first_n_sentences(title)}\nå‘½ä¸­ï¼š\n{hits_text}\n\n")
            f.write(summary + "\n\n")

    # å°å‡ºèˆ‡å¯«å› Firestoreï¼ˆåªå¯«å…¥ single-line concise stringï¼‰
    print(summary + "\n")

    # Firestore å¯«å›ï¼ˆå¯«å–®è¡Œå­—ä¸²åˆ° result collection under date docï¼‰
    try:
        db.collection(result_field).document(today.strftime("%Y%m%d")).set({
            "timestamp": datetime.now(TAIWAN_TZ).isoformat(),
            "result": summary,
        })
    except Exception as e:
        print(f"[warning] Firestore å¯«å›å¤±æ•—ï¼š{e}")

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    if not SILENT_MODE:
        print("ğŸš€ é–‹å§‹åˆ†æå°è‚¡ç„¦é»è‚¡ï¼ˆæº–ç¢ºç‡ä¿ç•™ï¼Œè¼¸å‡ºæ ¼å¼ç²¾ç°¡ï¼‰...\n")

    db = get_db()
    analyze_target(db, NEWS_COLLECTION_TSMC, "å°ç©é›»", "Groq_result")
    print("=" * 70)
    analyze_target(db, NEWS_COLLECTION_FOX, "é´»æµ·", "Groq_result_Foxxcon")
    print("=" * 70)
    analyze_target(db, NEWS_COLLECTION_UMC, "è¯é›»", "Groq_result_UMC")

if __name__ == "__main__":
    main()

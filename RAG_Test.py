# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
æ ¼å¼å®Œå…¨æ¯”ç…§ç¯„ä¾‹è¼¸å‡ºï¼š
- æ¯å‰‡æ–°èæœ‰å‘½ä¸­åˆ—è¡¨èˆ‡è¶¨å‹¢ç¬¦è™Ÿ
- æœ€å¾Œç”¨ Groq ç¸½çµæ˜æ—¥èµ°å‹¢èˆ‡åŸå› 
- Firestore å¯«å›çµæœ
"""

import os, signal, regex as re, sys, io
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Dict
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

# ---------- è¨­å®š ----------
SILENT_MODE = False
MAX_DISPLAY_NEWS = 5
TAIWAN_TZ = timezone(timedelta(hours=8))

# ---------- è®€ .env ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)

TOKENS_COLLECTION = os.getenv("FIREBASE_TOKENS_COLLECTION", "bull_tokens")
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"
SCORE_THRESHOLD = float(os.getenv("SCORE_THRESHOLD", "0.2"))
LOOKBACK_DAYS = int(os.getenv("LOOKBACK_DAYS", "2"))

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- è³‡æ–™çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def first_n_sentences(text: str, n: int = 3) -> str:
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    parts = [p for p in parts if p.strip()]
    joined = "".join(parts[:n])
    if not re.search(r'[ã€‚\.ï¼!\?ï¼Ÿï¼›;]$', joined):
        joined += "..."
    return joined

def parse_docid_time(doc_id: str):
    try:
        if "_" in doc_id:
            return datetime.strptime(doc_id, "%Y%m%d_%H%M%S").replace(tzinfo=TAIWAN_TZ)
        return datetime.strptime(doc_id, "%Y%m%d").replace(tzinfo=TAIWAN_TZ)
    except:
        return None

# ---------- åˆå§‹åŒ– ----------
def get_db(): return firestore.Client()
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ---------- Token è™•ç† ----------
def load_tokens(db):
    pos, neg = [], []
    for d in db.collection(TOKENS_COLLECTION).stream():
        data = d.to_dict() or {}
        t = Token(
            polarity=data.get("polarity",""),
            ttype=data.get("type","substr"),
            pattern=data.get("pattern",""),
            weight=float(data.get("weight",1.0)),
            note=data.get("note","")
        )
        if t.polarity.lower() == "positive":
            pos.append(t)
        elif t.polarity.lower() == "negative":
            neg.append(t)
    return pos, neg

# ---------- æ‰“åˆ† ----------
def score_text(text: str, pos_tokens, neg_tokens) -> MatchResult:
    text_norm = normalize(text)
    score, hits = 0.0, []
    for t in pos_tokens + neg_tokens:
        w = t.weight if t.polarity == "positive" else -abs(t.weight)
        matched = re.search(t.pattern, text_norm, re.I) if t.ttype == "regex" else t.pattern.lower() in text_norm
        if matched:
            hits.append((t.pattern, w, t.note))
            score += w
    return MatchResult(score, hits)

# ---------- Groq ç¸½çµ ----------
def groq_analyze(news_list, target):
    text_block = "\n".join([f"{i+1}. {n}" for i,n in enumerate(news_list)])
    prompt = f"""ä½ æ˜¯ä¸€ä½å°è‚¡åˆ†æå¸«ã€‚æ ¹æ“šä»¥ä¸‹{target}ç›¸é—œæ–°èï¼Œè«‹åˆ¤æ–·æ˜æ—¥{target}è‚¡åƒ¹èµ°å‹¢ï¼š
å›è¦†æ ¼å¼ï¼š
æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š<ä¸Šæ¼²/ä¸‹è·Œ/ä¸æ˜ç¢º> ğŸ”¼ğŸ”½âš ï¸
åŸå› ï¼š<40å­—å…§ç°¡è¿°>

{text_block}
"""
    try:
        resp = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role":"system","content":"ä½ æ˜¯å°ˆæ¥­è‚¡å¸‚åˆ†æå¸«ï¼Œå›ç­”ç°¡æ½”æº–ç¢ºã€‚"},
                {"role":"user","content":prompt}
            ],
            temperature=0.0,
            max_tokens=120,
        )
        ans = resp.choices[0].message.content.strip()
        return re.sub(r"\s+"," ",ans)
    except Exception as e:
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš ï¸\nåŸå› ï¼šGroqåˆ†æå¤±æ•—({e})"

# ---------- ä¸»åˆ†æ ----------
def analyze_target(db, collection, target, result_field):
    pos, neg = load_tokens(db)
    items = db.collection(collection).stream()
    now = datetime.now(TAIWAN_TZ)
    start = now - timedelta(days=LOOKBACK_DAYS)

    output_lines, groq_inputs, filtered = [], [], []

    for d in items:
        t = parse_docid_time(d.id)
        if not t or t < start: continue
        data = d.to_dict() or {}
        for k,v in data.items():
            if not isinstance(v,dict): continue
            title, content = v.get("title",""), v.get("content","")
            full = title + " " + content
            res = score_text(full, pos, neg)
            if abs(res.score) < SCORE_THRESHOLD or not res.hits:
                continue
            trend = "âœ… æ˜æ—¥å¯èƒ½å¤§æ¼²" if res.score > 0 else "âŒ æ˜æ—¥å¯èƒ½ä¸‹è·Œ"
            hit_lines = [f"  {'+' if w>0 else '-'} {p}ï¼ˆ{n}ï¼‰" for p,w,n in res.hits]
            part = f"[{d.id}#{k}]\næ¨™é¡Œï¼š{first_n_sentences(title)}\n{trend}\nå‘½ä¸­ï¼š\n" + "\n".join(hit_lines)
            output_lines.append(part+"\n")
            groq_inputs.append(full)
            filtered.append((d.id, k, res))

    if not filtered:
        return f"{target}ï¼šç„¡æ˜é¡¯è®ŠåŒ–\n"

    groq_result = groq_analyze(groq_inputs, target)
    output = "\n".join(output_lines) + "\n" + groq_result + "\n"

    # å¯«å› Firestore
    for doc_id, key, res in filtered:
        try:
            db.collection(collection).document(doc_id).set({
                result_field: {
                    key: {
                        "summary": groq_result,
                        "trend": "ä¸Šæ¼²" if res.score>0 else "ä¸‹è·Œ",
                        "reason": groq_result,
                        "hits": [{"pattern":p,"weight":w,"note":n} for p,w,n in res.hits],
                        "updated_at": datetime.now(TAIWAN_TZ).isoformat()
                    }
                }
            }, merge=True)
        except Exception as e:
            print(f"[warning] Firestore å¯«å›å¤±æ•— {doc_id}#{key}: {e}")
    return output

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    print("ğŸš€ é–‹å§‹åˆ†æå°è‚¡ç„¦é»è‚¡...\n")
    db = get_db()
    today = datetime.now(TAIWAN_TZ).strftime("%Y%m%d")
    os.makedirs("results", exist_ok=True)
    result_file = f"results/result_{today}.txt"

    results = []
    for i,(target,col,field) in enumerate([
        ("å°ç©é›»", NEWS_COLLECTION_TSMC, "Groq_result"),
        ("é´»æµ·", NEWS_COLLECTION_FOX, "Groq_result_Foxxcon"),
        ("è¯é›»", NEWS_COLLECTION_UMC, "Groq_result_UMC"),
    ]):
        print(f"ğŸ“ˆ åˆ†æï¼š{target}")
        res = analyze_target(db, col, target, field)
        results.append(f"{res.strip()}\n")
        if i < 2:
            print("="*70)

    final_output = "\n" + ("="*70 + "\n").join(results)
    with open(result_file, "w", encoding="utf-8") as f:
        f.write(final_output)

    print(f"\nâœ… çµæœå·²å„²å­˜è‡³ï¼š{result_file}")

if __name__ == "__main__":
    main()

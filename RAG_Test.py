# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
æ›´æ–°å…§å®¹ï¼š
- å»é™¤é‡è¤‡å‘½ä¸­ token
- æ”¹ç”¨ã€Œåå‘ä¸Šæ¼²ï¼åå‘ä¸‹è·Œï¼æŒå¹³ã€
- token åˆ†æ•¸é–€æª»æ”¹ç‚º 1.0
- çµæœå„²å­˜æç¤ºåªåœ¨æœ€å¾Œé¡¯ç¤ºä¸€æ¬¡
"""

import os, signal, regex as re, sys, io
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Dict
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

# ---------- è¨­å®š ----------
SILENT_MODE = False
MAX_DISPLAY_NEWS = 5
TAIWAN_TZ = timezone(timedelta(hours=8))
SCORE_THRESHOLD = float(os.getenv("SCORE_THRESHOLD", "1.0"))  # æ”¹ç‚º 1.0 åˆ†é–€æª»
LOOKBACK_DAYS = int(os.getenv("LOOKBACK_DAYS", "2"))

# ---------- è®€ .env ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)

TOKENS_COLLECTION = os.getenv("FIREBASE_TOKENS_COLLECTION", "bull_tokens")
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- è³‡æ–™çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def first_n_sentences(text: str, n: int = 3) -> str:
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    parts = [p for p in parts if p.strip()]
    joined = "".join(parts[:n])
    if not re.search(r'[ã€‚\.ï¼!\?ï¼Ÿï¼›;]$', joined):
        joined += "..."
    return joined

def parse_docid_time(doc_id: str):
    try:
        if "_" in doc_id:
            return datetime.strptime(doc_id, "%Y%m%d_%H%M%S").replace(tzinfo=TAIWAN_TZ)
        return datetime.strptime(doc_id, "%Y%m%d").replace(tzinfo=TAIWAN_TZ)
    except:
        return None

# ---------- åˆå§‹åŒ– ----------
def get_db(): return firestore.Client()
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ---------- Token è™•ç† ----------
def load_tokens(db):
    pos, neg = [], []
    for d in db.collection(TOKENS_COLLECTION).stream():
        data = d.to_dict() or {}
        t = Token(
            polarity=data.get("polarity",""),
            ttype=data.get("type","substr"),
            pattern=data.get("pattern",""),
            weight=float(data.get("weight",1.0)),
            note=data.get("note","")
        )
        if t.polarity.lower() == "positive":
            pos.append(t)
        elif t.polarity.lower() == "negative":
            neg.append(t)
    return pos, neg

# ---------- æ‰“åˆ† ----------
def score_text(text: str, pos_tokens, neg_tokens) -> MatchResult:
    text_norm = normalize(text)
    score, hits = 0.0, []
    seen = set()  # å»é™¤é‡è¤‡å‘½ä¸­ token
    for t in pos_tokens + neg_tokens:
        w = t.weight if t.polarity == "positive" else -abs(t.weight)
        matched = re.search(t.pattern, text_norm, re.I) if t.ttype == "regex" else t.pattern.lower() in text_norm
        if matched and t.pattern not in seen:
            seen.add(t.pattern)
            hits.append((t.pattern, w, t.note))
            score += w
    return MatchResult(score, hits)

# ---------- Groq ç¸½çµ ----------
def groq_analyze(news_list, target):
    if not news_list:
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šæŒå¹³ âšª\nåŸå› ï¼šç„¡ç›¸é—œæ–°è"

    text_block = "\n".join([f"{i+1}. {first_n_sentences(n)}" for i, n in enumerate(news_list)])
    prompt = f"""ä½ æ˜¯ä¸€ä½å°è‚¡åˆ†æå¸«ã€‚æ ¹æ“šä»¥ä¸‹{target}ç›¸é—œæ–°èï¼Œè«‹åˆ¤æ–·æ˜æ—¥{target}è‚¡åƒ¹èµ°å‹¢ã€‚
å›è¦†æ ¼å¼ï¼š
æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š<åå‘ä¸Šæ¼²/åå‘ä¸‹è·Œ/æŒå¹³> ğŸ”¼ğŸ”½âšª
åŸå› ï¼š<40å­—å…§ç°¡è¿°>

{text_block}
"""
    try:
        resp = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role":"system","content":"ä½ æ˜¯å°ˆæ¥­è‚¡å¸‚åˆ†æå¸«ï¼Œå›ç­”ç°¡æ½”æº–ç¢ºã€‚"},
                {"role":"user","content":prompt}
            ],
            temperature=0.0,
            max_tokens=120,
        )
        ans = resp.choices[0].message.content.strip()
        ans = re.sub(r"ä¸æ˜ç¢º", "æŒå¹³ âšª", ans)
        return ans
    except Exception as e:
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šæŒå¹³ âšª\nåŸå› ï¼šGroqåˆ†æå¤±æ•—({type(e).__name__})"

# ---------- ä¸»åˆ†æ ----------
def analyze_target(db, collection, target, result_field):
    pos, neg = load_tokens(db)
    items = db.collection(collection).stream()
    now = datetime.now(TAIWAN_TZ)
    start = now - timedelta(days=LOOKBACK_DAYS)

    output_lines, groq_inputs, filtered = [], [], []

    for d in items:
        t = parse_docid_time(d.id)
        if not t or t < start: continue
        data = d.to_dict() or {}
        for k,v in data.items():
            if not isinstance(v,dict): continue
            title, content = v.get("title",""), v.get("content","")
            full = title + " " + content
            res = score_text(full, pos, neg)
            if abs(res.score) < SCORE_THRESHOLD or not res.hits:
                continue
            trend = "âœ… æ˜æ—¥å¯èƒ½å¤§æ¼²" if res.score > 0 else "âŒ æ˜æ—¥å¯èƒ½ä¸‹è·Œ"
            hit_lines = [f"  {'+' if w>0 else '-'} {p}ï¼ˆ{n}ï¼‰" for p,w,n in res.hits]
            part = f"[{d.id}#{k}]\næ¨™é¡Œï¼š{first_n_sentences(title)}\n{trend}\nå‘½ä¸­ï¼š\n" + "\n".join(hit_lines)
            output_lines.append(part+"\n")
            groq_inputs.append(full)
            filtered.append((d.id, k, res))

    if not filtered:
        return f"{target}ï¼šç„¡æ˜é¡¯è®ŠåŒ–\n"

    groq_result = groq_analyze(groq_inputs, target)
    output = "\n".join(output_lines) + "\n" + groq_result + "\n"
    return output

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    print("ğŸš€ é–‹å§‹åˆ†æå°è‚¡ç„¦é»è‚¡...\n")
    db = get_db()
    today = datetime.now(TAIWAN_TZ).strftime("%Y%m%d")
    os.makedirs("results", exist_ok=True)
    result_file = f"results/result_{today}.txt"

    results = []
    for i,(target,col,field) in enumerate([
        ("å°ç©é›»", NEWS_COLLECTION_TSMC, "Groq_result"),
        ("é´»æµ·", NEWS_COLLECTION_FOX, "Groq_result_Foxxcon"),
        ("è¯é›»", NEWS_COLLECTION_UMC, "Groq_result_UMC"),
    ]):
        print(f"ğŸ“ˆ åˆ†æï¼š{target}")
        res = analyze_target(db, col, target, field)
        results.append(f"{res.strip()}\n")
        if i < 2:
            print("="*70)

    final_output = "\n" + ("="*70 + "\n").join(results)
    with open(result_file, "w", encoding="utf-8") as f:
        f.write(final_output)

    print(f"\nâœ… çµæœå·²å„²å­˜è‡³ï¼š{result_file}")

if __name__ == "__main__":
    main()

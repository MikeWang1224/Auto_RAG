# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èžåˆ†æžå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
è¼¸å‡ºæ ¼å¼ç²¾ç°¡ç‰ˆï¼šå–ç”¨æ–°èž + åå‘ + ç¸½åˆ†
"""

import os, signal, regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

SILENT_MODE = True
TAIWAN_TZ = timezone(timedelta(hours=8))

TOKENS_COLLECTION = "bull_tokens"
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"

SENSITIVE_WORDS = {
    "æ³•èªª": 1.5, "è²¡å ±": 1.4, "æ–°å“": 1.3, "åˆä½œ": 1.3, "ä½µè³¼": 1.4,
    "æŠ•è³‡": 1.3, "åœå·¥": 1.6, "ä¸‹ä¿®": 1.5, "åˆ©ç©º": 1.5, "çˆ†æ–™": 1.4,
    "ç‡Ÿæ”¶": 1.3, "å±•æœ›": 1.2,
}

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- åˆå§‹åŒ– ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

def get_db():
    return firestore.Client()

def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def parse_docid_time(doc_id: str):
    m = re.match(r"^(?P<ymd>\d{8})(?:_(?P<hms>\d{6}))?$", doc_id or "")
    if not m:
        return None
    ymd, hms = m.group("ymd"), m.group("hms") or "000000"
    try:
        return datetime.strptime(ymd + hms, "%Y%m%d%H%M%S").replace(tzinfo=TAIWAN_TZ)
    except:
        return None

# ---------- Token ----------
def load_tokens(db):
    pos, neg = [], []
    for d in db.collection(TOKENS_COLLECTION).stream():
        data = d.to_dict() or {}
        pol = data.get("polarity", "").lower()
        ttype = data.get("type", "substr").lower()
        patt = data.get("pattern", "")
        note = data.get("note", "")
        w = float(data.get("weight", 1.0))
        if pol == "positive":
            pos.append(Token(pol, ttype, patt, w, note))
        elif pol == "negative":
            neg.append(Token(pol, ttype, patt, -abs(w), note))
    return pos, neg

def compile_tokens(tokens: List[Token]):
    compiled = []
    for t in tokens:
        if t.ttype == "regex":
            try:
                compiled.append(("regex", re.compile(t.pattern, re.I), t.weight, t.note, t.pattern))
            except:
                continue
        else:
            compiled.append(("substr", None, t.weight, t.note, t.pattern.lower()))
    return compiled

def score_text(text: str, pos_c, neg_c, target: str = None) -> float:
    norm = normalize(text)
    score = 0.0
    aliases = {"å°ç©é›»": ["å°ç©é›»", "tsmc", "2330"],
               "é´»æµ·": ["é´»æµ·", "foxconn", "2317", "å¯Œå£«åº·"],
               "è¯é›»": ["è¯é›»", "umc", "2303"]}
    company_pattern = "|".join(re.escape(a) for a in aliases.get(target, []))
    if not re.search(company_pattern, norm):
        return 0.0
    for ttype, cre, w, note, patt in pos_c + neg_c:
        matched = cre.search(norm) if ttype == "regex" else patt in norm
        if matched:
            score += w
    return score

def adjust_score_for_context(text: str, base_score: float) -> float:
    if not text or base_score == 0:
        return base_score
    norm = text.lower()
    neutral_phrases = ["é‡ç”³", "ç¬¦åˆé æœŸ", "é æœŸå…§", "ä¸­æ€§çœ‹å¾…", "ç„¡é‡å¤§å½±éŸ¿", "æŒå¹³", "æœªè®Š"]
    if any(p in norm for p in neutral_phrases):
        base_score *= 0.4
    positive_boost = ["å‰µæ–°é«˜", "å€å¢ž", "å¤§å¹…æˆé•·", "ç²åˆ©æš´å¢ž", "å ±å–œ"]
    negative_boost = ["æš´è·Œ", "ä¸‹æ»‘", "è™§æ", "åœå·¥", "ä¸‹ä¿®", "è£å“¡", "è­¦è¨Š"]
    if any(p in norm for p in positive_boost):
        base_score *= 1.3
    if any(p in norm for p in negative_boost):
        base_score *= 1.3
    return base_score

# ---------- åˆ†æžå‡½å¼ ----------
def analyze_target(db, collection, target, result_field):
    pos, neg = load_tokens(db)
    pos_c, neg_c = compile_tokens(pos), compile_tokens(neg)
    today = datetime.now(TAIWAN_TZ).date()

    filtered = []
    for d in db.collection(collection).stream():
        dt = parse_docid_time(d.id)
        if not dt: continue
        delta_days = (today - dt.date()).days
        if delta_days > 2: continue

        day_weight = 1.0 if delta_days == 0 else 0.85 if delta_days == 1 else 0.7
        data = d.to_dict() or {}
        for k, v in data.items():
            if not isinstance(v, dict): continue
            title, content = v.get("title", ""), v.get("content", "")
            full = title + " " + content
            score = score_text(full, pos_c, neg_c, target)
            if score == 0: continue
            adj_score = adjust_score_for_context(full, score)
            filtered.append((d.id, k, title, adj_score * day_weight))

    if filtered:
        filtered.sort(key=lambda x: abs(x[3]), reverse=True)
        top_news = filtered[:10]

        print(f"\nðŸ“° {target} è¿‘æœŸé‡é»žæ–°èžï¼ˆå–ç”¨ï¼‰ï¼š")
        for docid, key, title, _ in top_news:
            print(f"[{docid}#{key}] {title}")

        avg_score = sum(s for _, _, _, s in top_news) / len(top_news)
        if avg_score >= 2:
            trend = "ä¸Šæ¼² ðŸ”¼"
        elif 0 < avg_score < 2:
            trend = "å¾®æ¼² â†—ï¸"
        elif -2 < avg_score <= 0:
            trend = "å¾®è·Œ â†˜ï¸"
        elif avg_score <= -2:
            trend = "ä¸‹è·Œ ðŸ”½"
        else:
            trend = "ä¸æ˜Žç¢º âš–ï¸"

        print(f"\næ˜Žæ—¥åå‘ï¼š{trend}")
        print(f"ç¸½åˆ†ï¼š{int(round(avg_score))}\n")

        try:
            db.collection(result_field).document(today.strftime("%Y%m%d")).set({
                "timestamp": datetime.now(TAIWAN_TZ).isoformat(),
                "trend": trend,
                "score": int(round(avg_score)),
                "news_list": [{"docid": d, "key": k, "title": t} for d, k, t, _ in top_news]
            })
        except Exception as e:
            print(f"[warning] Firestore å¯«å›žå¤±æ•—ï¼š{e}")

    else:
        print(f"\nðŸ“° {target} è¿‘æœŸé‡é»žæ–°èžï¼šç„¡å¯ç”¨æ–°èž")
        print(f"æ˜Žæ—¥åå‘ï¼šä¸æ˜Žç¢º âš–ï¸")
        print(f"ç¸½åˆ†ï¼š0\n")

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    if not SILENT_MODE:
        print("ðŸš€ é–‹å§‹åˆ†æžå°è‚¡ç„¦é»žè‚¡...\n")

    db = get_db()
    analyze_target(db, NEWS_COLLECTION_TSMC, "å°ç©é›»", "Groq_result")
    analyze_target(db, NEWS_COLLECTION_FOX, "é´»æµ·", "Groq_result_Foxxcon")
    analyze_target(db, NEWS_COLLECTION_UMC, "è¯é›»", "Groq_result_UMC")

if __name__ == "__main__":
    main()

# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
"""
import os, signal, regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

# ---------- è¨­å®š ----------
SILENT_MODE = True
TAIWAN_TZ = timezone(timedelta(hours=8))

TOKENS_COLLECTION = "bull_tokens"
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"

# å…§éƒ¨æ•æ„Ÿè©è¡¨ï¼ˆèˆŠç‰ˆä¿ç•™ï¼‰
SENSITIVE_WORDS = {
    "æ³•èªª": 1.5,
    "è²¡å ±": 1.4,
    "æ–°å“": 1.3,
    "åˆä½œ": 1.3,
    "ä½µè³¼": 1.4,
    "æŠ•è³‡": 1.3,
    "åœå·¥": 1.6,
    "ä¸‹ä¿®": 1.5,
    "åˆ©ç©º": 1.5,
    "çˆ†æ–™": 1.4,
    "ç‡Ÿæ”¶": 1.3,
    "å±•æœ›": 1.2,
}

# ç¡¬è¦å‰‡åŠ æ¬Š
HARD_WEIGHTS_POS = {
    "è²¡å ±": 1.5,
    "æ³•èªª": 1.5,
    "å±•æœ›": 1.5,
    "è³‡æœ¬æ”¯å‡º": 1.5,
    "è¨‚å–®": 1.2,
    "æ“´ç”¢": 1.2,
    "çˆ†å–®": 1.2,
    "æ¼²åƒ¹": 1.2,
}
HARD_WEIGHTS_NEG = {
    "åœå·¥": -1.5,
    "è£å“¡": -1.5,
    "è™§æ": -1.5,
    "ä¸‹ä¿®": -1.5,
}

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- åˆå§‹åŒ– ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ---------- è³‡æ–™çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
def get_db():
    return firestore.Client()

def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def first_n_sentences(text: str, n: int = 3) -> str:
    if not text:
        return ""
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    return "".join(parts[:n]) + ("..." if len(parts) > n else "")

def parse_docid_time(doc_id: str):
    m = re.match(r"^(?P<ymd>\d{8})(?:_(?P<hms>\d{6}))?$", doc_id or "")
    if not m:
        return None
    ymd, hms = m.group("ymd"), m.group("hms") or "000000"
    try:
        return datetime.strptime(ymd + hms, "%Y%m%d%H%M%S").replace(tzinfo=TAIWAN_TZ)
    except:
        return None

def parse_price_change(raw: str) -> float:
    if not raw:
        return 0.0
    m = re.search(r"\(([-+]?[\d\.]+)%\)", raw)
    if not m:
        return 0.0
    try:
        return float(m.group(1)) / 100.0
    except:
        return 0.0

# ---------- Token ----------
def load_tokens(db):
    pos, neg = [], []
    for d in db.collection(TOKENS_COLLECTION).stream():
        data = d.to_dict() or {}
        pol = data.get("polarity", "").lower()
        ttype = data.get("type", "substr").lower()
        patt = data.get("pattern", "")
        note = data.get("note", "")
        w = float(data.get("weight", 1.0))
        if pol == "positive":
            pos.append(Token(pol, ttype, patt, w, note))
        elif pol == "negative":
            neg.append(Token(pol, ttype, patt, -abs(w), note))
    return pos, neg

def compile_tokens(tokens: List[Token]):
    compiled = []
    for t in tokens:
        if t.ttype == "regex":
            try:
                compiled.append(("regex", re.compile(t.pattern, re.I), t.weight, t.note, t.pattern))
            except:
                continue
        else:
            compiled.append(("substr", None, t.weight, t.note, t.pattern.lower()))
    return compiled

def score_text(text: str, pos_c, neg_c, target: str = None) -> MatchResult:
    norm = normalize(text)
    score, hits, seen = 0.0, [], set()
    aliases = {"å°ç©é›»": ["å°ç©é›»", "tsmc", "2330"],
               "é´»æµ·": ["é´»æµ·", "foxconn", "2317", "å¯Œå£«åº·"],
               "è¯é›»": ["è¯é›»", "umc", "2303"]}
    company_pattern = "|".join(re.escape(a) for a in aliases.get(target, []))
    if not re.search(company_pattern, norm):
        return MatchResult(0.0, [])
    for ttype, cre, w, note, patt in pos_c + neg_c:
        key = (patt, note)
        if key in seen:
            continue
        matched = cre.search(norm) if ttype == "regex" else patt in norm
        if matched:
            score += w
            hits.append((patt, w, note))
            seen.add(key)
    return MatchResult(score, hits)

def adjust_score_for_context(text: str, base_score: float) -> float:
    if not text or base_score == 0:
        return base_score
    norm = text.lower()
    neutral_phrases = ["é‡ç”³", "ç¬¦åˆé æœŸ", "é æœŸå…§", "ä¸­æ€§çœ‹å¾…", "ç„¡é‡å¤§å½±éŸ¿", "æŒå¹³", "æœªè®Š"]
    if any(p in norm for p in neutral_phrases):
        base_score *= 0.4
    positive_boost = ["å‰µæ–°é«˜", "å€å¢", "å¤§å¹…æˆé•·", "ç²åˆ©æš´å¢", "å ±å–œ"]
    negative_boost = ["æš´è·Œ", "ä¸‹æ»‘", "è™§æ", "åœå·¥", "ä¸‹ä¿®", "è£å“¡", "è­¦è¨Š"]
    if any(p in norm for p in positive_boost):
        base_score *= 1.3
    if any(p in norm for p in negative_boost):
        base_score *= 1.3
    return base_score

# ---------- å¸‚å ´èª¿æ•´èˆ‡èƒŒé›¢ ----------
def adjust_by_market(avg_score: float, today_change: float) -> float:
    if today_change >= 0.03:
        return avg_score + 0.5
    if today_change <= -0.03:
        return avg_score - 0.5
    if today_change >= 0.01:
        return avg_score + 0.2
    if today_change <= -0.01:
        return avg_score - 0.2
    return avg_score

def detect_divergence(avg_score: float, today_change: float) -> str:
    if avg_score > 1.0 and today_change < -0.02:
        return "åˆ©å¤šä¸æ¼²ï¼ˆç–‘ä¼¼ä¸»åŠ›å‡ºè²¨ï¼‰"
    if avg_score < -1.0 and today_change > 0.02:
        return "åˆ©ç©ºä¸è·Œï¼ˆå¯èƒ½æœ‰éš±æ€§åˆ©å¤šï¼‰"
    return "ç„¡æ˜é¡¯èƒŒé›¢"

# ---------- ç¡¬è¦å‰‡æ±ºç­– fallback ----------
def decide_by_hard_rules(news_list: List[Tuple[str, float]], today_change: float) -> str:
    if not news_list:
        return "æ˜å¤©è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš–ï¸\næƒ…ç·’åˆ†æ•¸ï¼š0"
    total_score = sum(1.0 if s>0 else -1.0 if s<0 else 0 for _, s in news_list)
    standardized = total_score / (len(news_list)+1)
    if standardized >= 1.0:
        trend, symbol = "å¾®æ¼²", "â†—ï¸"
    elif standardized > -1.0:
        trend, symbol = "å¾®è·Œ", "â†˜ï¸"
    else:
        trend, symbol = "ä¸‹è·Œ", "ğŸ”½"
    mood_score = max(-10, min(10, int(round(standardized*3))))
    return f"æ˜å¤©è‚¡åƒ¹èµ°å‹¢ï¼š{trend} {symbol}\næƒ…ç·’åˆ†æ•¸ï¼š{mood_score:+d}"

# ---------- Groq LLM åˆ†æ ----------
def groq_llm_analyze(news_list: List[Tuple[str, float]], target: str, today_change: float) -> Tuple[str, str]:
    if not news_list:
        fallback_result = f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš–ï¸\næƒ…ç·’åˆ†æ•¸ï¼š0"
        return fallback_result, "è¿‘ä¸‰æ—¥ç„¡ç›¸é—œæ–°èï¼Œä¾å¸‚å ´è³‡è¨Šæ¨ç®—"

    prompt = f"ä½ æ˜¯ä¸€å€‹è‚¡ç¥¨æ–°èåˆ†æå°ˆå®¶ã€‚\nå€‹è‚¡ï¼š{target}\nä»Šæ—¥è‚¡åƒ¹æ¼²è·Œå¹…ï¼š{today_change*100:.2f}%\næœ€è¿‘æ–°èæ¨™é¡ŒåŠåˆ†æ•¸ï¼š\n"
    for i, (title, score) in enumerate(news_list):
        prompt += f"{i+1}. {title} (score: {score:+.2f})\n"
    prompt += "\nè«‹ç”Ÿæˆæ˜å¤©è‚¡åƒ¹èµ°å‹¢èˆ‡æƒ…ç·’åˆ†æ•¸ï¼Œä¸¦ç°¡è¿°åŸå› ï¼ˆ40å­—å…§ï¼‰"

    try:
        resp = client.chat.completions.create(
            model="llama-3.1-70b-versatile",
            messages=[{"role": "user", "content": prompt}],
            max_output_tokens=200,
        )
        text = resp.choices[0].message.content.strip()
        groq_result_lines = []
        reason_short = ""
        for line in text.split("\n"):
            line = line.strip()
            if not line:
                continue
            if "åŸå› " in line:
                reason_short = line.split("åŸå› ")[-1].strip()[:40]
            else:
                groq_result_lines.append(line)
        groq_result = "\n".join(groq_result_lines)
        if not reason_short:
            reason_short = "ä¾æ–°èæƒ…ç·’èˆ‡å¸‚å ´èª¿æ•´æ¨ç®—æ˜æ—¥æ–¹å‘"
        return groq_result, reason_short
    except Exception as e:
        print(f"[warning] Groq LLM å‘¼å«å¤±æ•—ï¼š{e}")
        fallback_result = decide_by_hard_rules(news_list, today_change)
        return fallback_result, "ä¾æ–°èæƒ…ç·’èˆ‡å¸‚å ´èª¿æ•´æ¨ç®—æ˜æ—¥æ–¹å‘"

# ---------- ä¸»åˆ†æ ----------
def analyze_target(db, collection, target, result_field):
    pos, neg = load_tokens(db)
    pos_c, neg_c = compile_tokens(pos), compile_tokens(neg)
    today = datetime.now(TAIWAN_TZ).date()
    filtered, weighted_scores = [], []
    today_price_change = 0.0

    # å…ˆæƒä¸€æ¬¡ collection å–å¾—ä»Šæ—¥ price_change
    try:
        for d in db.collection(collection).stream():
            dt = parse_docid_time(d.id)
            if not dt or dt.date() != today:
                continue
            data = d.to_dict() or {}
            for k, v in data.items():
                if isinstance(v, dict) and "price_change" in v:
                    today_price_change = parse_price_change(v.get("price_change"))
                    break
            if today_price_change != 0.0:
                break
    except Exception:
        today_price_change = 0.0

    # æ–°èæ‰“åˆ†æµç¨‹
    for d in db.collection(collection).stream():
        dt = parse_docid_time(d.id)
        if not dt or (today - dt.date()).days > 2:
            continue
        day_weight = 1.0 if (today - dt.date()).days == 0 else 0.85 if (today - dt.date()).days == 1 else 0.7
        data = d.to_dict() or {}
        for k, v in data.items():
            if not isinstance(v, dict):
                continue
            title, content = v.get("title", ""), v.get("content", "")
            full = title + " " + content
            res = score_text(full, pos_c, neg_c, target)
            if not res.hits:
                continue
            adj_score = adjust_score_for_context(full, res.score)
            token_weight = 1.0 + min(len(res.hits)*0.05, 0.3)
            impact = 1.0 + sum(w*0.05 for k_sens, w in SENSITIVE_WORDS.items() if k_sens in full)
            total_weight = day_weight * token_weight * impact
            filtered.append((d.id, k, title, full, res, total_weight))
            weighted_scores.append(adj_score * total_weight)

    # å»é‡æ–°è & top 10
    seen_text = set()
    top_news = []
    for docid, key, title, full, res, weight in sorted(filtered, key=lambda x: abs(x[4].score*x[5]), reverse=True):
        news_text = normalize(full)
        if news_text in seen_text:
            continue
        seen_text.add(news_text)
        top_news.append((docid, key, title, res, weight, full))
        if len(top_news) >= 10:
            break

    # æ§‹é€  news_with_scores
    news_with_scores = [(title, res.score*weight) for _, _, title, res, weight, _ in top_news]

    # ä½¿ç”¨ Groq LLM
    groq_result, reason_short = groq_llm_analyze(news_with_scores, target, today_price_change)

    # Firestore å¯«å›
    try:
        db.collection(result_field).document(today.strftime("%Y%m%d")).set({
            "timestamp": datetime.now(TAIWAN_TZ).isoformat(),
            "groq_result": groq_result,
            "reason_short": reason_short,
        })
    except Exception as e:
        print(f"[warning] Firestore å¯«å›å¤±æ•—ï¼š{e}")

    print(f"\n{groq_result}\nåŸå› ç°¡çŸ­ï¼š{reason_short}\n")

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    if not SILENT_MODE:
        print("ğŸš€ é–‹å§‹åˆ†æå°è‚¡ç„¦é»è‚¡ï¼ˆGroq LLM + å¸‚å ´èª¿æ•´ï¼‰...\n")
    db = get_db()
    analyze_target(db, NEWS_COLLECTION_TSMC, "å°ç©é›»", "Groq_result")
    print("="*70)
    analyze_target(db, NEWS_COLLECTION_FOX, "é´»æµ·", "Groq_result_Foxxcon")
    print("="*70)
    analyze_target(db, NEWS_COLLECTION_UMC, "è¯é›»", "Groq_result_UMC")

if __name__ == "__main__":
    main()

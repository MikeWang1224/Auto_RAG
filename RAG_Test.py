# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ·ï¼‰
æ”¹é€²ï¼šæ–°èå‘½ä¸­åªåˆ†æåŒ…å«å…¬å¸åç¨±çš„å¥å­ï¼Œé¿å…äº¤å‰èª¤åˆ¤ã€‚
"""

import os, signal, time, regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Dict
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

# ---------- è®€ .env ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)
    print(f"[info] å·²è¼‰å…¥ .envï¼š{os.path.abspath('.env')}")
else:
    load_dotenv(override=True)
    print("[info] æœªæ‰¾åˆ° .envï¼Œæ”¹ç”¨ç³»çµ±ç’°å¢ƒè®Šæ•¸")

# ---------- å¸¸æ•¸ ----------
TOKENS_COLLECTION = os.getenv("FIREBASE_TOKENS_COLLECTION", "bull_tokens")
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
SCORE_THRESHOLD = float(os.getenv("SCORE_THRESHOLD", "3.0"))
LOOKBACK_DAYS = int(os.getenv("LOOKBACK_DAYS", "3"))
TAIWAN_TZ = timezone(timedelta(hours=8))

COMPANY_KEYWORDS = {
    "å°ç©é›»": ["å°ç©é›»", "TSMC", "2330"],
    "é´»æµ·": ["é´»æµ·", "Foxconn", "2317"],
}

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
DOCID_RE = re.compile(r"^(?P<ymd>\d{8})_(?P<hms>\d{6})$")
def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def shorten_text(t: str, n=200):
    return t[:n] + "â€¦" if len(t) > n else t

def first_n_sentences(text: str, n: int = 3) -> str:
    if not text:
        return ""
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    parts = [p for p in parts if p.strip()]
    if not parts:
        return text.strip()
    selected = parts[:n]
    joined = "".join(selected)
    if not re.search(r'[ã€‚\.ï¼!\?ï¼Ÿï¼›;]\s*$', joined):
        joined = joined + "..."
    return joined

def parse_docid_time(doc_id: str):
    m = DOCID_RE.match(doc_id)
    if not m:
        return None
    try:
        return datetime.strptime(m.group("ymd")+m.group("hms"), "%Y%m%d%H%M%S").replace(tzinfo=TAIWAN_TZ)
    except:
        return None

def extract_company_related_text(text: str, company: str) -> str:
    """
    å¾æ–‡ç« ä¸­å–å‡ºåŒ…å«è©²å…¬å¸é—œéµå­—çš„å¥å­ã€‚
    è‹¥æ²’æ‰¾åˆ°ï¼Œå›å‚³ç©ºå­—ä¸²ï¼ˆä»£è¡¨è©²æ–°èèˆ‡å…¬å¸ç„¡é—œï¼‰ã€‚
    """
    keywords = COMPANY_KEYWORDS.get(company, [company])
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›.!?]', text)
    related = [s.strip() for s in sentences if any(kw in s for kw in keywords)]
    return "ã€‚".join(related)

# ---------- Firestore ----------
def get_db():
    return firestore.Client()

def load_tokens(db, col) -> Tuple[List[Token], List[Token]]:
    pos, neg = [], []
    for d in db.collection(col).stream():
        data = d.to_dict() or {}
        pol = (data.get("polarity") or "").lower()
        ttype = (data.get("type") or "substr").lower()
        patt = str(data.get("pattern") or "")
        note = str(data.get("note") or "")
        try:
            w = float(data.get("weight", 1.0))
        except:
            w = 1.0
        if not patt or pol not in ("positive","negative"):
            continue
        (pos if pol=="positive" else neg).append(Token(pol, ttype, patt, w, note))
    return pos, neg

def load_news_items(db, col_name: str, days: int) -> List[Dict]:
    items, seen = [], set()
    now = datetime.now(TAIWAN_TZ)
    start = now - timedelta(days=days)
    for d in db.collection(col_name).stream():
        dt = parse_docid_time(d.id)
        if dt and dt < start:
            continue
        data = d.to_dict() or {}
        for k, v in data.items():
            if not (k.startswith("news_") and isinstance(v, dict)):
                continue
            title = str(v.get("title") or "")
            content = str(v.get("content") or "")
            if not title and not content:
                continue
            uniq = f"{title}|{content}"
            if uniq in seen:
                continue
            seen.add(uniq)
            items.append({"id": f"{d.id}#{k}", "title": title, "content": content, "ts": dt})
    items.sort(key=lambda x: x["ts"] or datetime.min.replace(tzinfo=TAIWAN_TZ), reverse=True)
    return items

# ---------- Token æ‰“åˆ† ----------
def compile_tokens(tokens: List[Token]):
    out = []
    for t in tokens:
        w = t.weight if t.polarity == "positive" else -abs(t.weight)
        if t.ttype == "regex":
            try:
                cre = re.compile(t.pattern, flags=re.IGNORECASE)
                out.append(("regex", cre, w, t.note, t.pattern))
            except:
                continue
        else:
            out.append(("substr", None, w, t.note, t.pattern.lower()))
    return out

def score_text(text: str, pos_c, neg_c) -> MatchResult:
    norm = normalize(text)
    score, hits, seen = 0.0, [], set()
    for ttype, cre, w, note, patt in pos_c + neg_c:
        key = (ttype, patt)
        if key in seen:
            continue
        matched = cre.search(norm) if ttype == "regex" else patt in norm
        if matched:
            score += w
            hits.append((patt, w, note))
            seen.add(key)
    return MatchResult(score, hits)

# ---------- Groq ----------
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

def prepare_news_for_llm(news_items: List[str]) -> str:
    return "\n".join(f"æ–°è {i}ï¼š\n{shorten_text(t)}\n" for i,t in enumerate(news_items,1))

def ollama_analyze(texts: List[str], target: str) -> str:
    combined = prepare_news_for_llm(texts)
    prompt = f"""ä½ æ˜¯ä¸€ä½å°ç£è‚¡å¸‚ç ”ç©¶å“¡ã€‚æ ¹æ“šä»¥ä¸‹æ–°èï¼Œåˆ¤æ–·ã€Œæ˜å¤©{target}è‚¡åƒ¹ã€æœ€å¯èƒ½èµ°å‹¢ã€‚
è«‹åªå›è¦†ä»¥ä¸‹å…©è¡Œæ ¼å¼ï¼ˆä¸è¦å¤šé¤˜æ–‡å­—ï¼‰ï¼š

æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š<ä¸Šæ¼² / ä¸‹è·Œ / ä¸æ˜ç¢º>
åŸå› ï¼š<40å­—ä»¥å…§ï¼Œä¸€å¥è©±ç°¡æ½”èªªæ˜ä¸»è¦ç†ç”±>

æ–°èæ‘˜è¦ï¼š
{combined}
"""
    try:
        resp = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­è‚¡å¸‚æ–°èåˆ†æå“¡ï¼Œå›ç­”ç°¡æ½”æº–ç¢ºã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.0,
            max_tokens=150,
        )
        raw = resp.choices[0].message.content.strip()
        cleaned = re.sub(r"^```(?:\w+)?|```$", "", raw).strip()
        cleaned = re.sub(r"\s+", " ", cleaned)
        return cleaned
    except Exception as e:
        return f"[error] Groq å‘¼å«å¤±æ•—ï¼š{e}"

# ---------- åˆ†æ ----------
def analyze_target(db, news_col: str, target: str, result_col: str):
    print(f"\n[info] ===== é–‹å§‹åˆ†æ {target} =====")
    pos, neg = load_tokens(db, TOKENS_COLLECTION)
    pos_c, neg_c = compile_tokens(pos), compile_tokens(neg)

    items = load_news_items(db, news_col, LOOKBACK_DAYS)
    if not items:
        print(f"[info] {news_col} ç„¡è³‡æ–™")
        return

    filtered, local_log = [], []
    terminal_logs = []

    for it in items:
        if STOP:
            break

        # ğŸŸ© åªå–èˆ‡è©²å…¬å¸æœ‰é—œçš„å¥å­
        text_raw = f"{it.get('title','')}ã€‚{it.get('content','')}"
        text_for_score = extract_company_related_text(text_raw, target)
        if not text_for_score.strip():
            continue  # ç„¡å…¬å¸ç›¸é—œå…§å®¹ â†’ è·³é

        res = score_text(text_for_score, pos_c, neg_c)
        if abs(res.score) >= SCORE_THRESHOLD:
            filtered.append((it, res))
            trend = "âœ… æ˜æ—¥å¯èƒ½å¤§æ¼²" if res.score > 0 else "âŒ æ˜æ—¥å¯èƒ½ä¸‹è·Œ"
            hits_text_lines = [f"  {'+' if w>0 else '-'} {patt}ï¼ˆ{note}ï¼‰" for patt, w, note in res.hits]
            hits_text = "\n".join(hits_text_lines)
            local_entry = f"[{it['id']}] {it.get('title','')}\n{trend}\nå‘½ä¸­ï¼š\n{hits_text}\n\nå…§æ–‡ï¼š\n{text_for_score}\n"
            local_log.append(local_entry)
            truncated_title = first_n_sentences(it.get("title",""), 3)
            terminal_entry_lines = [
                f"[{it['id']}]",
                f"æ¨™é¡Œï¼š{truncated_title}",
                f"{trend}",
                "å‘½ä¸­ï¼š",
            ] + hits_text_lines
            terminal_logs.append("\n".join(terminal_entry_lines) + "\n")

    print(f"[info] éæ¿¾å¾Œæ–°èï¼š{len(filtered)} / {len(items)}")
    if not filtered:
        print("[info] ç„¡ç¬¦åˆæ¢ä»¶çš„æ–°è")
        return

    # é¡¯ç¤ºå‰äº”å‰‡å‘½ä¸­æ–°è
    print("\n===== å‘½ä¸­æ–°èåˆ—è¡¨ï¼ˆå‰5å‰‡ï¼‰ =====")
    for t in terminal_logs[:5]:
        print(t)

    news_for_llm = [(x[0].get("content") or x[0].get("title") or "") for x in filtered]
    summary = ollama_analyze(news_for_llm, target)
    print("===== Groq åˆ†æ =====")
    print(summary)

    os.makedirs("result", exist_ok=True)
    date_str = datetime.now(TAIWAN_TZ).strftime("%Y%m%d_%H%M%S")
    local_path = f"result/{target}_{date_str}.txt"
    with open(local_path, "w", encoding="utf-8") as f:
        f.write("\n".join(local_log))
        f.write("\n" + "="*60 + "\n")
        f.write(summary + "\n")

    doc_id = datetime.now(TAIWAN_TZ).strftime("%Y%m%d")
    try:
        db.collection(result_col).document(doc_id).set({
            "timestamp": datetime.now(TAIWAN_TZ),
            "result": summary,
        })
    except Exception as e:
        print(f"[error] å¯«å…¥ Firebase å¤±æ•—ï¼š{e}")

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    db = get_db()
    analyze_target(db, NEWS_COLLECTION_TSMC, "å°ç©é›»", "Groq_result")

    print("\n" + "="*80 + "\n")  # ğŸŸ¥ åˆ†éš”ç·šï¼ˆå°ç©é›»ï¼é´»æµ·ï¼‰

    analyze_target(db, NEWS_COLLECTION_FOX, "é´»æµ·", "Groq_result_Foxxcon")

if __name__ == "__main__":
    main()

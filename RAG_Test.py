# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
åŠ å¼·ç‰ˆï¼ˆå«å»¶é²æ•ˆæ‡‰æ™‚é–“çª— + é«˜æ¬Šé‡æ–°èå‰äº”å‰‡å„ªå…ˆï¼‰ï¼š
âœ… åˆ†æã€Œä»Šæ—¥ + æ˜¨æ—¥ã€æ–°èï¼ˆ2å¤©å»¶é²æ•ˆæ‡‰ï¼‰
âœ… ä»Šæ—¥æ–°èæ¬Šé‡ = 1.0ã€æ˜¨æ—¥ = 0.7
âœ… ä¾åŠ æ¬Šå¾Œåˆ†æ•¸çµ•å°å€¼æ’åºå–å‰ 5 å‰‡æ–°èé€ Groq
âœ… çµ‚ç«¯å°å‡ºçµ¦ Groq çš„æ–°èæ¨™é¡Œï¼ˆæ–¹ä¾¿ç¢ºèªï¼‰
âœ… Groq æ°¸é æœƒåˆ†æï¼ˆå³ä½¿ç„¡æ–°èï¼‰
âœ… è‹¥ Groq å›ã€Œä¸æ˜ç¢ºã€ï¼Œä¾åŠ æ¬Šå¹³å‡åˆ†æ•¸è‡ªå‹•å¾®èª¿
âœ… è‚¡ç¥¨é–“è¼¸å‡ºç”¨ ======= åˆ†éš”
"""

import os, signal, regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

# ---------- è¨­å®š ----------
SILENT_MODE = True
TAIWAN_TZ = timezone(timedelta(hours=8))
SCORE_THRESHOLD = 1.5

TOKENS_COLLECTION = "bull_tokens"
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- åˆå§‹åŒ– ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ---------- çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
DOCID_RE = re.compile(r"^(?P<ymd>\d{8})(?:_(?P<hms>\d{6}))?$")

def get_db():
    return firestore.Client()

def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def first_n_sentences(text: str, n: int = 3) -> str:
    if not text:
        return ""
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    return "".join(parts[:n]) + ("..." if len(parts) > n else "")

def parse_docid_time(doc_id: str):
    m = DOCID_RE.match(doc_id or "")
    if not m:
        return None
    ymd = m.group("ymd")
    hms = m.group("hms") or "000000"
    try:
        return datetime.strptime(ymd + hms, "%Y%m%d%H%M%S").replace(tzinfo=TAIWAN_TZ)
    except:
        return None

# ---------- Token ----------
def load_tokens(db) -> Tuple[List[Token], List[Token]]:
    pos, neg = [], []
    for d in db.collection(TOKENS_COLLECTION).stream():
        data = d.to_dict() or {}
        pol = data.get("polarity", "").lower()
        ttype = data.get("type", "substr").lower()
        patt = data.get("pattern", "")
        note = data.get("note", "")
        w = float(data.get("weight", 1.0))
        if pol == "positive":
            pos.append(Token(pol, ttype, patt, w, note))
        elif pol == "negative":
            neg.append(Token(pol, ttype, patt, -abs(w), note))
    return pos, neg

def compile_tokens(tokens: List[Token]):
    compiled = []
    for t in tokens:
        if t.ttype == "regex":
            try:
                compiled.append(("regex", re.compile(t.pattern, re.I), t.weight, t.note, t.pattern))
            except:
                continue
        else:
            compiled.append(("substr", None, t.weight, t.note, t.pattern.lower()))
    return compiled

# ---------- Scoring ----------
def score_text(text: str, pos_c, neg_c, target: str = None) -> MatchResult:
    norm = normalize(text)
    score, hits, seen = 0.0, [], set()
    aliases = {
        "å°ç©é›»": ["å°ç©é›»", "tsmc", "2330"],
        "é´»æµ·": ["é´»æµ·", "foxconn", "2317", "å¯Œå£«åº·"],
        "è¯é›»": ["è¯é›»", "umc", "2303"]
    }
    company_pattern = "|".join(re.escape(a) for a in aliases.get(target, []))
    if not re.search(company_pattern, norm):
        return MatchResult(0.0, [])
    for ttype, cre, w, note, patt in pos_c + neg_c:
        key = (patt, note)
        if key in seen:
            continue
        matched = cre.search(norm) if ttype == "regex" else patt in norm
        if matched:
            score += w
            hits.append((patt, w, note))
            seen.add(key)
    return MatchResult(score, hits)

# ---------- Groq ----------
def groq_analyze(news_list: List[str], target: str) -> str:
    combined = "\n".join(f"{i+1}. {t}" for i, t in enumerate(news_list[:10]))
    prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­å°è‚¡åˆ†æå¸«ã€‚æ ¹æ“šä»¥ä¸‹{target}çš„è¿‘æœŸæ–°èå…§å®¹ï¼Œ
è«‹åˆ¤æ–·æ˜å¤©{target}è‚¡åƒ¹æœ€å¯èƒ½çš„æ–¹å‘ï¼ˆä¸Šæ¼²æˆ–ä¸‹è·Œï¼Œå¦‚çœŸçš„é›£åˆ¤æ–·å†é¸ä¸æ˜ç¢ºï¼‰ï¼š
å›å‚³æ ¼å¼å¦‚ä¸‹ï¼š
æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š<ä¸Šæ¼² / ä¸‹è·Œ / ä¸æ˜ç¢º>
åŸå› ï¼š<ä¸€å¥è©±40å­—å…§>

{combined}
"""
    try:
        resp = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­è‚¡å¸‚åˆ†æå¸«ï¼Œå›ç­”ç°¡æ½”æº–ç¢ºã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.1,
            max_tokens=100,
            timeout=20,
        )
        ans = resp.choices[0].message.content.strip()
        ans = re.sub(r"\s+", " ", ans)
        m_trend = re.search(r"(ä¸Šæ¼²|ä¸‹è·Œ|ä¸æ˜ç¢º)", ans)
        trend = m_trend.group(1) if m_trend else "ä¸æ˜ç¢º"
        symbol_map = {"ä¸Šæ¼²": "ğŸ”¼", "ä¸‹è·Œ": "ğŸ”½", "ä¸æ˜ç¢º": "âš–ï¸"}
        m_reason = re.search(r"(?:åŸå› |ç†ç”±)[:ï¼š]?\s*(.+)", ans)
        reason = m_reason.group(1) if m_reason else "å¸‚å ´è§€æœ›"
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š{trend} {symbol_map.get(trend,'')}\nåŸå› ï¼š{reason[:40]}"
    except Exception as e:
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šæŒå¹³ âš–ï¸\nåŸå› ï¼šGroqåˆ†æå¤±æ•—({e})"

# ---------- ä¸»åˆ†æ ----------
def analyze_target(db, collection: str, target: str, result_field: str):
    pos, neg = load_tokens(db)
    pos_c, neg_c = compile_tokens(pos), compile_tokens(neg)

    today = datetime.now(TAIWAN_TZ).date()
    filtered = []
    weighted_scores = []

    for d in db.collection(collection).stream():
        dt = parse_docid_time(d.id)
        if not dt:
            continue
        news_date = dt.date()
        delta_days = (today - news_date).days
        if delta_days > 1:
            continue
        weight = 1.0 if delta_days == 0 else 0.7

        data = d.to_dict() or {}
        for k, v in data.items():
            if not isinstance(v, dict):
                continue
            title, content = v.get("title", ""), v.get("content", "")
            full = title + " " + content
            res = score_text(full, pos_c, neg_c, target)
            if not res.hits:
                continue
            filtered.append((d.id, k, title, res, weight))
            weighted_scores.append(res.score * weight)

    # âœ… è‹¥å®Œå…¨æ²’æœ‰æ–°è
    if not filtered:
        print(f"{target}ï¼šè¿‘å…©æ—¥ç„¡æ–°èï¼Œäº¤ç”± Groq åˆ¤æ–·ã€‚\n")
        summary = groq_analyze(["è¿‘å…©æ—¥ç„¡ç›¸é—œæ–°èï¼Œè«‹ä¾å¸‚å ´æƒ…ç·’ä¼°è¨ˆã€‚"], target)
    else:
        # âœ… å–å‰äº”å‰‡ï¼ˆåŒ Groqï¼‰
        filtered.sort(key=lambda x: abs(x[3].score * x[4]), reverse=True)
        top_news = filtered[:5]
        news_texts = [t for _, _, t, _, _ in top_news]

        # âœ… çµ¦ Groq çš„æ–°èåˆ—è¡¨ï¼ˆæ–°å¢é€™æ®µï¼‰
        print(f"\nğŸ“° çµ¦ Groq åˆ†æçš„ {target} å‰äº”å‰‡æ–°èï¼š")
        for i, (docid, key, title, res, weight) in enumerate(top_news, 1):
            print(f"[{docid}#{key}] ({weight:.1f}x, åˆ†æ•¸={res.score:.2f}) {title}")

        # --- è¼¸å‡º .txt ---
        fname = f"result_{today.strftime('%Y%m%d')}.txt"
        with open(fname, "a", encoding="utf-8") as f:
            f.write(f"======= {target} =======\n")
            for docid, key, title, res, weight in top_news:
                trend = "âœ… æ˜æ—¥å¯èƒ½å¤§æ¼²" if res.score > 0 else "âŒ æ˜æ—¥å¯èƒ½ä¸‹è·Œ"
                hits_text = "\n".join([f"  {'+' if w>0 else '-'} {p}ï¼ˆ{n}ï¼‰" for p, w, n in res.hits])
                f.write(f"[{docid}#{key}]ï¼ˆ{weight:.1f}xï¼‰\næ¨™é¡Œï¼š{first_n_sentences(title)}\n{trend}\nå‘½ä¸­ï¼š\n{hits_text}\n\n")

        summary = groq_analyze(news_texts, target)

        # âœ… æ ¹æ“šå¹³å‡åˆ†æ•¸å¾®èª¿
        if weighted_scores:
            avg_score = sum(weighted_scores) / len(weighted_scores)
            if avg_score > 1.5:
                summary = re.sub(r"ä¸æ˜ç¢º", "ä¸Šæ¼²", summary)
            elif avg_score < -1.5:
                summary = re.sub(r"ä¸æ˜ç¢º", "ä¸‹è·Œ", summary)

    print(summary + "\n")

    try:
        db.collection(result_field).document(today.strftime("%Y%m%d")).set({
            "timestamp": datetime.now(TAIWAN_TZ).isoformat(),
            "result": summary,
        })
    except Exception as e:
        print(f"[warning] Firestore å¯«å›å¤±æ•—ï¼š{e}")

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    if not SILENT_MODE:
        print("ğŸš€ é–‹å§‹åˆ†æå°è‚¡ç„¦é»è‚¡ï¼ˆ2å¤©å»¶é²æ•ˆæ‡‰ç‰ˆ + å‰äº”å‰‡é«˜æ¬Šé‡æ–°èï¼‰...\n")

    db = get_db()
    analyze_target(db, NEWS_COLLECTION_TSMC, "å°ç©é›»", "Groq_result")
    print("=" * 70)
    analyze_target(db, NEWS_COLLECTION_FOX, "é´»æµ·", "Groq_result_Foxxcon")
    print("=" * 70)
    analyze_target(db, NEWS_COLLECTION_UMC, "è¯é›»", "Groq_result_UMC")

if __name__ == "__main__":
    main()

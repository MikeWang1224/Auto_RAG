# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
ä½¿ç”¨ Groq LLM åšæƒ…ç·’åˆ†æ
"""
import os, signal, regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq  # æœ€æ–° SDK

# ---------- è¨­å®š ----------
SILENT_MODE = True
TAIWAN_TZ = timezone(timedelta(hours=8))

TOKENS_COLLECTION = "bull_tokens"
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- åˆå§‹åŒ– ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)

client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ---------- çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
def get_db():
    return firestore.Client()

def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def first_n_sentences(text: str, n: int = 3) -> str:
    if not text:
        return ""
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    return "".join(parts[:n]) + ("..." if len(parts) > n else "")

def parse_docid_time(doc_id: str):
    m = re.match(r"^(?P<ymd>\d{8})(?:_(?P<hms>\d{6}))?$", doc_id or "")
    if not m:
        return None
    ymd, hms = m.group("ymd"), m.group("hms") or "000000"
    try:
        return datetime.strptime(ymd + hms, "%Y%m%d%H%M%S").replace(tzinfo=TAIWAN_TZ)
    except:
        return None

# ---------- æ–°å¢ï¼šè§£æ price_change ----------
def parse_price_change(raw: str) -> float:
    if not raw:
        return 0.0
    m = re.search(r"\(([-+]?[\d\.]+)%\)", raw)
    if not m:
        return 0.0
    try:
        return float(m.group(1)) / 100.0
    except:
        return 0.0

# ---------- Token ----------
def load_tokens(db):
    pos, neg = [], []
    for d in db.collection(TOKENS_COLLECTION).stream():
        data = d.to_dict() or {}
        pol = data.get("polarity", "").lower()
        ttype = data.get("type", "substr").lower()
        patt = data.get("pattern", "")
        note = data.get("note", "")
        w = float(data.get("weight", 1.0))
        if pol == "positive":
            pos.append(Token(pol, ttype, patt, w, note))
        elif pol == "negative":
            neg.append(Token(pol, ttype, patt, -abs(w), note))
    return pos, neg

def compile_tokens(tokens: List[Token]):
    compiled = []
    for t in tokens:
        if t.ttype == "regex":
            try:
                compiled.append(("regex", re.compile(t.pattern, re.I), t.weight, t.note, t.pattern))
            except:
                continue
        else:
            compiled.append(("substr", None, t.weight, t.note, t.pattern.lower()))
    return compiled

def score_text(text: str, pos_c, neg_c, target: str = None) -> MatchResult:
    norm = normalize(text)
    score, hits, seen = 0.0, [], set()
    aliases = {"å°ç©é›»": ["å°ç©é›»", "tsmc", "2330"],
               "é´»æµ·": ["é´»æµ·", "foxconn", "2317", "å¯Œå£«åº·"],
               "è¯é›»": ["è¯é›»", "umc", "2303"]}
    company_pattern = "|".join(re.escape(a) for a in aliases.get(target, []))
    if not re.search(company_pattern, norm):
        return MatchResult(0.0, [])
    for ttype, cre, w, note, patt in pos_c + neg_c:
        key = (patt, note)
        if key in seen:
            continue
        matched = cre.search(norm) if ttype == "regex" else patt in norm
        if matched:
            score += w
            hits.append((patt, w, note))
            seen.add(key)
    return MatchResult(score, hits)

# ---------- Groq LLM åˆ†æ ----------
def groq_analyze_llm(news_texts: List[str], target: str) -> str:
    if not news_texts:
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš–ï¸\nåŸå› ï¼šè¿‘ä¸‰æ—¥ç„¡ç›¸é—œæ–°è\næƒ…ç·’åˆ†æ•¸ï¼š0"

    prompt = f"è«‹åˆ†æä»¥ä¸‹æ–°èå° {target} æ˜æ—¥è‚¡åƒ¹çš„å½±éŸ¿ï¼Œè¼¸å‡ºä¸­æ–‡èªªæ˜ã€æƒ…ç·’åˆ†æ•¸ (-10~10)ã€é©åˆ emoji è¡¨ç¤ºï¼š\n\n"
    prompt += "\n".join(f"- {t}" for t in news_texts)

    response = client.chat.create(
        messages=[{"role": "user", "content": prompt}],
        max_output_tokens=512
    )
    return response.output_text

# ---------- ä¸»åˆ†æ ----------
def analyze_target(db, collection, target, result_field):
    pos, neg = load_tokens(db)
    pos_c, neg_c = compile_tokens(pos), compile_tokens(neg)
    today = datetime.now(TAIWAN_TZ).date()

    filtered, weighted_scores = [], []
    today_price_change = 0.0

    # å–å¾—ä»Šæ—¥æ¼²è·Œå¹…
    for d in db.collection(collection).stream():
        dt = parse_docid_time(d.id)
        if not dt or dt.date() != today:
            continue
        data = d.to_dict() or {}
        for k, v in data.items():
            if isinstance(v, dict) and "price_change" in v:
                today_price_change = parse_price_change(v.get("price_change"))
                break
        if today_price_change != 0.0:
            break

    # è®€å–æ–°è
    news_texts = []
    for d in db.collection(collection).stream():
        dt = parse_docid_time(d.id)
        if not dt:
            continue
        delta_days = (today - dt.date()).days
        if delta_days > 2:
            continue
        data = d.to_dict() or {}
        for k, v in data.items():
            if not isinstance(v, dict):
                continue
            title, content = v.get("title", ""), v.get("content", "")
            full = title + " " + content
            res = score_text(full, pos_c, neg_c, target)
            if not res.hits:
                continue
            news_texts.append(full)

    # å‘¼å« Groq LLM
    summary = groq_analyze_llm(news_texts, target)

    # å°å‡º & å­˜æª”
    print(summary + "\n")
    fname = f"results/result_{today.strftime('%Y%m%d')}.txt"
    os.makedirs("results", exist_ok=True)
    with open(fname, "a", encoding="utf-8") as f:
        f.write(f"======= {target} =======\n")
        f.write(f"ä»Šæ—¥æ¼²è·Œï¼š{round(today_price_change*100,2)}%\n")
        f.write(summary + "\n\n")

    # Firestore å¯«å›
    try:
        db.collection(result_field).document(today.strftime("%Y%m%d")).set({
            "timestamp": datetime.now(TAIWAN_TZ).isoformat(),
            "result": summary,
        })
    except Exception as e:
        print(f"[warning] Firestore å¯«å›å¤±æ•—ï¼š{e}")

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    if not SILENT_MODE:
        print("ğŸš€ é–‹å§‹åˆ†æå°è‚¡ç„¦é»è‚¡ï¼ˆä½¿ç”¨ Groq LLMï¼‰...\n")

    db = get_db()
    analyze_target(db, NEWS_COLLECTION_TSMC, "å°ç©é›»", "Groq_result")
    print("="*70)
    analyze_target(db, NEWS_COLLECTION_FOX, "é´»æµ·", "Groq_result_Foxxcon")
    print("="*70)
    analyze_target(db, NEWS_COLLECTION_UMC, "è¯é›»", "Groq_result_UMC")

if __name__ == "__main__":
    main()

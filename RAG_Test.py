# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG + èƒŒé›¢ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
"""
import os, signal, regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

# ---------- è¨­å®š ----------
SILENT_MODE = True
TAIWAN_TZ = timezone(timedelta(hours=8))

TOKENS_COLLECTION = "bull_tokens"
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"

RESULTS_DIR = "results"
os.makedirs(RESULTS_DIR, exist_ok=True)

SENSITIVE_WORDS = {
    "æ³•èªª": 1.5, "è²¡å ±": 1.4, "æ–°å“": 1.3, "åˆä½œ": 1.3, "ä½µè³¼": 1.4,
    "æŠ•è³‡": 1.3, "åœå·¥": 1.6, "ä¸‹ä¿®": 1.5, "åˆ©ç©º": 1.5, "çˆ†æ–™": 1.4,
    "ç‡Ÿæ”¶": 1.3, "å±•æœ›": 1.2
}

HARD_WEIGHTS_POS = {
    "è²¡å ±": 1.5, "æ³•èªª": 1.5, "å±•æœ›": 1.5, "è³‡æœ¬æ”¯å‡º": 1.5,
    "è¨‚å–®": 1.2, "æ“´ç”¢": 1.2, "çˆ†å–®": 1.2, "æ¼²åƒ¹": 1.2
}

HARD_WEIGHTS_NEG = {
    "åœå·¥": -1.5, "è£å“¡": -1.5, "è™§æ": -1.5, "ä¸‹ä¿®": -1.5
}

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- åˆå§‹åŒ– ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)

client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ---------- çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
def get_db():
    return firestore.Client()

def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def first_n_sentences(text: str, n: int = 3) -> str:
    if not text:
        return ""
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    return "".join(parts[:n]) + ("..." if len(parts) > n else "")

def parse_docid_time(doc_id: str):
    m = re.match(r"^(?P<ymd>\d{8})(?:_(?P<hms>\d{6}))?$", doc_id or "")
    if not m: return None
    ymd, hms = m.group("ymd"), m.group("hms") or "000000"
    try: return datetime.strptime(ymd+hms, "%Y%m%d%H%M%S").replace(tzinfo=TAIWAN_TZ)
    except: return None

def parse_price_change(raw: str) -> float:
    if not raw: return 0.0
    m = re.search(r"\(([-+]?[\d\.]+)%\)", raw)
    if not m: return 0.0
    try: return float(m.group(1))/100.0
    except: return 0.0

# ---------- Token ----------
def load_tokens(db):
    pos, neg = [], []
    for d in db.collection(TOKENS_COLLECTION).stream():
        data = d.to_dict() or {}
        pol, ttype, patt, note = data.get("polarity","").lower(), data.get("type","substr").lower(), data.get("pattern",""), data.get("note","")
        w = float(data.get("weight",1.0))
        if pol=="positive": pos.append(Token(pol, ttype, patt, w, note))
        elif pol=="negative": neg.append(Token(pol, ttype, patt, -abs(w), note))
    return pos, neg

def compile_tokens(tokens: List[Token]):
    compiled = []
    for t in tokens:
        if t.ttype=="regex":
            try: compiled.append(("regex", re.compile(t.pattern, re.I), t.weight, t.note, t.pattern))
            except: continue
        else:
            compiled.append(("substr", None, t.weight, t.note, t.pattern.lower()))
    return compiled

# ---------- Scoring ----------
def score_text(text: str, pos_c, neg_c, target: str = None) -> MatchResult:
    norm, score, hits, seen = normalize(text), 0.0, [], set()
    aliases = {"å°ç©é›»":["å°ç©é›»","tsmc","2330"], "é´»æµ·":["é´»æµ·","foxconn","2317","å¯Œå£«åº·"], "è¯é›»":["è¯é›»","umc","2303"]}
    company_pattern = "|".join(re.escape(a) for a in aliases.get(target,[]))
    if not re.search(company_pattern, norm): return MatchResult(0.0,[])
    for ttype, cre, w, note, patt in pos_c + neg_c:
        key = (patt, note)
        if key in seen: continue
        matched = cre.search(norm) if ttype=="regex" else patt in norm
        if matched: score+=w; hits.append((patt,w,note)); seen.add(key)
    return MatchResult(score,hits)

def adjust_score_for_context(text: str, base_score: float) -> float:
    if not text or base_score==0: return base_score
    norm = text.lower()
    neutral=["é‡ç”³","ç¬¦åˆé æœŸ","é æœŸå…§","ä¸­æ€§çœ‹å¾…","ç„¡é‡å¤§å½±éŸ¿","æŒå¹³","æœªè®Š"]
    pos_boost=["å‰µæ–°é«˜","å€å¢","å¤§å¹…æˆé•·","ç²åˆ©æš´å¢","å ±å–œ"]
    neg_boost=["æš´è·Œ","ä¸‹æ»‘","è™§æ","åœå·¥","ä¸‹ä¿®","è£å“¡","è­¦è¨Š"]
    if any(p in norm for p in neutral): base_score*=0.4
    if any(p in norm for p in pos_boost): base_score*=1.3
    if any(p in norm for p in neg_boost): base_score*=1.3
    return base_score

def adjust_by_market(avg_score: float, today_change: float) -> float:
    if today_change>=0.03: return avg_score+0.5
    if today_change<=-0.03: return avg_score-0.5
    if today_change>=0.01: return avg_score+0.2
    if today_change<=-0.01: return avg_score-0.2
    return avg_score

def detect_divergence(avg_score: float, today_change: float) -> str:
    if avg_score>1.0 and today_change<-0.02: return "åˆ©å¤šä¸æ¼²ï¼ˆç–‘ä¼¼ä¸»åŠ›å‡ºè²¨ï¼‰"
    if avg_score<-1.0 and today_change>0.02: return "åˆ©ç©ºä¸è·Œï¼ˆå¯èƒ½æœ‰éš±æ€§åˆ©å¤šï¼‰"
    return "ç„¡æ˜é¡¯èƒŒé›¢"

# ---------- RAG èƒŒé›¢åˆ†æ ----------
def rag_divergence_analysis(news_list, divergence, today_change, target):
    if not news_list: return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš–ï¸\nåŸå› ï¼šè¿‘ä¸‰æ—¥ç„¡æ–°è\næƒ…ç·’åˆ†æ•¸ï¼š0"
    prompt = f"è«‹ä¾æ“šä»¥ä¸‹æ–°èæ¨™é¡ŒåŠæ‘˜è¦ï¼Œåˆ†æ {target} è‚¡åƒ¹æ˜æ—¥èµ°å‹¢ï¼Œä¸¦è€ƒæ…®èƒŒé›¢è¨Šè™Ÿ {divergence}ï¼Œä»Šæ—¥æ¼²è·Œ {today_change*100:.2f}%:\n"
    for idx,(docid,title,full,res,weight,full_text) in enumerate(news_list):
        prompt += f"\n[{idx+1}] æ¨™é¡Œ: {title}\næ‘˜è¦: {first_n_sentences(full_text,2)}\n"

    resp = client.chat.completions.create(
        messages=[
            {"role":"system","content":"ä½ æ˜¯ä¸€å€‹è‚¡ç¥¨åˆ†æåŠ©æ‰‹ï¼Œè«‹ç”Ÿæˆä¸­æ–‡åˆ†æå ±å‘Š"},
            {"role":"user","content":prompt}
        ],
        model="llama3-70b-8192",
        temperature=0.7,
    )
    return resp.choices[0].message.content

# ---------- ä¸»åˆ†æ ----------
def analyze_target(db, collection, target, result_field):
    pos, neg = load_tokens(db)
    pos_c, neg_c = compile_tokens(pos), compile_tokens(neg)
    today = datetime.now(TAIWAN_TZ).date()
    filtered=[]
    today_price_change=0.0

    # å–å¾—ä»Šæ—¥æ¼²è·Œ
    try:
        for d in db.collection(collection).stream():
            dt=parse_docid_time(d.id)
            if not dt or dt.date()!=today: continue
            data=d.to_dict() or {}
            for k,v in data.items():
                if isinstance(v,dict) and "price_change" in v:
                    today_price_change=parse_price_change(v.get("price_change"))
                    break
            if today_price_change!=0.0: break
    except: today_price_change=0.0

    # è©•åˆ†æ–°è
    for d in db.collection(collection).stream():
        dt=parse_docid_time(d.id)
        if not dt: continue
        delta_days=(today-dt.date()).days
        if delta_days>2: continue
        day_weight=1.0 if delta_days==0 else 0.85 if delta_days==1 else 0.7
        data=d.to_dict() or {}
        for k,v in data.items():
            if not isinstance(v,dict): continue
            title, content=v.get("title",""), v.get("content","")
            full=title+" "+content
            res=score_text(full,pos_c,neg_c,target)
            if not res.hits: continue
            adj_score=adjust_score_for_context(full,res.score)
            token_weight=1.0+min(len(res.hits)*0.05,0.3)
            impact=1.0+sum(w*0.05 for k_sens,w in SENSITIVE_WORDS.items() if k_sens in full)
            total_weight=day_weight*token_weight*impact
            filtered.append((d.id,k,title,res,total_weight,full))

    # å»é‡ä¸¦é¸å‰10å‰‡æ–°è
    seen=set(); top_news=[]
    for docid,key,title,res,weight,full in sorted(filtered,key=lambda x:abs(x[3].score*x[4]),reverse=True):
        txt=normalize(full)
        if txt in seen: continue
        seen.add(txt); top_news.append((docid,key,title,res,weight,full))
        if len(top_news)>=10: break

    # è¨ˆç®— avg_scoreã€èª¿æ•´ã€èƒŒé›¢
    if top_news:
        news_with_scores=[(title,res.score*weight) for _,_,title,res,weight,_ in top_news]
        avg_score=sum(s for _,s in news_with_scores)/len(news_with_scores)
        adjusted_avg=adjust_by_market(avg_score,today_price_change)
        divergence=detect_divergence(avg_score,today_price_change)
        summary=rag_divergence_analysis(top_news,divergence,today_price_change,target)
    else:
        summary=f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš–ï¸\nåŸå› ï¼šè¿‘ä¸‰æ—¥ç„¡æ–°è\næƒ…ç·’åˆ†æ•¸ï¼š0"

    # å¯«å…¥ results
    fname=os.path.join(RESULTS_DIR,f"{target}_{today.strftime('%Y%m%d')}.txt")
    with open(fname,"w",encoding="utf-8") as f:
        f.write(summary)

    print(summary+"\n")

    # Firestore å¯«å›
    try:
        db.collection(result_field).document(today.strftime("%Y%m%d")).set({
            "timestamp": datetime.now(TAIWAN_TZ).isoformat(),
            "result": summary
        })
    except Exception as e:
        print(f"[warning] Firestore å¯«å›å¤±æ•—ï¼š{e}")

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    if not SILENT_MODE:
        print("ğŸš€ é–‹å§‹åˆ†æå°è‚¡ç„¦é»è‚¡ï¼ˆRAG + èƒŒé›¢ç‰ˆï¼‰...\n")
    db=get_db()
    analyze_target(db, NEWS_COLLECTION_TSMC, "å°ç©é›»", "Groq_result")
    print("="*70)
    analyze_target(db, NEWS_COLLECTION_FOX, "é´»æµ·", "Groq_result_Foxxcon")
    print("="*70)
    analyze_target(db, NEWS_COLLECTION_UMC, "è¯é›»", "Groq_result_UMC")

if __name__=="__main__":
    main()

# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
æº–ç¢ºç‡æ¥µè‡´ç‰ˆï¼ˆçŸ­æœŸé æ¸¬ç‰¹åŒ–ï¼‰ - Context-aware + å»é‡æ–°èç‰ˆ
âœ… åš´æ ¼ä¾æ“šæƒ…ç·’åˆ†æ•¸æ±ºç­–
âœ… æ•æ„Ÿè©åŠ æ¬Šï¼ˆæ³•èªª / è²¡å ± / æ–°å“ / åœå·¥ç­‰ï¼‰
âœ… æ”¯æ´ 3 æ—¥å»¶é²æ•ˆæ‡‰
âœ… Firestore å¯«å› + æœ¬åœ° result.txt
âœ… æ–°å¢å¥å‹åˆ¤æ–·ï¼Œé¿å…ã€Œé‡ç”³ï¼é æœŸå…§ã€èª¤åˆ¤
âœ… ç›¸åŒæ–°èå…§å®¹å»é‡
"""

import os, signal, regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

# ---------- è¨­å®š ----------
SILENT_MODE = True
TAIWAN_TZ = timezone(timedelta(hours=8))

TOKENS_COLLECTION = "bull_tokens"
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"

SENSITIVE_WORDS = {
    "æ³•èªª": 1.5,
    "è²¡å ±": 1.4,
    "æ–°å“": 1.3,
    "åˆä½œ": 1.3,
    "ä½µè³¼": 1.4,
    "æŠ•è³‡": 1.3,
    "åœå·¥": 1.6,
    "ä¸‹ä¿®": 1.5,
    "åˆ©ç©º": 1.5,
    "çˆ†æ–™": 1.4,
    "ç‡Ÿæ”¶": 1.3,
    "å±•æœ›": 1.2,
}

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- åˆå§‹åŒ– ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ---------- çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
def get_db():
    return firestore.Client()

def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def first_n_sentences(text: str, n: int = 3) -> str:
    if not text:
        return ""
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    return "".join(parts[:n]) + ("..." if len(parts) > n else "")

def parse_docid_time(doc_id: str):
    m = re.match(r"^(?P<ymd>\d{8})(?:_(?P<hms>\d{6}))?$", doc_id or "")
    if not m:
        return None
    ymd, hms = m.group("ymd"), m.group("hms") or "000000"
    try:
        return datetime.strptime(ymd + hms, "%Y%m%d%H%M%S").replace(tzinfo=TAIWAN_TZ)
    except:
        return None

# ---------- æ–°å¢ï¼šè§£æ price_change ----------
def parse_price_change(raw: str) -> float:
    """
    è§£ææ ¼å¼ç¯„ä¾‹ï¼š
    "+7.50 (+3.28%)" -> 0.0328
    "-1.20 (-0.42%)" -> -0.0042
    è‹¥ç„¡æ³•è§£æå‰‡å›å‚³ 0.0
    """
    if not raw:
        return 0.0
    m = re.search(r"\(([-+]?[\d\.]+)%\)", raw)
    if not m:
        return 0.0
    try:
        return float(m.group(1)) / 100.0
    except:
        return 0.0

# ---------- Token ----------
def load_tokens(db):
    pos, neg = [], []
    for d in db.collection(TOKENS_COLLECTION).stream():
        data = d.to_dict() or {}
        pol = data.get("polarity", "").lower()
        ttype = data.get("type", "substr").lower()
        patt = data.get("pattern", "")
        note = data.get("note", "")
        w = float(data.get("weight", 1.0))
        if pol == "positive":
            pos.append(Token(pol, ttype, patt, w, note))
        elif pol == "negative":
            neg.append(Token(pol, ttype, patt, -abs(w), note))
    return pos, neg

def compile_tokens(tokens: List[Token]):
    compiled = []
    for t in tokens:
        if t.ttype == "regex":
            try:
                compiled.append(("regex", re.compile(t.pattern, re.I), t.weight, t.note, t.pattern))
            except:
                continue
        else:
            compiled.append(("substr", None, t.weight, t.note, t.pattern.lower()))
    return compiled

# ---------- Scoring ----------
def score_text(text: str, pos_c, neg_c, target: str = None) -> MatchResult:
    norm = normalize(text)
    score, hits, seen = 0.0, [], set()
    aliases = {"å°ç©é›»": ["å°ç©é›»", "tsmc", "2330"],
               "é´»æµ·": ["é´»æµ·", "foxconn", "2317", "å¯Œå£«åº·"],
               "è¯é›»": ["è¯é›»", "umc", "2303"]}
    company_pattern = "|".join(re.escape(a) for a in aliases.get(target, []))
    if not re.search(company_pattern, norm):
        return MatchResult(0.0, [])
    for ttype, cre, w, note, patt in pos_c + neg_c:
        key = (patt, note)
        if key in seen:
            continue
        matched = cre.search(norm) if ttype == "regex" else patt in norm
        if matched:
            score += w
            hits.append((patt, w, note))
            seen.add(key)
    return MatchResult(score, hits)

# ---------- Context-aware èª¿æ•´ ----------
def adjust_score_for_context(text: str, base_score: float) -> float:
    if not text or base_score == 0:
        return base_score

    norm = text.lower()
    neutral_phrases = ["é‡ç”³", "ç¬¦åˆé æœŸ", "é æœŸå…§", "ä¸­æ€§çœ‹å¾…", "ç„¡é‡å¤§å½±éŸ¿", "æŒå¹³", "æœªè®Š"]
    if any(p in norm for p in neutral_phrases):
        base_score *= 0.4

    positive_boost = ["å‰µæ–°é«˜", "å€å¢", "å¤§å¹…æˆé•·", "ç²åˆ©æš´å¢", "å ±å–œ"]
    negative_boost = ["æš´è·Œ", "ä¸‹æ»‘", "è™§æ", "åœå·¥", "ä¸‹ä¿®", "è£å“¡", "è­¦è¨Š"]
    if any(p in norm for p in positive_boost):
        base_score *= 1.3
    if any(p in norm for p in negative_boost):
        base_score *= 1.3

    return base_score

# ---------- ä¿®æ”¹ï¼šGroq åˆ¤æ–·ï¼ˆåŠ å…¥ today_changeï¼‰ ----------
def groq_analyze(news_list, target, avg_score, today_change):
    """
    news_list: [(title, score), ...]
    avg_score: å¹³å‡æƒ…ç·’åˆ†æ•¸
    today_change: ä»Šæ—¥å¯¦éš›æ¼²è·Œå¹… (float) ä¾‹å¦‚ +0.0328
    """
    if not news_list:
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš–ï¸\nåŸå› ï¼šè¿‘ä¸‰æ—¥ç„¡ç›¸é—œæ–°è\næƒ…ç·’åˆ†æ•¸ï¼š0"

    # æ ¼å¼åŒ–æ–°èæ¸…å–®
    news_details = []
    for i, (title, score) in enumerate(news_list, 1):
        impact_desc = "æ­£é¢" if score > 0 else "è² é¢"
        news_details.append(f"{i}. ã€Œ{title}ã€ â†’ {impact_desc}å½±éŸ¿ ({score:+.2f})")
    combined = "\n".join(news_details)

    # ç¨‹å¼ç«¯å»ºæ§‹åŸå› 
    pos_news = sorted([(t, s) for t, s in news_list if s > 0], key=lambda x: x[1], reverse=True)
    neg_news = sorted([(t, s) for t, s in news_list if s < 0], key=lambda x: x[1])
    top_pos = pos_news[:2]
    top_neg = neg_news[:2]

    sensitive_hits = []
    for t, s in news_list:
        tl = t.lower()
        for kw in SENSITIVE_WORDS.keys():
            if kw in tl:
                sensitive_hits.append((t, kw))
                break

    reason_lines = []
    if top_pos:
        rp = "; ".join([f"ã€Œ{t}ã€({s:+.2f})" for t, s in top_pos])
        reason_lines.append(f"ä¸»è¦åˆ©å¤šï¼š{rp}")
    if top_neg:
        rn = "; ".join([f"ã€Œ{t}ã€({s:+.2f})" for t, s in top_neg])
        reason_lines.append(f"ä¸»è¦åˆ©ç©ºï¼š{rn}")
    if sensitive_hits:
        sh = "; ".join([f"ã€Œ{t}ã€(å« {kw})" for t, kw in sensitive_hits])
        reason_lines.append(f"æ•æ„Ÿè­°é¡Œå¼·åŒ–å½±éŸ¿ï¼š{sh}")

    reason_lines.append(f"ç¶œåˆä¾†çœ‹å¹³å‡æƒ…ç·’åˆ†æ•¸ç‚º {avg_score:+.2f}ï¼Œåæ˜ æ­£è² æ–°èäº¤éŒ¯ï¼Œä½†ä»åå‘{'å¤šé ­' if avg_score>0 else 'ç©ºé ­' if avg_score<0 else 'ä¸­æ€§'}ã€‚")

    # ä»Šæ—¥æ¼²è·Œå­—ä¸²
    pct = round(today_change * 100, 2)
    trend_today = "ä¸Šæ¼²" if today_change > 0 else "ä¸‹è·Œ" if today_change < 0 else "å¹³ç›¤"
    reason_lines.append(f"ä»Šæ—¥å¸‚å ´çœŸå¯¦èµ°å‹¢ï¼š{trend_today}ï¼ˆ{pct}%ï¼‰ï¼Œä½œç‚ºå¸‚å ´å³æ™‚åæ‡‰æŒ‡æ¨™ã€‚")

    constructed_reason = "ï¼›".join(reason_lines)

    # æ§‹é€  promptï¼Œæ˜ç¢ºè¦æ±‚æ¨¡å‹æ¯”è¼ƒæ–°èèˆ‡ä»Šæ—¥å¸‚å ´åæ‡‰
    prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­å°è‚¡é‡‘èåˆ†æå¸«ï¼Œè«‹ä¾æ“šä»¥ä¸‹ã€Œ{target}ã€è¿‘ä¸‰æ—¥æ–°èæ‘˜è¦èˆ‡ä»Šæ—¥å¸‚å ´èµ°å‹¢ï¼Œ
åš´æ ¼æ¨è«–æ˜æ—¥è‚¡åƒ¹æ–¹å‘ï¼Œä¸¦çµ¦å‡ºè©³ç´°åŸå› ã€‚è«‹å‹™å¿…åœ¨ã€ŒåŸå› ã€æ®µè½ä¸­ï¼š
1) é€æ¢è©•ä¼°æ¯å‰‡æ–°èå°è‚¡åƒ¹çš„æ­£/è² è²¢ç»ï¼ˆå¯æ¡ä¸Šæ–¹åˆ—å‡ºçš„æ ¼å¼ï¼‰ï¼Œ
2) æŒ‡å‡ºä¸»è¦åˆ©å¤šèˆ‡ä¸»è¦åˆ©ç©ºï¼ˆå„è‡³å¤šå…©é …ï¼‰ï¼Œ
3) è‹¥æ–°èå«æ•æ„Ÿè©ï¼ˆæ³•èªªã€è²¡å ±ã€æ–°å“ã€åœå·¥ç­‰ï¼‰ï¼Œè«‹èªªæ˜å…¶æ”¾å¤§æ•ˆæœï¼Œ
4) è©•ä¼°ä»Šæ—¥å¸‚å ´èµ°å‹¢ï¼ˆå·²æä¾›ï¼‰æ˜¯å¦ã€Œå¼·åŒ–ã€æˆ–ã€ŒæŠµéŠ·ã€æ–°èç™¼å‡ºçš„è¨Šè™Ÿï¼Œ
5) æœ€å¾Œçµ¦å‡ºä¸€å¥æ•´é«”ç¸½çµï¼ˆ40å­—ä»¥å…§ï¼‰ã€‚

ä¸‹é¢æ˜¯ç¨‹å¼ç«¯çš„é å…ˆæ•´ç†ï¼ˆè«‹åœ¨èªªæ˜ä¸­å¼•ç”¨æˆ–ä¿®æ­£ï¼‰ï¼š
---- ç¨‹å¼ç«¯æ‘˜è¦é–‹å§‹ ----
{combined}

ç¨‹å¼ç«¯å¿«é€Ÿåˆ¤æ–·ï¼ˆä¾›ä½ åƒè€ƒï¼Œéæœ€çµ‚çµè«–ï¼‰ï¼š
{constructed_reason}
---- ç¨‹å¼ç«¯æ‘˜è¦çµæŸ ----

ã€ä»Šæ—¥å¸‚å ´å³æ™‚èµ°å‹¢ï¼ˆç¨‹å¼æä¾›ï¼‰ã€‘
- ä»Šæ—¥è‚¡åƒ¹ï¼š{trend_today}ï¼ˆ{pct}%ï¼‰

è«‹æ ¹æ“šä¸Šé¢å…§å®¹ä¸¦çµåˆä½ çš„é‡‘èå¸¸è­˜ç”¢å‡ºä»¥ä¸‹æ ¼å¼ï¼ˆæ‰€æœ‰æ¬„ä½éƒ½è¦å‡ºç¾ï¼‰ï¼š
æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š{{ä¸Šæ¼²ï¼å¾®æ¼²ï¼å¾®è·Œï¼ä¸‹è·Œï¼ä¸æ˜ç¢º}}ï¼ˆé™„ç¬¦è™Ÿï¼‰
åŸå› ï¼š{{è©³ç›¡èªªæ˜ï¼ŒåŒ…å«æ¯å‰‡æ–°èè²¢ç»ã€ä¸»è¦åˆ©å¤š/åˆ©ç©ºã€æ•æ„Ÿè©å½±éŸ¿ã€ä»Šæ—¥èµ°å‹¢å¦‚ä½•å½±éŸ¿æ˜æ—¥åˆ¤æ–·èˆ‡ç°¡çŸ­ç¸½çµ}}
æƒ…ç·’åˆ†æ•¸ï¼š{{æ•´æ•¸ -10~+10}}

æ³¨æ„ï¼šå¦‚æœä½ æ¡ç”¨ç¨‹å¼ç«¯æä¾›çš„ã€Œä¸»è¦åˆ©å¤š/åˆ©ç©ºã€æˆ–ã€Œæ•æ„Ÿè­°é¡Œã€ï¼Œè«‹åœ¨åŸå› ä¸­æ˜ç¢ºæ¨™ç¤ºä½ æ˜¯å¦åŒæ„ï¼Œä¸¦èªªæ˜ç†ç”±ã€‚
"""

    try:
        resp = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å°è‚¡é‡åŒ–åˆ†æå“¡ï¼Œéœ€æ ¹æ“šæ–°èæƒ…ç·’èˆ‡ç•¶æ—¥ç›¤å‹¢ç”Ÿæˆæ˜ç¢ºè¶¨å‹¢å’Œè©³ç´°åŸå› ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.12,
            max_tokens=450,
        )
        ans = resp.choices[0].message.content.strip()
        ans = re.sub(r"\s+", " ", ans)

        # è§£æ model å›å‚³ï¼ˆä¿ç•™ trend / model åŸå›  / model åˆ†æ•¸ï¼‰
        m_trend = re.search(r"(ä¸Šæ¼²|å¾®æ¼²|å¾®è·Œ|ä¸‹è·Œ|ä¸æ˜ç¢º)", ans)
        trend = m_trend.group(1) if m_trend else "ä¸æ˜ç¢º"
        symbol_map = {"ä¸Šæ¼²": "ğŸ”¼", "å¾®æ¼²": "â†—ï¸", "å¾®è·Œ": "â†˜ï¸", "ä¸‹è·Œ": "ğŸ”½", "ä¸æ˜ç¢º": "âš–ï¸"}

        m_reason = re.search(r"(?:åŸå› |ç†ç”±)[:ï¼š]?\s*(.+?)(?:æƒ…ç·’åˆ†æ•¸|$)", ans)
        model_reason = m_reason.group(1).strip() if m_reason and m_reason.group(1).strip() else None

        m_score = re.search(r"æƒ…ç·’åˆ†æ•¸[:ï¼š]?\s*(-?\d+)", ans)
        mood_score = int(m_score.group(1)) if m_score else max(-10, min(10, int(round(avg_score * 3))))

        # çµ„åˆæœ€çµ‚ reason
        if model_reason:
            short_model = len(model_reason) < 30 or model_reason.lower().strip() in ["æ•´é«”å¹³å‡", "ç¶œåˆå„æ–°èæ­£è² å½±éŸ¿å½¢æˆå¸‚å ´çŸ­ç·šè§€æœ›ã€‚"]
            if short_model:
                final_reason = constructed_reason
            else:
                final_reason = model_reason + "ï¼›" + constructed_reason
        else:
            final_reason = constructed_reason

        if len(final_reason) > 600:
            final_reason = final_reason[:590].rsplit("ã€‚", 1)[0] + "ã€‚ (æ‘˜è¦...)"

        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š{trend} {symbol_map.get(trend,'')}\nåŸå› ï¼š{final_reason}\næƒ…ç·’åˆ†æ•¸ï¼š{mood_score:+d}"

    except Exception as e:
        # fallback
        fallback_reason = constructed_reason + "ï¼ˆGroq å‘¼å«å¤±æ•—ï¼Œä½¿ç”¨ç¨‹å¼ç«¯é å…ˆç”Ÿæˆä¹‹åˆ†æã€‚ï¼‰"
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš–ï¸\nåŸå› ï¼š{fallback_reason}\næƒ…ç·’åˆ†æ•¸ï¼š{max(-10, min(10, int(round(avg_score * 3)))):+d}"

# ---------- ä¸»åˆ†æ ----------
def analyze_target(db, collection, target, result_field):
    pos, neg = load_tokens(db)
    pos_c, neg_c = compile_tokens(pos), compile_tokens(neg)
    today = datetime.now(TAIWAN_TZ).date()

    filtered, weighted_scores = [], []
    today_price_change = 0.0

    # ---------- å…ˆæƒä¸€æ¬¡ collection å–å¾—ä»Šæ—¥ price_changeï¼ˆè‹¥æœ‰ï¼‰ ----------
    try:
        for d in db.collection(collection).stream():
            dt = parse_docid_time(d.id)
            if not dt:
                continue
            if dt.date() != today:
                continue
            data = d.to_dict() or {}
            # data å¯èƒ½åŒ…å«å¤šå€‹ keyï¼Œæ¯å€‹ key æ˜¯ä¸€ç¯‡æ–°èçš„ dict
            for k, v in data.items():
                if isinstance(v, dict) and "price_change" in v:
                    today_price_change = parse_price_change(v.get("price_change"))
                    break
            if today_price_change != 0.0:
                break
    except Exception:
        # è‹¥è®€å–éç¨‹æœ‰å•é¡Œï¼Œä¿ç•™ today_price_change = 0.0
        today_price_change = 0.0

    # ---------- åŸæœ‰æ–°èæ‰“åˆ†æµç¨‹ï¼ˆä¿ç•™ï¼‰ ----------
    for d in db.collection(collection).stream():
        dt = parse_docid_time(d.id)
        if not dt:
            continue
        delta_days = (today - dt.date()).days
        if delta_days > 2:
            continue

        day_weight = 1.0 if delta_days == 0 else 0.85 if delta_days == 1 else 0.7
        data = d.to_dict() or {}

        for k, v in data.items():
            if not isinstance(v, dict):
                continue
            title, content = v.get("title", ""), v.get("content", "")
            full = title + " " + content
            res = score_text(full, pos_c, neg_c, target)
            if not res.hits:
                continue

            adj_score = adjust_score_for_context(full, res.score)
            token_weight = 1.0 + min(len(res.hits) * 0.05, 0.3)
            impact = 1.0 + sum(w * 0.05 for k_sens, w in SENSITIVE_WORDS.items() if k_sens in full)
            total_weight = day_weight * token_weight * impact

            filtered.append((d.id, k, title, full, res, total_weight))
            weighted_scores.append(adj_score * total_weight)

    # ---------- ç„¡æ–°è fallback ----------
    if not filtered:
        summary = groq_analyze([], target, 0, today_price_change)
    else:
        # å»é‡æ–°è
        seen_text = set()
        top_news = []
        for docid, key, title, full, res, weight in sorted(filtered, key=lambda x: abs(x[4].score * x[5]), reverse=True):
            news_text = normalize(full)
            if news_text in seen_text:
                continue
            seen_text.add(news_text)
            top_news.append((docid, key, title, res, weight))
            if len(top_news) >= 10:
                break

        # è¼¸å‡ºæ–°èæ‘˜è¦ï¼ˆconsoleï¼‰
        print(f"\nğŸ“° {target} è¿‘æœŸé‡é»æ–°èï¼ˆå«è¡æ“Šï¼‰:")
        for docid, key, title, res, weight in top_news:
            impact_val = sum(w for k_sens, w in SENSITIVE_WORDS.items() if k_sens in title)
            print(f"[{docid}#{key}] ({weight:.2f}x, åˆ†æ•¸={res.score:+.2f}, è¡æ“Š={1+impact_val/10:.2f}) {title}")
            for p, w, n in res.hits:
                sign = "+" if w>0 else "-"
                print(f"   {sign} {p}ï¼ˆ{n}ï¼‰")

        news_with_scores = [(t, res.score * weight) for _, _, t, res, weight in top_news]
        avg_score = sum(s for _, s in news_with_scores) / len(news_with_scores)
        summary = groq_analyze(news_with_scores, target, avg_score, today_price_change)

        # æœ¬åœ°å­˜æª”
        fname = f"result_{today.strftime('%Y%m%d')}.txt"
        with open(fname, "a", encoding="utf-8") as f:
            f.write(f"======= {target} =======\n")
            for docid, key, title, res, weight in top_news:
                hits_text = "\n".join([f"  {'+' if w>0 else '-'} {p}ï¼ˆ{n}ï¼‰" for p, w, n in res.hits])
                f.write(f"[{docid}#{key}]ï¼ˆ{weight:.2f}xï¼‰\næ¨™é¡Œï¼š{first_n_sentences(title)}\nå‘½ä¸­ï¼š\n{hits_text}\n\n")
            f.write(summary + "\n\n")

    print(summary + "\n")

    # Firestore å¯«å›
    try:
        db.collection(result_field).document(today.strftime("%Y%m%d")).set({
            "timestamp": datetime.now(TAIWAN_TZ).isoformat(),
            "result": summary,
        })
    except Exception as e:
        print(f"[warning] Firestore å¯«å›å¤±æ•—ï¼š{e}")

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    if not SILENT_MODE:
        print("ğŸš€ é–‹å§‹åˆ†æå°è‚¡ç„¦é»è‚¡ï¼ˆæº–ç¢ºç‡æ¥µè‡´ç‰ˆï¼‰...\n")

    db = get_db()
    analyze_target(db, NEWS_COLLECTION_TSMC, "å°ç©é›»", "Groq_result")
    print("=" * 70)
    analyze_target(db, NEWS_COLLECTION_FOX, "é´»æµ·", "Groq_result_Foxxcon")
    print("=" * 70)
    analyze_target(db, NEWS_COLLECTION_UMC, "è¯é›»", "Groq_result_UMC")

if __name__ == "__main__":
    main()

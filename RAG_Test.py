# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆGitHub Actions å„ªåŒ–ç‰ˆ + æ¯å‰‡æ–°èè©³ç´°è¼¸å‡ºï¼‰
ğŸ”¥ TXT = æ¯å‰‡æ–°èéƒ½åˆ†æ
ğŸ”¥ Firestore = Groq ç›´æ¥è¼¸å‡º 3 è¡ŒçŸ­ç‰ˆï¼ˆå›ºå®šæ ¼å¼ï¼‰
"""

import os
import signal
import regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

# ---------- è¨­å®š ----------
SILENT_MODE = True
TAIWAN_TZ = timezone(timedelta(hours=8))
TOKENS_COLLECTION = "bull_tokens"
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"

RESULTS_DIR = "results"
os.makedirs(RESULTS_DIR, exist_ok=True)

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- åˆå§‹åŒ– ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ---------- è³‡æ–™çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

DOCID_RE = re.compile(r"^(?P<ymd>\d{8})(?:_(?P<hms>\d{6}))?$")

def get_db() -> firestore.Client:
    return firestore.Client()

def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def first_n_sentences(text: str, n: int = 3) -> str:
    if not text:
        return ""
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    return "".join(parts[:n]) + ("..." if len(parts) > n else "")

def parse_docid_time(doc_id: str):
    m = DOCID_RE.match(doc_id or "")
    if not m:
        return None
    ymd = m.group("ymd")
    hms = m.group("hms") or "000000"
    try:
        return datetime.strptime(ymd + hms, "%Y%m%d%H%M%S").replace(tzinfo=TAIWAN_TZ)
    except:
        return None

# ---------- Token è¼‰å…¥ ----------
def load_tokens(db) -> Tuple[List[Token], List[Token]]:
    pos, neg = [], []
    for d in db.collection(TOKENS_COLLECTION).stream():
        data = d.to_dict() or {}
        pol = data.get("polarity", "").lower()
        ttype = data.get("type", "substr").lower()
        patt = data.get("pattern", "")
        note = data.get("note", "")
        w = float(data.get("weight", 1.0))
        if pol == "positive":
            pos.append(Token(pol, ttype, patt, w, note))
        elif pol == "negative":
            neg.append(Token(pol, ttype, patt, -abs(w), note))
    return pos, neg

def compile_tokens(tokens: List[Token]):
    compiled = []
    for t in tokens:
        if t.ttype == "regex":
            try:
                compiled.append(("regex", re.compile(t.pattern, re.I), t.weight, t.note, t.pattern))
            except:
                continue
        else:
            compiled.append(("substr", None, t.weight, t.note, t.pattern.lower()))
    return compiled

# ---------- è¨ˆåˆ† ----------
def score_text(text: str, pos_c, neg_c, target: str = None) -> MatchResult:
    norm = normalize(text)
    score, hits, seen = 0.0, [], set()
    aliases = {
        "å°ç©é›»": ["å°ç©é›»", "tsmc", "2330"],
        "é´»æµ·": ["é´»æµ·", "foxconn", "2317", "å¯Œå£«åº·"],
        "è¯é›»": ["è¯é›»", "umc", "2303"]
    }
    company_pattern = "|".join(re.escape(a) for a in aliases.get(target, []))
    if not re.search(company_pattern, norm):
        return MatchResult(0.0, [])
    for ttype, cre, w, note, patt in pos_c + neg_c:
        key = (patt, note)
        if key in seen:
            continue
        matched = cre.search(norm) if ttype == "regex" else patt in norm
        if matched:
            score += w
            hits.append((patt, w, note))
            seen.add(key)
    return MatchResult(score, hits)

# ---------- Groq çŸ­ç‰ˆ 3 è¡Œ ----------
def groq_analyze_batch(news_with_scores: List[Tuple[str, float]], target: str, price_change: str = "") -> str:
    symbol = {"ä¸Šæ¼²": "ğŸ”¼","ä¸‹è·Œ": "ğŸ”½","ä¸æ˜ç¢º":"âš–ï¸","å¾®æ¼²":"â†—ï¸","å¾®è·Œ":"â†˜ï¸"}
    if not news_with_scores:
        return f"""æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš–ï¸
åŸå› ï¼šè¿‘ä¸‰æ—¥ç„¡ç›¸é—œæ–°èã€‚ä»Šæ—¥æ¼²è·Œï¼š{price_change}
æƒ…ç·’åˆ†æ•¸ï¼š0"""
    combined = "\n".join(f"{i+1}. ({s:+.2f}) {t}" for i, (t, s) in enumerate(news_with_scores))
    avg_score = sum(s for _, s in news_with_scores)/len(news_with_scores)
    prompt_text = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­å°è‚¡åˆ†æå¸«ï¼Œè«‹ä¾ä»¥ä¸‹è¦å‰‡è¼¸å‡ºç­”æ¡ˆï¼š
âš ï¸ å¿…é ˆåš´æ ¼è¼¸å‡ºä¸‰è¡Œï¼š
æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š{{ä¸Šæ¼²ï¼ä¸‹è·Œï¼ä¸æ˜ç¢º}} {{ç¬¦è™Ÿ}}
åŸå› ï¼šä¸€å¥åŸå› 
æƒ…ç·’åˆ†æ•¸ï¼š-10~10

å¹³å‡æƒ…ç·’åˆ†æ•¸ï¼š{avg_score:+.2f}
{combined}
"""
    try:
        resp = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[{"role":"system","content":"ä½ æ˜¯å°ˆæ¥­å°è‚¡åˆ†æå¸«ã€‚å¿…é ˆè¼¸å‡ºä¸‰è¡ŒçŸ­ç‰ˆã€‚"},
                      {"role":"user","content":prompt_text}],
            temperature=0.1,
            max_tokens=150,
            timeout=25
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"""æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš–ï¸
åŸå› ï¼šGroq åˆ†æå¤±æ•—({e})
æƒ…ç·’åˆ†æ•¸ï¼š0"""

# ---------- TXT è©³ç´°è¼¸å‡º ----------
def dump_detailed_news(target: str, today, all_news: List[Tuple]):
    fname = os.path.join(RESULTS_DIR, f"result_{today.strftime('%Y%m%d')}.txt")
    with open(fname, "a", encoding="utf-8") as f:
        f.write(f"ğŸ“° {target} è¿‘æœŸæ–°èè©³ç´°åˆ†æï¼ˆå«è¡æ“Šï¼‰:\n\n")
        for docid, key, title, res, weight in all_news:
            f.write(f"[{docid}#{key}] ({weight:.2f}x, åˆ†æ•¸={res.score:+.2f}) {first_n_sentences(title)}\n")
            for patt, w, note in res.hits:
                sign = "+" if w>0 else "-"
                f.write(f"   {sign} {patt}ï¼ˆ{note}ï¼‰\n")
            f.write("\n")

# ---------- ä¸»åˆ†æ ----------
def analyze_target(db, collection: str, target: str, result_field: str):
    pos, neg = load_tokens(db)
    pos_c, neg_c = compile_tokens(pos), compile_tokens(neg)

    today = datetime.now(TAIWAN_TZ).date()
    all_news = []
    price_change = ""

    # Firestore æ‹‰å–
    for d in db.collection(collection).stream():
        dt = parse_docid_time(d.id)
        if not dt:
            continue
        delta_days = (today - dt.date()).days
        if delta_days > 2:
            continue
        day_weight = {0:1.0,1:0.85,2:0.7}.get(delta_days,0.7)
        data = d.to_dict() or {}
        for k,v in data.items():
            if not isinstance(v, dict):
                continue
            if not price_change:
                price_change = v.get("price_change","")
            title, content = v.get("title",""), v.get("content","")
            res = score_text(title + " " + content, pos_c, neg_c, target)
            if not res.hits:
                continue
            token_weight = 1.0 + min(len(res.hits)*0.05,0.3)
            total_weight = day_weight * token_weight
            all_news.append((d.id, k, title, res, total_weight))

    # TXT è¼¸å‡º
    if all_news:
        dump_detailed_news(target, today, all_news)

    # Firestore çŸ­ç‰ˆ
    if not all_news:
        summary = groq_analyze_batch([], target, price_change)
    else:
        all_news_sorted = sorted(all_news, key=lambda x: abs(x[3].score*x[4]), reverse=True)
        news_with_scores = [(t,res.score*weight) for _,_,t,res,weight in all_news_sorted[:10]]
        summary = groq_analyze_batch(news_with_scores, target, price_change)

        # åŒæ­¥å¯«å…¥ TXT
        fname = os.path.join(RESULTS_DIR, f"result_{today.strftime('%Y%m%d')}.txt")
        with open(fname,"a",encoding="utf-8") as f:
            f.write(summary + "\n\n")

    print(summary + "\n")

    # Firestore å¯«å›
    try:
        db.collection(result_field).document(today.strftime("%Y%m%d")).set({
            "timestamp": datetime.now(TAIWAN_TZ).isoformat(),
            "result": summary,
        })
    except Exception as e:
        print("[warning] Firestore å¯«å›å¤±æ•—ï¼š", e)

# ---------- main ----------
def main():
    if not SILENT_MODE:
        print("ğŸš€ é–‹å§‹åˆ†æå°è‚¡ç„¦é»è‚¡...\n")
    db = get_db()
    analyze_target(db, NEWS_COLLECTION_TSMC, "å°ç©é›»", "Groq_result")
    print("="*70)
    analyze_target(db, NEWS_COLLECTION_FOX, "é´»æµ·", "Groq_result_Foxxcon")
    print("="*70)
    analyze_target(db, NEWS_COLLECTION_UMC, "è¯é›»", "Groq_result_UMC")

if __name__ == "__main__":
    main()

# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
æº–ç¢ºç‡æå‡ç‰ˆï¼ˆæƒ…ç·’èåˆ + å¤šå±¤æ¬Šé‡ + èªæ„è£œå„Ÿï¼‰
âœ… Firestore å¯«å› + æœ¬åœ° result.txt
âœ… Groq åŒæ™‚è€ƒæ…®æ¯å‰‡æƒ…ç·’åˆ†æ•¸ + å¹³å‡åˆ†æ•¸
âœ… å‘½ä¸­å¤šå‰‡æ–°èæ™‚æå‡ç©©å®šåº¦
âœ… æ–°å¢ï¼šæ”¯æ´ 3 å¤©å…§æ–°èï¼ˆå»¶é²æ•ˆæ‡‰ï¼‰
âœ… æ–°å¢ï¼šå¾æ–°èæŠ“ä»Šæ—¥æ¼²è·Œ price_changeï¼Œé™„åœ¨ Groq åˆ†æåŸå› 
"""

import os, signal, regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

# ---------- è¨­å®š ----------
SILENT_MODE = True
TAIWAN_TZ = timezone(timedelta(hours=8))
SCORE_THRESHOLD = 1.5

TOKENS_COLLECTION = "bull_tokens"
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
    print("\n[info] åµæ¸¬åˆ° Ctrl+Cï¼Œå°‡å®‰å…¨åœæ­¢â€¦")
signal.signal(signal.SIGINT, _sigint_handler)

# ---------- åˆå§‹åŒ– ----------
if os.path.exists(".env"):
    load_dotenv(".env", override=True)
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ---------- çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
DOCID_RE = re.compile(r"^(?P<ymd>\d{8})(?:_(?P<hms>\d{6}))?$")

def get_db():
    return firestore.Client()

def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

def first_n_sentences(text: str, n: int = 3) -> str:
    if not text:
        return ""
    parts = re.split(r'(?<=[ã€‚\.ï¼!\?ï¼Ÿï¼›;])\s*', text.strip())
    return "".join(parts[:n]) + ("..." if len(parts) > n else "")

def parse_docid_time(doc_id: str):
    m = DOCID_RE.match(doc_id or "")
    if not m:
        return None
    ymd = m.group("ymd")
    hms = m.group("hms") or "000000"
    try:
        return datetime.strptime(ymd + hms, "%Y%m%d%H%M%S").replace(tzinfo=TAIWAN_TZ)
    except:
        return None

# ---------- Token ----------
def load_tokens(db) -> Tuple[List[Token], List[Token]]:
    pos, neg = [], []
    for d in db.collection(TOKENS_COLLECTION).stream():
        data = d.to_dict() or {}
        pol = data.get("polarity", "").lower()
        ttype = data.get("type", "substr").lower()
        patt = data.get("pattern", "")
        note = data.get("note", "")
        w = float(data.get("weight", 1.0))
        if pol == "positive":
            pos.append(Token(pol, ttype, patt, w, note))
        elif pol == "negative":
            neg.append(Token(pol, ttype, patt, -abs(w), note))
    return pos, neg

def compile_tokens(tokens: List[Token]):
    compiled = []
    for t in tokens:
        if t.ttype == "regex":
            try:
                compiled.append(("regex", re.compile(t.pattern, re.I), t.weight, t.note, t.pattern))
            except:
                continue
        else:
            compiled.append(("substr", None, t.weight, t.note, t.pattern.lower()))
    return compiled

# ---------- Scoring ----------
def score_text(text: str, pos_c, neg_c, target: str = None) -> MatchResult:
    norm = normalize(text)
    score, hits, seen = 0.0, [], set()
    aliases = {
        "å°ç©é›»": ["å°ç©é›»", "tsmc", "2330"],
        "é´»æµ·": ["é´»æµ·", "foxconn", "2317", "å¯Œå£«åº·"],
        "è¯é›»": ["è¯é›»", "umc", "2303"]
    }
    company_pattern = "|".join(re.escape(a) for a in aliases.get(target, []))
    if not re.search(company_pattern, norm):
        return MatchResult(0.0, [])
    for ttype, cre, w, note, patt in pos_c + neg_c:
        key = (patt, note)
        if key in seen:
            continue
        matched = cre.search(norm) if ttype == "regex" else patt in norm
        if matched:
            score += w
            hits.append((patt, w, note))
            seen.add(key)
    return MatchResult(score, hits)

# ---------- Groqï¼ˆæƒ…ç·’èåˆ + æº–ç¢ºç‡å¼·åŒ–ï¼‰ ----------
def groq_analyze(news_list: List[Tuple[str, float]], target: str, avg_score: float, price_change: str = "") -> str:
    if not news_list:
        reason_text = f"è¿‘ä¸‰æ—¥ç„¡ç›¸é—œæ–°èã€‚ä»Šæ—¥æ¼²è·Œï¼š{price_change}" if price_change else "è¿‘ä¸‰æ—¥ç„¡ç›¸é—œæ–°è"
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš–ï¸\nåŸå› ï¼š{reason_text}"

    # å°‡æ–°èå…§å®¹èˆ‡åˆ†æ•¸æ•´åˆ
    combined = "\n".join(f"{i+1}. ({s:+.2f}) {t}" for i, (t, s) in enumerate(news_list))

    prompt_text = f"""
ä½ æ˜¯ä¸€ä½é‡‘èæ–°èåˆ†æå“¡ã€‚
è«‹é–±è®€ä»¥ä¸‹é—œæ–¼ã€Œ{target}ã€æœ€è¿‘ä¸‰å¤©çš„æ–°èæ‘˜è¦ï¼Œ
ä»¥ã€Œæƒ…ç·’èåˆæ¨¡å¼ã€é€²è¡Œæƒ…ç·’ç¸½çµèˆ‡èµ°å‹¢é æ¸¬ï¼š

1. ç¶œåˆæ–°èä¸­çš„åˆ©å¤šèˆ‡åˆ©ç©ºæƒ…ç·’ï¼Œçµ¦å‡ºæ•´é«”æƒ…ç·’åˆ†æ•¸ï¼ˆ-10 ~ +10ï¼‰ã€‚
2. è‹¥åˆ©å¤šèˆ‡åˆ©ç©ºå‹¢å‡åŠ›æ•µï¼Œè«‹å›ç­”ã€Œä¸æ˜ç¢º âš–ï¸ã€ã€‚
3. è‹¥åˆ©å¤šæƒ…ç·’æ˜é¡¯ä½”å„ªï¼ˆ> +2ï¼‰ï¼Œè«‹å›ç­”ã€Œä¸Šæ¼² ğŸ”¼ã€ã€‚
4. è‹¥åˆ©ç©ºæƒ…ç·’æ˜é¡¯ä½”å„ªï¼ˆ< -2ï¼‰ï¼Œè«‹å›ç­”ã€Œä¸‹è·Œ ğŸ”½ã€ã€‚
5. é™„ä¸Šç°¡çŸ­åŸå› ï¼ˆ40 å­—å…§ï¼‰ï¼Œèªªæ˜ä¸»å°æƒ…ç·’çš„ä¸»è¦å› ç´ ã€‚
6. å¯åƒè€ƒä»Šæ—¥è‚¡åƒ¹æ¼²è·Œï¼š{price_change}

æ•´é«”å¹³å‡æƒ…ç·’åˆ†æ•¸ç‚º {avg_score:+.2f}ã€‚

ä»¥ä¸‹æ˜¯æ–°èæ‘˜è¦ï¼ˆå«æƒ…ç·’åˆ†æ•¸ï¼‰ï¼š
{combined}

è«‹è¼¸å‡ºæ ¼å¼å¦‚ä¸‹ï¼š
æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š{{ä¸Šæ¼²ï¼ä¸‹è·Œï¼ä¸æ˜ç¢º}}ï¼ˆé™„ç¬¦è™Ÿï¼‰
åŸå› ï¼š{{ä¸€å¥ç¸½çµç†ç”±}}
æƒ…ç·’åˆ†æ•¸ï¼š{{æ•´æ•¸ï¼ˆ-10~10ï¼‰}}
"""

    try:
        resp = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­å°è‚¡åˆ†æå¸«ï¼Œéœ€ç¶œåˆæƒ…ç·’èˆ‡å¸‚å ´åæ‡‰åšå‡ºåˆ¤æ–·ã€‚"},
                {"role": "user", "content": prompt_text},
            ],
            temperature=0.2,
            max_tokens=200,
            timeout=25,
        )
        ans = resp.choices[0].message.content.strip()
        ans = re.sub(r"\s+", " ", ans)

        # æå–èµ°å‹¢
        m_trend = re.search(r"(ä¸Šæ¼²|ä¸‹è·Œ|ä¸æ˜ç¢º|å¾®æ¼²|å¾®è·Œ)", ans)
        trend = m_trend.group(1) if m_trend else "ä¸æ˜ç¢º"
        symbol_map = {"ä¸Šæ¼²": "ğŸ”¼", "å¾®æ¼²": "â†—ï¸", "å¾®è·Œ": "â†˜ï¸", "ä¸‹è·Œ": "ğŸ”½", "ä¸æ˜ç¢º": "âš–ï¸"}

        # æå–ç†ç”±
        m_reason = re.search(r"(?:åŸå› |ç†ç”±)[:ï¼š]?\s*(.+?)(?:æƒ…ç·’åˆ†æ•¸|$)", ans)
        reason = m_reason.group(1).strip() if m_reason else f"å¸‚å ´è§€æœ›ã€‚ä»Šæ—¥æ¼²è·Œï¼š{price_change}"

        # æå–æƒ…ç·’åˆ†æ•¸
        m_score = re.search(r"æƒ…ç·’åˆ†æ•¸[:ï¼š]?\s*(-?\d+)", ans)
        mood_score = int(m_score.group(1)) if m_score else 0

        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š{trend} {symbol_map.get(trend,'')}\nåŸå› ï¼š{reason}\næƒ…ç·’åˆ†æ•¸ï¼š{mood_score:+d}"

    except Exception as e:
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šæŒå¹³ âš–ï¸\nåŸå› ï¼šGroqåˆ†æå¤±æ•—({e})\næƒ…ç·’åˆ†æ•¸ï¼š0"


# ---------- ä¸»åˆ†æ ----------
def analyze_target(db, collection: str, target: str, result_field: str):
    pos, neg = load_tokens(db)
    pos_c, neg_c = compile_tokens(pos), compile_tokens(neg)

    today = datetime.now(TAIWAN_TZ).date()
    filtered, weighted_scores = [], []

    price_change = ""  # åªæŠ“ä¸€æ¬¡
    for d in db.collection(collection).stream():
        dt = parse_docid_time(d.id)
        if not dt:
            continue
        news_date = dt.date()
        delta_days = (today - news_date).days

        if delta_days > 2:
            continue

        day_weight = {0:1.0, 1:0.85, 2:0.7}.get(delta_days,0.7)
        data = d.to_dict() or {}
        for k, v in data.items():
            if not isinstance(v, dict):
                continue
            if not price_change:
                price_change = v.get("price_change", "")  # åªå–ç¬¬ä¸€ç­†

            title, content = v.get("title", ""), v.get("content", "")
            full = title + " " + content
            res = score_text(full, pos_c, neg_c, target)
            if not res.hits:
                continue

            token_weight = 1.0 + min(len(res.hits) * 0.05, 0.3)
            total_weight = day_weight * token_weight

            filtered.append((d.id, k, title, res, total_weight))
            weighted_scores.append(res.score * total_weight)

    if not filtered:
        print(f"{target}ï¼šè¿‘ä¸‰æ—¥ç„¡æ–°èï¼Œäº¤ç”± Groq åˆ¤æ–·ã€‚\n")
        summary = groq_analyze([], target, 0, price_change)
    else:
        filtered.sort(key=lambda x: abs(x[3].score * x[4]), reverse=True)
        top_news = filtered[:10]

        print(f"\nğŸ“° {target} è¿‘æœŸé‡é»æ–°èï¼š")
        for docid, key, title, res, weight in top_news:
            print(f"[{docid}#{key}] ({weight:.2f}x, åˆ†æ•¸={res.score:+.2f}) {title}")
            for p, w, n in res.hits:
                print(f"   {'+' if w>0 else '-'} {p}ï¼ˆ{n}ï¼‰")

        news_with_scores = [(t, res.score * weight) for _, _, t, res, weight in top_news]
        avg_score = sum(s for _, s in news_with_scores) / len(news_with_scores)
        summary = groq_analyze(news_with_scores, target, avg_score, price_change)

        fname = f"result_{today.strftime('%Y%m%d')}.txt"
        with open(fname, "a", encoding="utf-8") as f:
            f.write(f"======= {target} =======\n")
            for docid, key, title, res, weight in top_news:
                hits_text = "\n".join([f"  {'+' if w>0 else '-'} {p}ï¼ˆ{n}ï¼‰" for p, w, n in res.hits])
                f.write(f"[{docid}#{key}]ï¼ˆ{weight:.2f}xï¼‰\næ¨™é¡Œï¼š{first_n_sentences(title)}\nå‘½ä¸­ï¼š\n{hits_text}\n\n")
            f.write(summary + "\n\n")

    print(summary + "\n")

    try:
        db.collection(result_field).document(today.strftime("%Y%m%d")).set({
            "timestamp": datetime.now(TAIWAN_TZ).isoformat(),
            "result": summary,
        })
    except Exception as e:
        print(f"[warning] Firestore å¯«å›å¤±æ•—ï¼š{e}")

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    if not SILENT_MODE:
        print("ğŸš€ é–‹å§‹åˆ†æå°è‚¡ç„¦é»è‚¡ï¼ˆæº–ç¢ºç‡æå‡ç‰ˆï¼‰...\n")

    db = get_db()
    analyze_target(db, NEWS_COLLECTION_TSMC, "å°ç©é›»", "Groq_result")
    print("=" * 70)
    analyze_target(db, NEWS_COLLECTION_FOX, "é´»æµ·", "Groq_result_Foxxcon")
    print("=" * 70)
    analyze_target(db, NEWS_COLLECTION_UMC, "è¯é›»", "Groq_result_UMC")

if __name__ == "__main__":
    main()

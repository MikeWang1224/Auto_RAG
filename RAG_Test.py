# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ–°èåˆ†æå·¥å…·ï¼ˆå¤šå…¬å¸ RAG ç‰ˆï¼šå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
æ•´åˆ price_change èˆ‡æƒ…ç·’åˆ†æ•¸ï¼Œä¸€æ¬¡å‚³çµ¦ Groq åšæ˜æ—¥è‚¡åƒ¹é æ¸¬
"""

import os, signal, regex as re
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import List, Tuple
from google.cloud import firestore
from dotenv import load_dotenv
from groq import Groq

# ---------- è¨­å®š ----------
SILENT_MODE = False
TAIWAN_TZ = timezone(timedelta(hours=8))

TOKENS_COLLECTION = "bull_tokens"
NEWS_COLLECTION_TSMC = "NEWS"
NEWS_COLLECTION_FOX = "NEWS_Foxxcon"
NEWS_COLLECTION_UMC = "NEWS_UMC"

STOP = False
def _sigint_handler(signum, frame):
    global STOP
    STOP = True
signal.signal(signal.SIGINT, _sigint_handler)

if os.path.exists(".env"):
    load_dotenv(".env", override=True)
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ---------- çµæ§‹ ----------
@dataclass
class Token:
    polarity: str
    ttype: str
    pattern: str
    weight: float
    note: str

@dataclass
class MatchResult:
    score: float
    hits: List[Tuple[str, float, str]]

# ---------- å·¥å…· ----------
def get_db():
    return firestore.Client()

def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "").strip().lower())

# ---------- Token ----------
def load_tokens(db):
    pos, neg = [], []
    for d in db.collection(TOKENS_COLLECTION).stream():
        data = d.to_dict() or {}
        pol = data.get("polarity", "").lower()
        ttype = data.get("type", "substr").lower()
        patt = data.get("pattern", "")
        note = data.get("note", "")
        w = float(data.get("weight", 1.0))
        if pol == "positive":
            pos.append(Token(pol, ttype, patt, w, note))
        elif pol == "negative":
            neg.append(Token(pol, ttype, patt, -abs(w), note))
    return pos, neg

def compile_tokens(tokens: List[Token]):
    compiled = []
    for t in tokens:
        if t.ttype == "regex":
            try:
                compiled.append(("regex", re.compile(t.pattern, re.I), t.weight, t.note, t.pattern))
            except:
                continue
        else:
            compiled.append(("substr", None, t.weight, t.note, t.pattern.lower()))
    return compiled

# ---------- Scoring ----------
def score_text(text: str, pos_c, neg_c, target: str = None) -> MatchResult:
    norm = normalize(text)
    score, hits, seen = 0.0, [], set()
    aliases = {
        "å°ç©é›»": ["å°ç©é›»", "tsmc", "2330"],
        "é´»æµ·": ["é´»æµ·", "foxconn", "2317", "å¯Œå£«åº·"],
        "è¯é›»": ["è¯é›»", "umc", "2303"],
    }
    company_pattern = "|".join(re.escape(a) for a in aliases.get(target, []))
    if not re.search(company_pattern, norm):
        return MatchResult(0.0, [])
    for ttype, cre, w, note, patt in pos_c + neg_c:
        key = (patt, note)
        if key in seen:
            continue
        matched = cre.search(norm) if ttype == "regex" else patt in norm
        if matched:
            score += w
            hits.append((patt, w, note))
            seen.add(key)
    return MatchResult(score, hits)

def adjust_score_for_context(text: str, base_score: float) -> float:
    if not text or base_score == 0:
        return base_score
    norm = text.lower()
    neutral_phrases = ["é‡ç”³", "ç¬¦åˆé æœŸ", "é æœŸå…§", "ä¸­æ€§çœ‹å¾…", "ç„¡é‡å¤§å½±éŸ¿", "æŒå¹³", "æœªè®Š"]
    if any(p in norm for p in neutral_phrases):
        base_score *= 0.4
    positive_boost = ["å‰µæ–°é«˜", "å€å¢", "å¤§å¹…æˆé•·", "ç²åˆ©æš´å¢", "å ±å–œ"]
    negative_boost = ["æš´è·Œ", "ä¸‹æ»‘", "è™§æ", "åœå·¥", "ä¸‹ä¿®", "è£å“¡", "è­¦è¨Š"]
    if any(p in norm for p in positive_boost):
        base_score *= 1.3
    if any(p in norm for p in negative_boost):
        base_score *= 1.3
    return base_score

# ---------- Groq åˆ†æ ----------
def groq_analyze(news_list: List[Tuple[str,str,float]], target: str) -> str:
    if not news_list:
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šä¸æ˜ç¢º âš–ï¸\nåŸå› ï¼šè¿‘ä¸‰æ—¥ç„¡ç›¸é—œæ–°è\næƒ…ç·’åˆ†æ•¸ï¼š0"

    avg_score = sum(score for _, _, score in news_list) / len(news_list)
    combined = "\n".join(
        f"{i+1}. æ¨™é¡Œï¼š{title}\n   ç•¶æ—¥è‚¡åƒ¹æ¼²è·Œï¼š{pc}\n   æƒ…ç·’åˆ†æ•¸ï¼š{score:+.2f}"
        for i, (title, pc, score) in enumerate(news_list)
    )

    prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å°è‚¡é‡‘èåˆ†æå¸«ï¼Œè«‹æ ¹æ“šä»¥ä¸‹ã€Œ{target}ã€è¿‘ä¸‰æ—¥æ–°èæ‘˜è¦ï¼Œ
ä¾æƒ…ç·’åˆ†æ•¸èˆ‡ç•¶æ—¥è‚¡åƒ¹æ¼²è·Œï¼Œåš´æ ¼æ¨è«–æ˜æ—¥è‚¡åƒ¹æ–¹å‘ã€‚

æ•´é«”å¹³å‡æƒ…ç·’åˆ†æ•¸ï¼š{avg_score:+.2f}

{combined}

è«‹çµ¦å‡ºæ˜å¤©è‚¡åƒ¹èµ°å‹¢ã€åŸå› åŠæƒ…ç·’åˆ†æ•¸ï¼ˆ-10~+10ï¼‰ã€‚
æ³¨æ„ï¼šåŸå› æ–‡å­—å¿…é ˆèˆ‡è‚¡åƒ¹èµ°å‹¢ä¸€è‡´ã€‚
"""
    try:
        resp = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å°è‚¡é‡åŒ–åˆ†æå“¡ï¼Œéœ€ä¾æƒ…ç·’åˆ†æ•¸èˆ‡è‚¡åƒ¹æ¼²è·Œè¦å‰‡ç”¢ç”Ÿçµè«–ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.15,
            max_tokens=220,
        )
        ans = re.sub(r"\s+", " ", resp.choices[0].message.content.strip())

        m_trend = re.search(r"(ä¸Šæ¼²|å¾®æ¼²|å¾®è·Œ|ä¸‹è·Œ|ä¸æ˜ç¢º)", ans)
        trend = m_trend.group(1) if m_trend else "ä¸æ˜ç¢º"
        symbol_map = {"ä¸Šæ¼²": "ğŸ”¼", "å¾®æ¼²": "â†—ï¸", "å¾®è·Œ": "â†˜ï¸", "ä¸‹è·Œ": "ğŸ”½", "ä¸æ˜ç¢º": "âš–ï¸"}

        m_reason = re.search(r"(?:åŸå› |ç†ç”±)[:ï¼š]?\s*(.+?)(?:æƒ…ç·’åˆ†æ•¸|$)", ans)
        reason = m_reason.group(1).strip() if m_reason else "æ–°èè¨Šæ¯èˆ‡è‚¡åƒ¹è¶¨å‹¢æ•´åˆåˆ†æå¾—å‡ºçš„çµè«–ã€‚"

        m_score = re.search(r"æƒ…ç·’åˆ†æ•¸[:ï¼š]?\s*(-?\d+)", ans)
        mood_score = int(m_score.group(1)) if m_score else max(-10, min(10, int(round(avg_score * 3))))

        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼š{trend} {symbol_map.get(trend,'')}\nåŸå› ï¼š{reason}\næƒ…ç·’åˆ†æ•¸ï¼š{mood_score:+d}"

    except Exception as e:
        return f"æ˜å¤©{target}è‚¡åƒ¹èµ°å‹¢ï¼šæŒå¹³ âš–ï¸\nåŸå› ï¼šGroqåˆ†æå¤±æ•—({e})\næƒ…ç·’åˆ†æ•¸ï¼š0"

# ---------- åˆ†ææµç¨‹ ----------
def analyze_target(db, collection_name, target):
    pos_tokens, neg_tokens = load_tokens(db)
    pos_c, neg_c = compile_tokens(pos_tokens), compile_tokens(neg_tokens)

    news_docs = list(db.collection(collection_name)
                     .order_by("timestamp", direction=firestore.Query.DESCENDING)
                     .limit(3).stream())

    news_list = []
    for doc in news_docs:
        data = doc.to_dict()
        title = data.get("title", "")
        price_change = data.get("price_change", "æœªæä¾›")
        score = score_text(title, pos_c, neg_c, target).score
        score = adjust_score_for_context(title, score)
        news_list.append((title, price_change, score))

    print(groq_analyze(news_list, target))

# ---------- ä¸»ç¨‹å¼ï¼ˆèˆŠç‰ˆé¢¨æ ¼ï¼‰ ----------
def main():
    db = get_db()
    analyze_target(db, NEWS_COLLECTION_TSMC, "å°ç©é›»")
    print("=" * 70)
    analyze_target(db, NEWS_COLLECTION_FOX, "é´»æµ·")
    print("=" * 70)
    analyze_target(db, NEWS_COLLECTION_UMC, "è¯é›»")

if __name__ == "__main__":
    main()
